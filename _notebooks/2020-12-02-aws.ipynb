{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS\n",
    "\n",
    "- hide: true\n",
    "- toc: true\n",
    "- comments: true\n",
    "- categories: [python, aws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List content of bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fgu-mdb/data_000.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'fgu-mdb'\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "fs.ls(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 27)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'data_000.csv'\n",
    "\n",
    "df = pd.read_csv(f's3://{bucket}/{file}', sep='|')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data with selected profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are multiple profile in aws credential file, can select desired one as below. In this case, I select different profile depending on where code is being run. Botocore is required because Pandas uses it to read s3 files, and, by default, uses the aws default profile to read data. If, as below, I instead want Pandas to read data using another profile, I need to use a botocore session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to S3 with correct aws credential profile\n",
    "profile = 'tracker-fgu'\n",
    "\n",
    "if platform.system() == 'Linux':\n",
    "    profile = 'default'\n",
    "\n",
    "session = botocore.session.Session(profile=profile)\n",
    "fs = s3fs.S3FileSystem(session=session)\n",
    "fs.ls('/')\n",
    "\n",
    "path = 'path-to-file'\n",
    "df = pd.read_csv(fs.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habits",
   "language": "python",
   "name": "habits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
