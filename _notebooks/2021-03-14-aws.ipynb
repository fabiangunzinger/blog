{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS\n",
    "\n",
    "- hide: false\n",
    "- toc: true\n",
    "- comments: true\n",
    "- categories: [python, pandas, aws]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently used interaction patterns with AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- There are multiple ways to access your AWS account. I store config and credential files in `~/.aws` as discussed [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). AWS access methods find these files automatically so I don't have to worry about that.\n",
    "\n",
    "- What I do have to worry about is choosing the appropriate profile depending on what AWS account I want to interact with (e.g. my personal one or one for work). This is different for each library, so I cover this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `s3fs`\n",
    "\n",
    "- Built by people at Dask, [`s3fs`](https://github.com/dask/s3fs) is built on top of `botocore` and provides a convenient way to interact with S3.\n",
    "\n",
    "- While it can be used to read and write data, I find there are easier ways to do that and so I don't use the library for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigate buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# establish connection\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "# count items in s3 root bucket\n",
    "len(fs.ls('/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose profile\n",
    "\n",
    "To choose a profile other than `default`, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect using a different profile\n",
    "fs = s3fs.S3FileSystem(profile='tracker-fgu')\n",
    "len(fs.ls('/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Read and write directly from `Pandas`\n",
    "\n",
    "- Pandas can read and write files to and from S3 directly if you provide the file name as `s3://<bucket>/<filename>`.\n",
    "\n",
    "- By default, `Pandas` uses the default profile to access S3. Recent versions of `Pandas` have a `storage_options` parameter that can be used to provide, among other things, a profile name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168943, 21)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read using default profile \n",
    "\n",
    "fp = 's3://fgu-samples/transactions.parquet'\n",
    "df = pd.read_parquet(fp)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9388334, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read using custom profile\n",
    "\n",
    "fp = 's3://temp-mdb/data_XX7.parquet'\n",
    "df = pd.read_parquet(fp, storage_options = dict(profile='tracker-fgu'))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for simple jobs, but in a large project, passing the profile information to each read and write call is cumbersome and ugly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple improvement using `functools.partial` \n",
    "\n",
    "`functools.partial`provides a simple solution, as it allows me to create a custom function with a fixed storage options argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9388334, 23)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "options = dict(storage_options=dict(profile='tracker-fgu'))\n",
    "read_parquet_s3 = functools.partial(pd.read_parquet, **options)\n",
    "df = read_parquet_s3(fp)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More flexible solution with custom function\n",
    "\n",
    "Often, I run projects on my Mac for testing and a virtual machine to run the full code. In this case, I need a way to automatically provide the correct profile name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FabsMacBook.local'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "platform.node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_s3_funcs():\n",
    "    \"\"\"\n",
    "    Return readers and writers with frozen AWS profile name.\n",
    "    \"\"\"\n",
    "    # identify required profile (depends on project)\n",
    "    if platform.node() == 'FabsMacBook.local':\n",
    "        profile = 'tracker-fgu'\n",
    "    else:\n",
    "        profile = 'default'\n",
    "        \n",
    "    # create partial readers and writers\n",
    "    options = dict(storage_options=dict(profile=profile))\n",
    "    read_csv_s3 = functools.partial(pd.read_csv, **options)\n",
    "    write_csv_s3 = functools.partial(pd.write_csv, **options)\n",
    "    read_parquet_s3 = functools.partial(pd.read_parquet, **options)\n",
    "    write_parquet_s3 = functools.partial(pd.write_parquet, **options)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is not ideal, as it requires cumbersome unpacking of return. Maybe using decorator is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `awswrangler`\n",
    "\n",
    "A new [library](https://github.com/awslabs/aws-data-wrangler) from AWS labs for Pandas interaction with a number of AWS services. Looks very promising, but haven't had any use for it thus far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "blog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
