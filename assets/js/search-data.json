{
  
    
        "post0": {
            "title": "Web frameworks",
            "content": "Every time I visit a website, an application called a web server who usually resides on a machine also called a web server sends HTML to my browser. HTML (Hypertext markup language) is used by browsers to describe the content and structure of a website. . | The essence of every web application is to send HTML to a browser. . | How does a web server know what data it needs to send? It sends what I request using the HTTP protocol. (HTTP means hypertext transfer protocol; a protocol is a universally agreed data format and sequence of steps to enable communication between two parties.) . | So, a web application receives HTTP requests (e.g. get or post) and responds with an HTTP request, usually in the form of the HTML for the requested page. . | . Main sources . Jeff Knupp post | Robert Chang post | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/04/10/web-frameworks.html",
            "relUrl": "/python/2021/04/10/web-frameworks.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Tree based methods",
            "content": "Trees Are an easy and intuitive way to split data within a sample.  | Problem, they are not good at predicting out-of-sample (with different datasets)   | . | Random Forests  . Random forests remedy this by combining the simplicity of decision trees with flexibility, which leads to a large improvement in predictive accuracy.  | How to make a random forest:  Create bootstrapped sample from data (i.e. sample observations of the original sample with replacement)  | Create a decision tree using only a subset of randomly selected variables at each step (e.g. only 2 out of 4 for root, then 2 out of remaining 3 at next node, ect.)  | Repeat above two steps many times (e.g. 1000) to build many trees (and build a random forest)  | . | To predict outcome for new observation, do the following:  Feed data into each tree in the forest and keep score of the predictions (either Yes or No for each tree). The outcome with the most scores is the prediction.  | . | The process is called “Bagging” because we Bootstrap the data and rely on the AGGregate to make a decision.  | How can we test how good a tree is at out-of sample prediction without having another sample?  Bootstrapping relies on randomly sampling from data with replacement, hence, not all observations will be used to create a tree.  | The unused observations are called the “Out-of-bag Dataset”. We can use these test whether our Forest is any good at predicting.  | We simply take the out-of-bag dataset from each tree, run through the entire Forest and check whether the Forest accurately classifies the observation. We then repeat this for each out-of-bag dataset.  | The proportion of incorrectly classified out-of-bag samples is called the “out-of-bag error”.  | . | The out-of-bag error is what helps us determine how many variables to use when building our random trees above. The algorithm builds different forests with different numbers of variables (typically starting with the square-root of the total number of variables – e.g. 2 if we have 4 variables – and then calculating a few above and below that) and then picks the one with the smallest out-of-bag error.   | . | Ada boosts  . When building random forests, trees vary in their depth.  | When using Ada boost to create a Random Forest, each tree is usually just one node and two leaves. (A tree with one node and two leaves is a “stump”). So, Ada boost produces a Forest of Stumps.  | Because a stump only makes use of a single variable, they are generally poor predictors.  | Main ideas of ada boost  Take Forest of Stumps  | Stumps have different weights (mounts of say) in the calculation of the out-of-bag error (with the weights being proportional to the gravity of the prediction errors they make. Loosely speaking, for how many observations they get the prediction wrong).   | Each stump takes the errors of the previous stump into account (it does this by treating as more important those observations that the previous stump misclassified).   | . | Process  Create first Stump using the variable that best classifies outcomes  | Then calculate classification error  | The size of that error determines the weight this stump gets in the overall classification (i.e. in the Forest of Stumps).  | The next stump will be build using the variable that best classifies outcomes in a dataset that over-emphasizes the observations that the previous stump misclassified.  | As in a Random Forst, we run all obseravtions through all Stumps and keep track of the classification. Instead of adding up the Yes and No, we add up the amount of say of the Yes Stumps and No Stumps and classify as Yes if total amount of say of yes Stumps is larger.  | . | . | Gradient boosting (most used configuration)  . Comparison to Ada boost  Like Ada boost builds fixed size trees, but they can be larger than a Stump (in our specification, we use trees with a depth of 5)  | GB also scales trees, but all by same amount  | Also builds tree based on error of previous tree  | . | Algorithm  Predict based on average and calculate (pseudo residuals)  | Then build a tree to predict residuals  | Scale predicted residual by the learning rate (we use 0.1) and add to original prediction.  | Calculate new pseudo residuals and build new tree to predict.  | Add scaled predictions to the previous prediction (i.e. to the original prediction and the previous scaled prediction).  | Keep going like this until additional trees no longer improve prediction or hit number of max trees.  | . | . | Initial prediction is log of odds: log(number in HE/number not in HE) and convert to a probability using the logistic function (e^logodds / 1 + e^logodds). If probability &gt; 0.5, prediction is “Yes” for all observations. Else is no  . | Calculate pseudo residuals as actual - predicted (e.g. 1 - 0.7)  | Build tree to predict residuals | .",
            "url": "https://fabiangunzinger.github.io/blog/python/ml/stats/2021/04/10/tree-based-methods.html",
            "relUrl": "/python/ml/stats/2021/04/10/tree-based-methods.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Splitting a large file in Python",
            "content": "https://www.wefearchange.org/2013/05/resource-management-in-python-33-or.html . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/04/10/splitting-file.html",
            "relUrl": "/python/2021/04/10/splitting-file.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Regular expressions",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . path = &#39;/Users/fgu/tmp/data_777.csv&#39; df = pd.read_csv(path, sep=&#39;|&#39;) . a = &#39;b&#39; f&#39;he {a}&#39; . &#39;he b&#39; . [str(n) for n in range(1, 10)] . [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] . with open(path) as f: next(f) line = f.readline() pattern = &#39;&quot; d+&quot; |&quot;(?P&lt;user_id&gt; d+)&quot;&#39; match = re.match(pattern, line) print(line) print(match) print(match.group(&#39;user_id&#39;)) . &#34;688293&#34;|&#34;777&#34;|&#34;2011-07-20&#34;|&#34;1969&#34;|&#34;20K to 30K&#34;|&#34;WA1 4&#34;|&#34;E01012553&#34;|&#34;E02002603&#34;|&#34;M&#34;|&#34;2012-01-25&#34;|&#34;262916&#34;|&#34;NatWest Bank&#34;|&#34;Current&#34;|&#34;364.22&#34;|&#34;9572 24jan12 , tcs bowdon , bowdon gb - pos&#34;|&#34;Debit&#34;|&#34;25.03&#34;|&#34;No Tag&#34;|&#34;No Tag&#34;|&#34;No Tag&#34;|&#34;No Merchant&#34;|&#34;Unknown Merchant&#34;|&#34;2011-07-20&#34;|&#34;2020-07-21 20:32:00&#34;|&#34;2014-07-18&#34;|&#34;2017-10-24&#34;|&#34;U&#34; &lt;re.Match object; span=(0, 14), match=&#39;&#34;688293&#34;|&#34;777&#34;&#39;&gt; 777 . re.Match.group . def colname_cleaner(df): &quot;&quot;&quot;Convert column names to stripped lowercase with underscores.&quot;&quot;&quot; df.columns = df.columns.str.lower().str.strip() return df def str_cleaner(df): &quot;&quot;&quot;Convert string values to stripped lowercase.&quot;&quot;&quot; str_cols = df.select_dtypes(&#39;object&#39;) for col in str_cols: df[col] = df[col].str.lower().str.strip() return df movies = (data.movies() .pipe(colname_cleaner) .pipe(str_cleaner)) movies.head(2) . title us gross worldwide gross us dvd sales production budget release date mpaa rating running time min distributor source major genre creative type director rotten tomatoes rating imdb rating imdb votes . 0 the land girls | 146083.0 | 146083.0 | NaN | 8000000.0 | jun 12 1998 | r | NaN | gramercy | None | None | None | None | NaN | 6.1 | 1071.0 | . 1 first love, last rites | 10876.0 | 10876.0 | NaN | 300000.0 | aug 07 1998 | r | NaN | strand | None | drama | None | None | NaN | 6.9 | 207.0 | . import re . Finding a single pattern in text . pattern = &#39;hello&#39; text = &#39;hello world it is a beautiful day.&#39; match = re.search(pattern, text) match.start(), match.end(), match.group() . (0, 5, &#39;hello&#39;) . In Pandas . movies.title.str.extract(&#39;(love)&#39;) . 0 . 0 NaN | . 1 love | . 2 NaN | . 3 NaN | . 4 NaN | . ... ... | . 3196 NaN | . 3197 NaN | . 3198 NaN | . 3199 NaN | . 3200 NaN | . 3201 rows × 1 columns . contains(): Test if pattern or regex is contained within a string of a Series or Index. | match(): Determine if each string starts with a match of a regular expression. | fullmatch(): | extract(): Extract capture groups in the regex pat as columns in a DataFrame. | extractall(): Returns all matches (not just the first match). | find(): | findall(): | replace(): | . movies.title.replace(&#39;girls&#39;, &#39;hello&#39;) . 0 the land girls 1 first love, last rites 2 i married a strange person 3 let&#39;s talk about sex 4 slam ... 3196 zack and miri make a porno 3197 zodiac 3198 zoom 3199 the legend of zorro 3200 the mask of zorro Name: title, Length: 3201, dtype: object . Let&#39;s drop all movies by distributors with &quot;Pictures&quot; and &quot;Universal&quot; in their title. . names = [&#39;Universal&#39;, &#39;Pictures&#39;] pattern = &#39;|&#39;.join(names) mask = movies.distributor.str.contains(pattern, na=True) result = movies[~mask] result.head(2) . title us_gross worldwide_gross us_dvd_sales production_budget release_date mpaa_rating running_time_min distributor source major_genre creative_type director rotten_tomatoes_rating imdb_rating imdb_votes . 0 The Land Girls | 146083.0 | 146083.0 | NaN | 8000000.0 | Jun 12 1998 | R | NaN | Gramercy | None | None | None | None | NaN | 6.1 | 1071.0 | . 1 First Love, Last Rites | 10876.0 | 10876.0 | NaN | 300000.0 | Aug 07 1998 | R | NaN | Strand | None | Drama | None | None | NaN | 6.9 | 207.0 | . names = [&#39;Universal&#39;, &#39;Pictures&#39;] pattern = &#39; |&#39;.join(names) neg_pattern = f&#39;[^{pattern}]&#39; neg_pattern mask = movies.distributor.str.contains(neg_pattern, na=False) result2 =movies[mask] . neg_pattern . &#39;[^Universal |Pictures]&#39; . result == result2 . ValueError Traceback (most recent call last) &lt;ipython-input-114-1dac580d3c6f&gt; in &lt;module&gt; -&gt; 1 result == result2 ~/miniconda3/envs/habits/lib/python3.7/site-packages/pandas/core/ops/__init__.py in f(self, other) 837 if not self._indexed_same(other): 838 raise ValueError( --&gt; 839 &#34;Can only compare identically-labeled DataFrame objects&#34; 840 ) 841 new_data = dispatch_to_series(self, other, op, str_rep) ValueError: Can only compare identically-labeled DataFrame objects . def drop_card_repayments(df): &quot;&quot;&quot;Drop card repayment transactions from current accounts.&quot;&quot;&quot; tags = [&#39;credit card repayment&#39;, &#39;credit card payment&#39;, &#39;credit card&#39;] pattern = &#39;|&#39;.join(tags) mask = df.auto_tag.str.contains(pattern) &amp; df.account_type.eq(&#39;current&#39;) return df[~mask] . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/04/10/regex.html",
            "relUrl": "/python/2021/04/10/regex.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Profiling",
            "content": "We should forget about small efficiencies, say about 97% of the time:premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. -- Donald Knuth Takeaway: optimise where it matters. And to know where it matters, you need to profile your code. . from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Intro . What&#39;s efficient code? . Fast (minimal completion time) | Has no unnecessary memory overhead (minimal resource consumption) | . Profiling runtime . Optimise slow lines inside slow function. | . Finding slow functions (IPython implementation): time.time() decorator (%time, or %timeit for more precision) | cProfile (%prun) | Snakeviz is helpful for cProfile results visualisation | . | . Finding slow lines: line_profiler (%lprun) | . | . Best practices: . Check overall CPU usage during profiling (e.g. use activity monitor on Mac) to make sure that no other processes are influencing my results (e.g. Dropbox update). . | Form a hypothesis about what parts of the code are slow and then compare to the profiling results to improve your intuition over time. . | Start with quick and dirty profiling to zoom into the relevant area (e.g. use a timer decorator, use %time instead of %timeit in Jupyter) before doing more costly profiling. . | | . | . Useful: . https://www.machinelearningplus.com/python/cprofile-how-to-profile-your-python-code/ | . Things to know . cProfile() doesn&#39;t seem to work with multiprocessing. | . Sources . High Performance Python | Effective Python | Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/04/10/profiling.html",
            "relUrl": "/python/pandas/2021/04/10/profiling.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Principal component analysis",
            "content": "from datetime import datetime import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns; sns.set() import statsmodels.api as sm . def make_data(): &quot;&quot;&quot;Fetch and prepare data used for examples.&quot;&quot;&quot; fp = &#39;s3://fgu-samples/transactions.parquet&#39; cols = [&#39;amount&#39;, &#39;user_id&#39;, &#39;year_of_birth&#39;, &#39;gender&#39;] df = pd.read_parquet(fp, columns=cols) df = df[df.gender.isin([&#39;m&#39;, &#39;f&#39;])] df[&#39;age&#39;] = datetime.now().year - df.year_of_birth df[&#39;male&#39;] = df.gender == &#39;m&#39; df = df.rename(columns={&#39;amount&#39;: &#39;spend&#39;}) g = df.groupby(&#39;user_id&#39;) df = g.agg({&#39;spend&#39;: &#39;sum&#39;, &#39;age&#39;: &#39;first&#39;, &#39;male&#39;: &#39;first&#39;}) return df df = make_data() df.head(3) . spend age male . user_id . 977 36665.800781 | 48.0 | True | . 3277 17830.087891 | 34.0 | False | . 4277 -24827.089844 | 62.0 | True | . PCA basics . from sklearn.decomposition import PCA pca = PCA(n_components=1) pca.fit(df) . PCA(n_components=1) . The fitted PCA&#39;s component attribute contains the loading vectors of each principal component, which, in turn, contain the loadings for each of the original features. . pca.components_ . array([[ 1.00000000e+00, 1.30713265e-05, -5.78356798e-07]]) . By definition, their squares have to sum to zero. Let&#39;s check: . np.sum(np.square(pca.components_)) . 1.0 . Let&#39;s transform the data . df_pca = pca.transform(df) df_pca[:5] . array([[ 21732.90091993], [ 2897.18784849], [-39759.98951681], [ 72551.13144968], [-21768.62012473]]) . We could have done this manually, like to: . ((df - df.mean()) * pca.components_[0]).sum(1)[:5] . user_id 977 21732.900508 3277 2897.187436 4277 -39759.989929 5377 72551.131038 7077 -21768.620537 dtype: float64 . Dimensionality reduction . plt.scatter(data=df, x=&#39;age&#39;, y=&#39;spend&#39;, alpha=0.8); . pca = PCA(n_components=1) df_pca = pca.fit_transform(df) df_new = pca.inverse_transform(df_pca) . plt.scatter(data=df, x=&#39;age&#39;, y=&#39;spend&#39;, alpha=0.2); plt.scatter(df_new[:, 1], df_new[:, 0], alpha=0.6); . Sources . The hundred-page machine learning book | An introduction to statistical learning | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/stats/clean/2021/04/10/principal-component-analysis.html",
            "relUrl": "/ml/stats/clean/2021/04/10/principal-component-analysis.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Penalised regression",
            "content": "import numpy as np import pandas as pd import seaborn as sns import statsmodels.api as sm . Both are methods to penalise large coefficients. . | Why would we want to do this? Because we know from bias-variance trade-off -- the U-Shaped curve in complexity-MSE space -- that an increase in bias will result in a reduction of the variance. Now, on some points on that curve, increasing bias by a little might lead to a comparatively large reduction in variance, and thus reduce MSE. Exploiting this possibility is the idea behind penalising large coefficients: biasing coefficients towards zero will introduce a bit of bias, but we hope to get rewarded by lower variance so that overall model performance improves. . | . Difference between approaches . Ridge uses the sum of L2 norms as the penalty: the sum of the squared coefficients. . | Lasso used the sum of L1 norms: the sum of absolute coefficients. . | With lasso, some coefficients might become exactly zero and thus drop out of the model, which is why Lasso is considered a model selection approach. . | The reason for this is shape of the penalties in coefficient size - penalty space: when we square penalties as in Ridge, that gives us a smooth U-shaped parabola that&#39;s fairly flat at teh bottom, meaning that as we move towards zero coefficient size, the reduction in the penalty gets increasingly small and will eventually hit a point where a further reduction in coefficient size is not worth it. In contrast, using absolute penalties leads to a triangle shaped curve with equal (absolute) slope on all points except zero. This means that the marginal gain of moving coefficient size to zero stays constant, so that, for some coefficients, going all the way might be worth it. . | . Bayesian intuition behind difference in performance . Lasso and ridge regression implicitly assume a world where most coefficients are very small (or zero, in the case of lasso) and don&#39;t contribute much to our prediction capacity and only a few are helpful. As opposted to an alternative world where we think most variables we can measure contribute a little bit in equal part to our model&#39;s predictive power. Laplace vs gaussian vs uniform distribution of true coefficient values. . | If the world is truly Laplacian, then biasing our model towards it by using penalised regression will improve the model&#39;s performance. If the world is uniform, then penalised regression will perform worse than linear regression. . | . Sources . The hundred-page machine learning book | An introduction to statistical learning | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/stats/2021/04/10/penalised-regression.html",
            "relUrl": "/ml/stats/2021/04/10/penalised-regression.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Logistic regression",
            "content": "Advantages over linear regression . For more than two outcome values (e.g. a, b, c): . We can use linear regression and classify outcomes as 1, 2, 3, but we then have to assume that (1) there is a natural ordering between the values, and (2) the distances between them are the same, which is often not true. . | Furthermore, we can get predictions outside the interval [0, 1], which is nonsensical. . | With only two possible outcomes, only the latter is a problem. . | . | . The (general) logistic function is a curve with equation: . $$f(x) = frac{L}{1 + e^{-k(x - x_0)}},$$ . where $L$ is the asymptotic upper value of the curve, $e$ is the base of the natural logarithm, $k$ is the steepness of the curve, and $x_0$ is its midpoint. . The standard logistic function, with $L = k = 1$ and $x_0 = 0$ maps values from minus infinity to plus infinity to between 0 and 1. . def general_logistic(x, L, k, x0): return L / (1 + np.exp(-k*(x - x0))) x = np.linspace(-domain, domain, 100) fig, (ax0, ax1) = plt.subplots(1, 2, sharey=True, figsize=(10, 4)) ax0.plot(x, general_logistic(x, 1, 1, 0)) ax0.axvline(0, color=&#39;orange&#39;, linestyle=&#39;dashed&#39;) ax0.set(title=&#39;Standard logistic function&#39;, xlabel=&#39;$x$&#39;, ylabel=&#39;$y$&#39;) ax1.plot(x, general_logistic(x, 1, 4, 0)) ax1.axvline(0, color=&#39;orange&#39;, linestyle=&#39;dashed&#39;) ax1.set(title=&#39;Steep logistic function&#39;, xlabel=&#39;$x$&#39;, ylabel=&#39;$y$&#39;); .",
            "url": "https://fabiangunzinger.github.io/blog/python/stats/2021/04/10/logistic-regression.html",
            "relUrl": "/python/stats/2021/04/10/logistic-regression.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Linear regression",
            "content": "import numpy as np import pandas as pd import seaborn as sns import statsmodels.api as sm . Notes: . When will regression do well: when the underlying model is linear. Check KNN vs linear regression comparison. | . Example . path = &#39;https://www.statlearning.com/s/Advertising.csv&#39; cols = [&#39;TV&#39;, &#39;radio&#39;, &#39;newspaper&#39;, &#39;sales&#39;] df = (pd.read_csv(path, usecols=cols) .rename(lambda col: col.lower(), axis=1) .reset_index() .rename(columns={&#39;index&#39;: &#39;market&#39;}) ) print(df.shape) df.head(3) . (200, 5) . market tv radio newspaper sales . 0 0 | 230.1 | 37.8 | 69.2 | 22.1 | . 1 1 | 44.5 | 39.3 | 45.1 | 10.4 | . 2 2 | 17.2 | 45.9 | 69.3 | 9.3 | . tidy = (df.melt(id_vars=[&#39;market&#39;, &#39;sales&#39;], var_name=&#39;channel&#39;, value_name=&#39;budget&#39;)) sns.lmplot(x=&#39;budget&#39;, y=&#39;sales&#39;, hue=&#39;channel&#39;, col=&#39;channel&#39;, sharex=False, data=tidy); . Estimating coefficients . mod = sm.OLS.from_formula(&#39;sales ~ tv + radio + newspaper&#39;, data=df) res = mod.fit() print(res.summary()) . OLS Regression Results ============================================================================== Dep. Variable: sales R-squared: 0.897 Model: OLS Adj. R-squared: 0.896 Method: Least Squares F-statistic: 570.3 Date: Mon, 15 Mar 2021 Prob (F-statistic): 1.58e-96 Time: 15:43:14 Log-Likelihood: -386.18 No. Observations: 200 AIC: 780.4 Df Residuals: 196 BIC: 793.6 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] Intercept 2.9389 0.312 9.422 0.000 2.324 3.554 tv 0.0458 0.001 32.809 0.000 0.043 0.049 radio 0.1885 0.009 21.893 0.000 0.172 0.206 newspaper -0.0010 0.006 -0.177 0.860 -0.013 0.011 ============================================================================== Omnibus: 60.414 Durbin-Watson: 2.084 Prob(Omnibus): 0.000 Jarque-Bera (JB): 151.241 Skew: -1.327 Prob(JB): 1.44e-33 Kurtosis: 6.332 Cond. No. 454. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . Evaluating accuracy of coefficient estimates . Hypothesis testing | . Evaluating accuracy of model . Residual standard error . The RSE is an estimate of the irreducible error, $ epsilon$, and thus meansures how much, on average, the response will deviate from the true regression line. . | The RSE is 3.25, which implies that our estimates deviate about 3.25 from the actual values (this would be true even if we knew the population parameters, as the RSE is an estimate of the error standard deviation). Given the average value of sales, the percentage error is about 12 percent. Whether this is a lot or not depends on the application. . | Becaue the RSE is an absolute measure of lack of fit, expressed in units of y, it&#39;s not always easy to interpret whether a given RSE is small or large. . | . rse = np.sqrt(res.scale) x_mean = df.sales.mean() print(rse) print(rse/ x_mean) print(res.rsquared) . 1.685510373414744 0.1202004188564624 0.8972106381789522 . $R^2$ . $R^2$, which is a relative measure of lack of fit, and measures the percentage of variance in y that the model can explain (and is thus always between 0 and 1). In the simple linear regression setting, $R^2 = Cor(X, Y)^2$. . | A low $R^2$ can mean that the true relationship is non-linear or that the error variance is very high or both. What constitutes &quot;low&quot; depends on the application. . | In the model above, more than 90 percent of the variation is explained by the set of explanatory variables. . | . Multiple linear regression . Estimating the coefficients . Questions of interest . Is there a relationship between the response and predictors? . To test whether at least one of the predictors is useful in predicting the response, we can look at the reported F statistic. | . res.fvalue, res.f_pvalue . (570.2707036590942, 1.575227256092437e-96) . To test whether a subset of parameters is useful, we can run our own F-test. To manually test for all parameters, we can use: | . a = np.identity(len(res.params))[1:] res.f_test(a) . &lt;class &#39;statsmodels.stats.contrast.ContrastResults&#39;&gt; &lt;F test: F=array([[570.27070366]]), p=1.5752272560925203e-96, df_denom=196, df_num=3&gt; . Which is equivalent to the statistic provided in the output. To test the (joint) usefulness of radio and newspaper, we can use: | . a = np.identity(len(res.params))[[2, 3]] res.f_test(a) . &lt;class &#39;statsmodels.stats.contrast.ContrastResults&#39;&gt; &lt;F test: F=array([[272.04067681]]), p=2.829486915701129e-57, df_denom=196, df_num=2&gt; . Remember: the F statistic is valuable because irrespective of $p$, there is only a 5 percent change that the p-value is below 0.05. In contrast, individual predictors each have that probability, so for a large number of predictors, it&#39;s very likely that we observe significant ones solely due to chance. | . Are all of the predictors or only a subset useful in explaining y? . Application . rse = np.sqrt(res.scale) x_mean = df.sales.mean() print(rse) print(rse/ x_mean) print(res.rsquared) . 1.685510373414744 0.1202004188564624 0.8972106381789522 . The RSE is 3.25, which implies that our estimates deviate about 3.25 from the actual values (this would be true even if we knew the population parameters, as the RSE is an estimate of the error standard deviation). Given the average value of sales, the percentage error is about 12 percent. Whether this is a lot or not depends on the application. . | Becaue the RSE is an absolute measure of lack of fit, expressed in units of y, it&#39;s not always easy to interpret whether a given RSE is small or large. . | $R^2$, which is a relative measure of lack of fit, and measures the percentage of variance in y that the model can explain (and is thus always between 0 and 1). In the simple linear regression setting, $R^2 = Cor(X, Y)^2$. . | A low $R^2$ can mean that the true relationship is non-linear or that the error variance is very high or both. What constitutes &quot;low&quot; depends on the application. . | In the model above, more than 90 percent of the variation is explained by the set of explanatory variables. . | . res.fvalue, res.f_pvalue . (570.2707036590942, 1.575227256092437e-96) . a = np.identity(len(res.params))[[2, 3]] res.f_test(a) . &lt;class &#39;statsmodels.stats.contrast.ContrastResults&#39;&gt; &lt;F test: F=array([[272.04067681]]), p=2.829486915701129e-57, df_denom=196, df_num=2&gt; . Remember: the F statistic is valuable because irrespective of $p$, there is only a 5 percent change that the p-value is below 0.05. In contrast, individual predictors each have that probability, so for a large number of predictors, it&#39;s very likely that we observe significant ones solely due to chance. . Sources . The hundred-page machine learning book | An introduction to statistical learning | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/stats/2021/04/10/linear-regression.html",
            "relUrl": "/ml/stats/2021/04/10/linear-regression.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Linear discriminant analysis",
            "content": "import numpy as np import pandas as pd import seaborn as sns import statsmodels.api as sm . Advantages over logistic regression: . More stable if responses are clearly separated. . | More stable if featurs are all approximately normally distributed. . | Often preferrable if response can take more than two classes. . | . Sources . An introduction to statistical learning | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/stats/2021/04/10/linear-discriminant-analysis.html",
            "relUrl": "/ml/stats/2021/04/10/linear-discriminant-analysis.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Misc learning",
            "content": "import numpy as np import scipy as sp import statsmodels.api as sm from statsmodels.formula.api import ols import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import seaborn as sns # sns.set_context(&quot;poster&quot;) # sns.set(rc={&#39;figure.figsize&#39;: (16, 9.)}) sns.set_style(&quot;whitegrid&quot;) import pandas as pd pd.set_option(&#39;display.max_rows&#39;, 50) pd.set_option(&#39;display.max_columns&#39;, 120) pd.set_option(&#39;max_colwidth&#39;, None) from sklearn.model_selection import train_test_split from sklearn.linear_model import RidgeClassifier from sklearn.linear_model import LogisticRegression from xgboost import XGBClassifier from sklearn.metrics import plot_roc_curve from sklearn.metrics import plot_precision_recall_curve from stargazer.stargazer import Stargazer from IPython.core.display import HTML from habits.cleaning import list_s3_files from habits.processing import read_o2_data, classify_users, make_yX, print_info from habits.modelling import make_stargazer, make_roc %load_ext autoreload %autoreload 2 . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-2df7d6552618&gt; in &lt;module&gt; 29 from IPython.core.display import HTML 30 &gt; 31 from habits.cleaning import list_s3_files 32 from habits.processing import read_o2_data, classify_users, make_yX, print_info 33 from habits.modelling import make_stargazer, make_roc ModuleNotFoundError: No module named &#39;habits&#39; . Creating datetime indices . from dateutil.parser import parse parse(&#39;3 Apr 2020&#39;).month parse(&#39;3.4.2020&#39;).month parse(&#39;3.4.2020&#39;, dayfirst=True).month . 4 . 3 . 4 . dates = pd.date_range(start=&#39;1/1/2000&#39;, freq=&#39;A-DEC&#39;, periods = 100) values = np.random.randn(100) ts = pd.Series(values, index=dates) ts.head(10) . 2000-12-31 0.843433 2001-12-31 -0.391251 2002-12-31 0.149087 2003-12-31 0.086131 2004-12-31 -2.308920 2005-12-31 -0.569420 2006-12-31 0.033575 2007-12-31 0.449340 2008-12-31 0.846790 2009-12-31 0.633025 Freq: A-DEC, dtype: float64 . idx = pd.period_range(&#39;2018-1&#39;, &#39;2019-12&#39;, freq=&#39;Q-DEC&#39;) s = pd.Series(np.random.randn(len(idx)), index=idx) s.asfreq(&#39;d&#39;, how=&#39;start&#39;).asfreq(&#39;Q&#39;) . 2018Q1 -0.205351 2018Q2 -0.673207 2018Q3 -0.872625 2018Q4 2.045383 2019Q1 -0.696708 2019Q2 -0.798782 2019Q3 -1.904917 2019Q4 -0.436799 Freq: Q-DEC, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;M&#39;, kind=&#39;period&#39;).mean() . 2000-01 0.014726 2000-02 0.056242 2000-03 -0.016878 2000-04 -0.690987 Freq: M, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, freq=&#39;H&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;d&#39;).ohlc() . open high low close . 2000-01-01 -1.945039 | 2.231004 | -2.751270 | 0.272018 | . 2000-01-02 0.335163 | 2.173964 | -0.895301 | 0.995886 | . 2000-01-03 0.771501 | 1.399791 | -2.465960 | -0.030100 | . 2000-01-04 1.765987 | 1.765987 | -1.698415 | -0.056522 | . 2000-01-05 0.215849 | 1.192958 | 0.215849 | 0.786069 | . s.resample(&#39;min&#39;).asfreq().ffill() . 2000-01-01 00:00:00 -1.945039 2000-01-01 00:01:00 -1.945039 2000-01-01 00:02:00 -1.945039 2000-01-01 00:03:00 -1.945039 2000-01-01 00:04:00 -1.945039 ... 2000-01-05 02:56:00 1.192958 2000-01-05 02:57:00 1.192958 2000-01-05 02:58:00 1.192958 2000-01-05 02:59:00 1.192958 2000-01-05 03:00:00 0.786069 Freq: T, Length: 5941, dtype: float64 . data = df.reset_index(level=0).head(100).sort_index()[[&#39;amount&#39;]] data . amount . transaction_date . 2012-08-29 12.00 | . 2012-08-30 13.50 | . 2012-08-30 7.44 | . 2012-08-30 7.44 | . 2012-08-30 13.50 | . ... ... | . 2012-12-27 135.00 | . 2012-12-27 3.60 | . 2012-12-27 8.50 | . 2012-12-27 135.00 | . 2012-12-27 8.50 | . 100 rows × 1 columns . IPython / notebook shortcuts . clean_nb = !ls *2* clean_nb . [&#39;2.0-fgu-clean-and-split-data.ipynb&#39;] . %%timeit a = range(1000) . 174 ns ± 4.23 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) . %%writefile pythoncode.py import numpy def append_if_not_exists(arr, x): if x not in arr: arr.append(x) def some_useless_slow_function(): arr = list() for i in range(10000): x = numpy.random.randint(0, 10000) append_if_not_exists(arr, x) . Writing pythoncode.py . %pycat pythoncode.py . import numpy def append_if_not_exists(arr, x): if x not in arr: arr.append(x) def some_useless_slow_function(): arr = list() for i in range(10000): x = numpy.random.randint(0, 10000) append_if_not_exists(arr, x) . . I can transfer variables with any content from one notebook to the next (useful if you run a number of notebooks in sequence, for instance, and the ouput of one serves as the input of another). . var_to_pass_on = dfu.head() %store var_to_pass_on . Stored &#39;var_to_pass_on&#39; (DataFrame) . %store -r var_to_pass_on var_to_pass_on . user_id year_of_birth user_registration_date salary_range postcode gender soa_lower soa_middle user_uid . 0 3706 | 1967-01-01 | 2012-09-30 | NaN | XXXX 0 | M | NaN | NaN | 3706-0 | . 1 1078 | 1964-01-01 | 2011-11-29 | 20K to 30K | M25 9 | M | E01005038 | E02001043 | 1078-0 | . 2 232 | 1965-01-01 | 2010-09-09 | 30K to 40K | CM4 0 | M | E01021551 | E02004495 | 232-0 | . 3 6133 | 1968-01-01 | 2012-10-21 | 10K to 20K | SK12 1 | M | E01018665 | E02003854 | 6133-0 | . 4 7993 | 1961-01-01 | 2012-10-29 | 20K to 30K | LS17 8 | M | E01011556 | E02002344 | 7993-0 | . !conda list | grep pandas . pandas 1.0.1 py37h6c726b0_0 pandas-flavor 0.2.0 py_0 conda-forge . Using R and Python together . # %conda install tzlocal # %conda install simplegeneric . import rpy2 . # pandas2ri.activate() . %reload_ext rpy2.ipython . %R require(ggplot2) . R[write to console]: Loading required package: ggplot2 . array([0], dtype=int32) . import pandas as pd df = pd.DataFrame({ &#39;Letter&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;c&#39;], &#39;X&#39;: [4, 3, 5, 2, 1, 7, 7, 5, 9], &#39;Y&#39;: [0, 4, 3, 6, 7, 10, 11, 9, 13], &#39;Z&#39;: [1, 2, 3, 1, 2, 3, 1, 2, 3] }) . %%R -i df ggplot(data = df) + geom_point(aes(x = X, y= Y, color = Letter, size = Z)) . R[write to console]: Error in ggplot(data = df) : could not find function &#34;ggplot&#34; Calls: &lt;Anonymous&gt; -&gt; &lt;Anonymous&gt; -&gt; withVisible R[write to console]: In addition: R[write to console]: Warning messages: R[write to console]: 1: R[write to console]: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : R[write to console]: R[write to console]: there is no package called ‘ggplot’ R[write to console]: 2: R[write to console]: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : R[write to console]: R[write to console]: there is no package called ‘ggplot2’ . Error in ggplot(data = df) : could not find function &#34;ggplot&#34; Calls: &lt;Anonymous&gt; -&gt; &lt;Anonymous&gt; -&gt; withVisible . Printing virtually anything . happy_squirrels = !ls /Users/fgu/Library/Mobile Documents/com~apple~CloudDocs/fab/photos/squirrels/ . from IPython.display import display, Image for s in happy_squirrels: display(Image(&#39;/Users/fgu/Library/Mobile Documents/com~apple~CloudDocs/fab/photos/squirrels/&#39; + s, width=100)) . Write code to and load code from file . %%writefile test.py print(&#39;Hello&#39;) . Overwriting test.py . %pycat test.py . print(&#39;Hello&#39;) . Data from wide to long and back . data = pd.read_csv(&#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/macrodata.csv&#39;) data.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name = &#39;date&#39;) columns = pd.Index([&#39;realgdp&#39;, &#39;cpi&#39;, &#39;unemp&#39;, &#39;infl&#39;], name=&#39;item&#39;) df = data.reindex(columns=columns) df.index = periods.to_timestamp(&#39;D&#39;, &#39;End&#39;) dfl = df.stack().reset_index().rename(columns={0:&#39;value&#39;}) dfl . date item value . 0 1959-03-31 23:59:59.999999999 | realgdp | 2710.349 | . 1 1959-03-31 23:59:59.999999999 | cpi | 28.980 | . 2 1959-03-31 23:59:59.999999999 | unemp | 5.800 | . 3 1959-03-31 23:59:59.999999999 | infl | 0.000 | . 4 1959-06-30 23:59:59.999999999 | realgdp | 2778.801 | . ... ... | ... | ... | . 807 2009-06-30 23:59:59.999999999 | infl | 3.370 | . 808 2009-09-30 23:59:59.999999999 | realgdp | 12990.341 | . 809 2009-09-30 23:59:59.999999999 | cpi | 216.385 | . 810 2009-09-30 23:59:59.999999999 | unemp | 9.600 | . 811 2009-09-30 23:59:59.999999999 | infl | 3.560 | . 812 rows × 3 columns . dfl.set_index([&#39;date&#39;, &#39;item&#39;]).unstack() . value . item cpi infl realgdp unemp . date . 1959-03-31 23:59:59.999999999 28.980 | 0.00 | 2710.349 | 5.8 | . 1959-06-30 23:59:59.999999999 29.150 | 2.34 | 2778.801 | 5.1 | . 1959-09-30 23:59:59.999999999 29.350 | 2.74 | 2775.488 | 5.3 | . 1959-12-31 23:59:59.999999999 29.370 | 0.27 | 2785.204 | 5.6 | . 1960-03-31 23:59:59.999999999 29.540 | 2.31 | 2847.699 | 5.2 | . ... ... | ... | ... | ... | . 2008-09-30 23:59:59.999999999 216.889 | -3.16 | 13324.600 | 6.0 | . 2008-12-31 23:59:59.999999999 212.174 | -8.79 | 13141.920 | 6.9 | . 2009-03-31 23:59:59.999999999 212.671 | 0.94 | 12925.410 | 8.1 | . 2009-06-30 23:59:59.999999999 214.469 | 3.37 | 12901.504 | 9.2 | . 2009-09-30 23:59:59.999999999 216.385 | 3.56 | 12990.341 | 9.6 | . 203 rows × 4 columns . dfl.pivot(&#39;date&#39;, &#39;item&#39;, &#39;value&#39;) . item cpi infl realgdp unemp . date . 1959-03-31 23:59:59.999999999 28.980 | 0.00 | 2710.349 | 5.8 | . 1959-06-30 23:59:59.999999999 29.150 | 2.34 | 2778.801 | 5.1 | . 1959-09-30 23:59:59.999999999 29.350 | 2.74 | 2775.488 | 5.3 | . 1959-12-31 23:59:59.999999999 29.370 | 0.27 | 2785.204 | 5.6 | . 1960-03-31 23:59:59.999999999 29.540 | 2.31 | 2847.699 | 5.2 | . ... ... | ... | ... | ... | . 2008-09-30 23:59:59.999999999 216.889 | -3.16 | 13324.600 | 6.0 | . 2008-12-31 23:59:59.999999999 212.174 | -8.79 | 13141.920 | 6.9 | . 2009-03-31 23:59:59.999999999 212.671 | 0.94 | 12925.410 | 8.1 | . 2009-06-30 23:59:59.999999999 214.469 | 3.37 | 12901.504 | 9.2 | . 2009-09-30 23:59:59.999999999 216.385 | 3.56 | 12990.341 | 9.6 | . 203 rows × 4 columns . Misc. . idx = pd.IndexSlice df.loc[idx[:, &quot;2014&quot;], :] . from scipy import stats s = np.arange(5) std = np.std(s) mean = np.mean(s) manual_z = (s - mean) / std scipy_z = stats.zscore(s) manual_z == scipy_z . array([ True, True, True, True, True]) . np.arange(32).reshape((4, 8)) . array([[ 0, 1, 2, 3, 4, 5, 6, 7], [ 8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]) . import random print(&quot;one&quot;) if random.randint(0, 1) else print(&quot;zero&quot;) . zero . ndraws = 1000 draws = np.random.randint(0, 2, ndraws) steps = np.where(draws &gt; 0, 1, -1) walk = steps.cumsum() # Find how long it took to make 10 steps in either direction idx = (np.abs(walk) &gt;= 10).argmax() print(&quot;It took {} steps to get to {}&quot;.format(idx, walk[idx])) . It took 57 steps to get to -10 . nwalks = 5000 ndraws = 1000 draws = np.random.randint(0, 2, size=(nwalks, ndraws)) steps = np.where(draws &gt; 0, 1, -1) walk = steps.cumsum(axis=1) # Find number of walks that cross 30 and the average crossing time hit30 = (np.abs(walk) &gt;= 30).any(1) crossing_times = (np.abs(walk[hit30]) &gt;= 30).argmax(1) print( &quot;{} walks cross 30, taking {} steps on average.&quot;.format( hit30.sum(), crossing_times.mean() ) ) . 3385 walks cross 30, taking 498.33353028064994 steps on average. . Grouping . df = pd.DataFrame({&#39;key1&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], &#39;key2&#39;: [&#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;], &#39;data1&#39;: np.random.randn(5), &#39;data2&#39;: np.random.randn(5)}) df . key1 key2 data1 data2 . 0 a | one | -0.722346 | 1.350070 | . 1 b | two | -0.864278 | 0.652911 | . 2 c | one | 0.875035 | 0.838726 | . 3 d | two | 1.420677 | -0.464896 | . 4 e | one | -0.789309 | 0.148121 | . for group, data in df.groupby(&#39;key1&#39;): print(group) print(data) . a key1 key2 data1 data2 0 a one -0.722346 1.35007 b key1 key2 data1 data2 1 b two -0.864278 0.652911 c key1 key2 data1 data2 2 c one 0.875035 0.838726 d key1 key2 data1 data2 3 d two 1.420677 -0.464896 e key1 key2 data1 data2 4 e one -0.789309 0.148121 . pieces = dict(list(df.groupby(&#39;key1&#39;))) pieces[&#39;a&#39;] . key1 key2 data1 data2 . 0 a | one | -0.722346 | 1.35007 | . grouped = df.groupby(df.dtypes, axis=1) for dtype, data in grouped: print(dtype) print(data) . float64 data1 data2 0 -0.722346 1.350070 1 -0.864278 0.652911 2 0.875035 0.838726 3 1.420677 -0.464896 4 -0.789309 0.148121 object key1 key2 0 a one 1 b two 2 c one 3 d two 4 e one . Solution to my common indexing problem (index with shorter boolean series) . Problem: I want to index my df based on a boolean series that is shorter than the length of the df. E.g. I have a subset of users that fulfill a condition and want to keep these only. . data = o2.sample(frac=0.05) . high_spender = data.groupby(&quot;user_id&quot;).amount.sum() &gt; 350 . . todrop = high_spender[~high_spender].index.values data.drop(todrop, level=0).sort_index() . transaction_id transaction_description amount auto_tag merchant_name account_type . user_id transaction_date . 20912 2013-12-18 12152457 | card purchase o2 uk ref xxx xxxxxx0000 bcc | 11.990000 | mobile | o2 | current | . 2014-04-17 17466459 | direct debit o2 gedxxxxx154 ddr | 44.160000 | mobile | o2 | current | . 2015-01-19 54324542 | direct debit o2 gedxxxx9154 ddr | 38.380001 | mobile | o2 | current | . 2015-03-24 80422755 | direct debit o2 gedxxxx9097 ddr | 34.799999 | mobile | o2 | current | . 2015-09-23 96952964 | counter credit &lt;mdbremoved&gt; o2 bgc | 54.500000 | mobile | o2 | current | . 2016-04-25 134730196 | direct debit o2 gedxxxx9097 ddr | 40.990002 | mobile | o2 | current | . 2016-05-17 138708140 | direct debit o2 gedxxxx1717 ddr | 30.000000 | mobile | o2 | current | . 2016-08-17 154735556 | direct debit o2 gedxxxx9154 ddr | 34.270000 | mobile | o2 | current | . 2016-10-07 163699913 | counter credit &lt;mdbremoved&gt; o2/ &lt;mdbremoved&gt; bgc | 68.059998 | mobile | o2 | current | . 160992 2014-09-29 38730708 | o2 uk cd 9531 | 25.000000 | mobile | o2 | current | . 2015-01-19 54406685 | o2 uk cd 9531 | 175.000000 | mobile | o2 | current | . 2015-01-19 54406686 | o2 uk cd 9531 | 202.539993 | mobile | o2 | current | . high_spender . user_id 8 False 659 False 1078 False 1146 False 2324 False ... 420102 False 421678 False 423912 False 424865 False 425830 False Name: amount, Length: 210, dtype: bool . hs = high_spender[high_spender].index.values mask = data.index.isin(hs, level=0) data[mask].sort_index() . transaction_id transaction_description amount auto_tag merchant_name account_type . user_id transaction_date . 20912 2013-12-18 12152457 | card purchase o2 uk ref xxx xxxxxx0000 bcc | 11.990000 | mobile | o2 | current | . 2014-04-17 17466459 | direct debit o2 gedxxxxx154 ddr | 44.160000 | mobile | o2 | current | . 2015-01-19 54324542 | direct debit o2 gedxxxx9154 ddr | 38.380001 | mobile | o2 | current | . 2015-03-24 80422755 | direct debit o2 gedxxxx9097 ddr | 34.799999 | mobile | o2 | current | . 2015-09-23 96952964 | counter credit &lt;mdbremoved&gt; o2 bgc | 54.500000 | mobile | o2 | current | . 2016-04-25 134730196 | direct debit o2 gedxxxx9097 ddr | 40.990002 | mobile | o2 | current | . 2016-05-17 138708140 | direct debit o2 gedxxxx1717 ddr | 30.000000 | mobile | o2 | current | . 2016-08-17 154735556 | direct debit o2 gedxxxx9154 ddr | 34.270000 | mobile | o2 | current | . 2016-10-07 163699913 | counter credit &lt;mdbremoved&gt; o2/ &lt;mdbremoved&gt; bgc | 68.059998 | mobile | o2 | current | . 160992 2014-09-29 38730708 | o2 uk cd 9531 | 25.000000 | mobile | o2 | current | . 2015-01-19 54406685 | o2 uk cd 9531 | 175.000000 | mobile | o2 | current | . 2015-01-19 54406686 | o2 uk cd 9531 | 202.539993 | mobile | o2 | current | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/04/10/learning.html",
            "relUrl": "/python/pandas/2021/04/10/learning.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Iterators and generators",
            "content": "Writing my own iterator pattern . (From the Python Cookbook recipee 4.3) . def frange(start, stop, increment): x = start while x &lt; stop: yield x x += increment rng = frange(1, 10, 2) next(rng) next(rng) . 3 . list(rng) . [5, 7, 9] . I&#39;m a little confused by this still. . def aritprog(begin, step, end=None): result = type(begin + step)(begin) forever = end is None index = 0 while forever or result &lt; end: yield result index += 1 result = begin + step * index a = aritprog(0, 5, 20) for a in a: print(a) . 0 5 10 15 . a = iter([1, 2, 3]) . import inspect def gen(x): yield x a = gen(5) print(inspect.getgeneratorstate(a)) next(a) print(inspect.getgeneratorstate(a)) try: next(a) except StopIteration: print(inspect.getgeneratorstate(a)) . GEN_CREATED GEN_SUSPENDED GEN_CLOSED . Using generators for line-by-line data processing for large files . Example here, see &quot;Case Study: Generators in a Database Conversion Utility&quot; at end of Chap 14 in Fluent Python for context. . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/04/10/iterators-and-generators.html",
            "relUrl": "/python/2021/04/10/iterators-and-generators.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Python data science idioms",
            "content": "When working with data, certain basic steps occur over and over. This is the place where I document my current best-practices. . from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Reorder dataframe columns . first = [&#39;date&#39;, &#39;yw&#39;, &#39;pcsector&#39;] rest = set(df.columns) - set(first) df = df[first + list(rest)] . Select a subset of users from a panel dataset based on user-level criteria . I have a folder of files which I want to clean and append. How to do this? | I want to convert a df column to datatime, how to do this (at read, using np, using Pandas)? What are tradeoffs? | . Misc. . Use ast.literal_eval() instead of eval() . Basically, because eval is very dangerous and would happile evaluate a string like os.system(rm -rf /), while ast.literal_eval will only evaluate Python literals. . Sources . Fluent Python | Python Cookbook | Learning Python | The Hitchhiker&#39;s Guide to Python | Effective Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/04/10/idioms.html",
            "relUrl": "/python/2021/04/10/idioms.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Predicting happiness of Cypriots",
            "content": "Examples from the hands on machine learning book by Aurélien Geron. . import pandas as pd import numpy as np import matplotlib.pyplot as plt . Load data . def load_bli(): bli = pd.read_csv(&#39;./data/oecd-bli.csv&#39;, thousands=&#39;,&#39;) bli = bli.iloc[:, [0, 1, 3, 14]] bli = bli.rename(columns={&#39;LOCATION&#39;: &#39;country_code&#39;}) bli = bli.pivot_table(&#39;Value&#39;, [&#39;country_code&#39;, &#39;Country&#39;], &#39;Indicator&#39;) bli = bli[[&#39;Life satisfaction&#39;]] bli = bli.rename(columns={&#39;Life satisfaction&#39;: &#39;life_satisfaction&#39;}) return bli.reset_index().rename(columns=str.lower) def load_gdp(): gdppc = pd.read_csv(&#39;./data/oecd-gdppc.csv&#39;, thousands=&#39;,&#39;) gdppc = gdppc[gdppc.TIME == 2019] gdppc = gdppc.rename(columns=str.lower) gdppc = gdppc.rename(columns={&#39;location&#39;: &#39;country_code&#39;, &#39;value&#39;: &#39;gdppc&#39;}) return gdppc[[&#39;country_code&#39;, &#39;gdppc&#39;]] def make_dataset(bli, gdppc): bli = bli.merge(gdppc) bli = bli.sort_values(&#39;gdppc&#39;, ascending=False) drop_idx = [0] keep_idx = list(set(bli.index) - set(drop_idx)) return bli.iloc[keep_idx] bli = load_bli() gdppc = load_gdp() data = make_dataset(bli, gdppc) X = data[[&#39;gdppc&#39;]] y = data[&#39;life_satisfaction&#39;] data.head() . country_code country life_satisfaction gdppc . 17 IRL | Ireland | 7.05 | 90140.764256 | . 4 CHE | Switzerland | 7.50 | 70485.014502 | . 28 NOR | Norway | 7.62 | 69169.203192 | . 38 USA | United States | 7.00 | 65126.619982 | . 27 NLD | Netherlands | 7.50 | 59419.863105 | . Plot relationship of interest . import seaborn as sns sns.set_style(&#39;whitegrid&#39;) data.plot(kind=&#39;scatter&#39;, x=&#39;gdppc&#39;, y=&#39;life_satisfaction&#39;, c=&#39;blue&#39;); . Model and predict . from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsRegressor cyprus_gdp = [[22_587]] lr = LinearRegression() lr.fit(X, y) print(f&#39;Linear regression: {lr.predict(cyprus_gdp)[0]:.1f}&#39;) knn1 = KNeighborsRegressor(n_neighbors=1) knn1.fit(X, y) print(f&#39;KNN, k=1: {knn1.predict(cyprus_gdp)[0]:.1f}&#39;) knn3 = KNeighborsRegressor(n_neighbors=3) knn3.fit(X, y) print(f&#39;KNN, k=3: {knn3.predict(cyprus_gdp)[0]:.1f}&#39;) . Linear regression: 5.9 KNN, k=1: 6.5 KNN, k=3: 6.2 .",
            "url": "https://fabiangunzinger.github.io/blog/python/stats/2021/04/10/handson-examples.html",
            "relUrl": "/python/stats/2021/04/10/handson-examples.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Fast groupby operations",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext line_profiler %load_ext autoreload %autoreload 2 . path = &#39;/Users/fgu/tmp/mdb/data_777.parquet&#39; df = pd.read_parquet(path) print(df.shape) df.head() . (157287, 22) . user_id transaction_date amount transaction_description merchant_name tag gender up_tag account_id year_of_birth merchant_business_line salary_range latest_balance account_type credit_debit transaction_id bank postcode ym account_created user_registration_date account_last_refreshed . 0 777 | 2012-01-03 | 3.03 | aviva pa - d/d | aviva | life insurance | m | life insurance | 262916 | 1969.0 | aviva | 20k to 30k | 364.22 | current | debit | 688262 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 1 777 | 2012-01-03 | 6.68 | 9572 31dec11 , tesco stores 3345 , warrington gb - pos | tesco | food, groceries, household | m | food, groceries, household | 262916 | 1969.0 | tesco supermarket | 20k to 30k | 364.22 | current | debit | 688263 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 2 777 | 2012-01-03 | 10.27 | 9572 30dec11 , mcdonalds , restaurant , winwick road gb - pos | mcdonalds | dining and drinking | m | dining and drinking | 262916 | 1969.0 | mcdonalds | 20k to 30k | 364.22 | current | debit | 688264 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 3 777 | 2012-01-03 | 12.00 | 9572 31dec11 , tesco stores 3345 , warrington gb - pos | tesco | food, groceries, household | m | food, groceries, household | 262916 | 1969.0 | tesco supermarket | 20k to 30k | 364.22 | current | debit | 688265 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 4 777 | 2012-01-03 | 400.00 | &lt;mdbremoved&gt; - s/o | no merchant | other account | m | other account | 262916 | 1969.0 | non merchant mbl | 20k to 30k | 364.22 | current | debit | 688261 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . Basics . Boolean comparisons . df = sns.load_dataset(&#39;iris&#39;) . %timeit df.sepal_width &gt; 3 . 105 µs ± 2.61 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) . %timeit df.sepal_width.values &gt; 0 . 6.11 µs ± 181 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) . Given the above, the below is rather surprising: . %timeit df[df.sepal_width &gt; 3] . 335 µs ± 23.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . %timeit df[df.sepal_width.values &gt; 3] . 148 µs ± 6.14 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) . Group means . From https://cmdlinetips.com/2019/05/how-to-implement-pandas-groupby-operation-with-numpy/ . df = sns.load_dataset(&#39;iris&#39;) . %timeit df.groupby(&#39;species&#39;).sepal_length.mean() . 481 µs ± 11.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . %%timeit spec = df.species.values sl = df.sepal_length.values groups = df.species.unique() [(group, np.mean(sl[spec == group])) for group in groups] . 111 µs ± 7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/04/10/fast-groupby.html",
            "relUrl": "/python/pandas/2021/04/10/fast-groupby.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://fabiangunzinger.github.io/blog/jupyter/2021/04/10/example-post.html",
            "relUrl": "/jupyter/2021/04/10/example-post.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Concurrency",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Theory . Elements: . RAM | memory controller (interface between cup and ram) | CPU | Kernel . Lowest level of software. It manages cpu resources, file system, memory resources, drivers, networking, etc. | . | Program . Code and data structures that can be executed (like newspaper) | . | . Cores (units of cpu?) . Multi core: instead of dividing up cpu time among different processes, can actually run stuff in parallel, but within each core, the process is exactly the same as with only one core. | . | Process . A logical container that tells kernel what program to run. Process has id, priority, status, memory space, etc.. Kernel uses that info to allocate cpu time to each program. Because happens at milisecond level, creates illusion of concurrency (comic analogy). | A program that competes for CPU resources (e.g. a web browser, or a bit of cleaning code) | The program in execution. Given that program is composed of multiple threads, program is program + state of all threads executing the program | . | Threads . Threads run within a single process, to allow it to do multiple things at once (like a newspaper) | They are lines of control cursing through the code and data structures of the program (called a thread of execution through the program). Akin to one reader scanning through a section of the newspaper. | Multiple threads: different lines of control cursing through the program (multiple readers reading different sections of the newspaper). | Potential conflict: threads operate on (read/update) same data structures. Solution: locks | A ... running within a program (e.g. reading data, processing, writing to disk) | . | . Type of tasks: . CPU-bound tasks: using CPU to crunch numbers . | I/O-bound tasks: waiting for some I/O operations to complete (downloading files from internet, reading and writing to file-system) . | . Theading: . Gives speed ups for I/O-bound tasks | . Multiprocessing: . Run CPU-bound tasks in prallel | . Usage . os.cpu_count() . 8 . Sources . Udacity on process vs threads | Gary explains - processes and threads | Gary explains - what is a kernel | Corey Schafer on threading | Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/04/10/concurrency.html",
            "relUrl": "/python/2021/04/10/concurrency.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Beautiful code",
            "content": "Sources to look at: . Hitchhiker&#39;s guide (link below) | IPython cookbook | . &quot;Terseness and obscurity are the limits where brevity should stop&quot; -- The Hitchhiker&#39;s Guide to Python . Use common design patterns . Use extra variables and functions instead of comments . Naming . Be descriptive. | Use plain and unabbreviated words. | Be concise and omit needless words (e.g. &quot;get&quot;, &quot;calculate&quot;) | . Add a docstring to each function . I roughly follow PEP-257 and Google. | Omit docstring if function is short and obvious (the one below would easily qualify...). | . def sum_numbers(num1=0, num2=1): &quot;&quot;&quot;Sum two numbers. Arguments: num1: the first number (default, 0) num2: the second number (default, 1) Return: Sum of numbers. &quot;&quot;&quot; return num1 + num2 . One function performs one task . import pandas as pd df = pd.DataFrame({&#39;data&#39;: [1, 2, 3, 4]}) # bad def calc_and_print_stats(df): mean = df.data.mean() maximum = df.data.max() print(f&#39;Mean is {mean}&#39;) print(f&#39;Max is {maximum}&#39;) calc_and_print_stats(df) . Mean is 2.5 Max is 4 . def calc_stats(df): return df.data.mean(), df.data.max() def print_stats(stats): print(f&#39;Mean is {stats[0]}&#39;) print(f&#39;Max is {stats[1]}&#39;) stats = calc_stats(df) print_stats(stats) . Mean is 2.5 Max is 4 . Principles . Based on this . 1. . Return a value | Such as True, to show that the function completed when there is nothing obvious to return. . Keep them short | Less than 50 lines as a rule of thumb. . Idempotent and pure | Idempotent functions return the same output for a given input every time, pure functions are idempotent and have no side-effects. . Main sources . The Hitchhiker&#39;s Guide to Python, code style | IPython cookbook, writing high-quality Python code | Google style guide | Jeff Knupp post | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/04/10/beautiful-code.html",
            "relUrl": "/python/2021/04/10/beautiful-code.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Bayesian statistics",
            "content": "import numpy as np from scipy import stats np.random.seed(1) # for repeatability F_true = 1000 # true flux, say number of photons measured in 1 second N = 50 # number of measurements F = stats.poisson(F_true).rvs(N) # N measurements of the flux e = np.sqrt(F) # errors on Poisson counts estimated via square root . import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.errorbar(F, np.arange(N), xerr=e, fmt=&#39;ok&#39;, ecolor=&#39;gray&#39;, alpha=0.5) ax.vlines([F_true], 0, N, linewidth=5, alpha=0.2) ax.set_xlabel(&quot;Flux&quot;);ax.set_ylabel(&quot;measurement number&quot;); . w = 1 / e ** 2 print(&quot;&quot;&quot; F_true = {0} F_est = {1:.0f} +/- {2:.0f} (based on {3} measurements) &quot;&quot;&quot;.format(F_true, (w * F).sum() / w.sum(), w.sum() ** -0.5, N)) . F_true = 1000 F_est = 998 +/- 4 (based on 50 measurements) .",
            "url": "https://fabiangunzinger.github.io/blog/stats/ml/2021/04/10/bayesian_stats.html",
            "relUrl": "/stats/ml/2021/04/10/bayesian_stats.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Ssh",
            "content": "Mounting server on my mac . I want to mount home/fgu/dev/remote (which I’m gonna call aws_remote from my virtual machine onto Users/fgu/dev/remote (mac_remote) on my mac. I can do this like so: . sshfs fgu@$te_ip:$te_remote $mac_remote -o identityfile=$mac_pem&#39; . What happens here: . mac_pem contains the private key for a ssh keypair that was generated on my AWS virtual machine. So, in this exchange, | .",
            "url": "https://fabiangunzinger.github.io/blog/2021/03/31/ssh.html",
            "relUrl": "/2021/03/31/ssh.html",
            "date": " • Mar 31, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Principal component analysis",
            "content": "from datetime import datetime import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns; sns.set() import statsmodels.api as sm . def make_data(): &quot;&quot;&quot;Fetch and prepare data used for examples.&quot;&quot;&quot; fp = &#39;s3://fgu-samples/transactions.parquet&#39; cols = [&#39;amount&#39;, &#39;user_id&#39;, &#39;year_of_birth&#39;, &#39;gender&#39;] df = pd.read_parquet(fp, columns=cols) df = df[df.gender.isin([&#39;m&#39;, &#39;f&#39;])] df[&#39;age&#39;] = datetime.now().year - df.year_of_birth df[&#39;male&#39;] = df.gender == &#39;m&#39; df = df.rename(columns={&#39;amount&#39;: &#39;spend&#39;}) g = df.groupby(&#39;user_id&#39;) df = g.agg({&#39;spend&#39;: &#39;sum&#39;, &#39;age&#39;: &#39;first&#39;, &#39;male&#39;: &#39;first&#39;}) return df df = make_data() df.head(3) . spend age male . user_id . 977 36665.800781 | 48.0 | True | . 3277 17830.087891 | 34.0 | False | . 4277 -24827.089844 | 62.0 | True | . PCA basics . from sklearn.decomposition import PCA pca = PCA(n_components=1) pca.fit(df) . PCA(n_components=1) . The fitted PCA&#39;s component attribute contains the loading vectors of each principal component, which, in turn, contain the loadings for each of the original features. . pca.components_ . array([[ 1.00000000e+00, 1.30713265e-05, -5.78356798e-07]]) . By definition, their squares have to sum to zero. Let&#39;s check: . np.sum(np.square(pca.components_)) . 1.0 . Let&#39;s transform the data . df_pca = pca.transform(df) df_pca[:5] . array([[ 21732.90091993], [ 2897.18784849], [-39759.98951681], [ 72551.13144968], [-21768.62012473]]) . We could have done this manually, like to: . ((df - df.mean()) * pca.components_[0]).sum(1)[:5] . user_id 977 21732.900508 3277 2897.187436 4277 -39759.989929 5377 72551.131038 7077 -21768.620537 dtype: float64 . Dimensionality reduction . plt.scatter(data=df, x=&#39;age&#39;, y=&#39;spend&#39;, alpha=0.8); . pca = PCA(n_components=1) df_pca = pca.fit_transform(df) df_new = pca.inverse_transform(df_pca) . plt.scatter(data=df, x=&#39;age&#39;, y=&#39;spend&#39;, alpha=0.2); plt.scatter(df_new[:, 1], df_new[:, 0], alpha=0.6); . Sources . The hundred-page machine learning book | An introduction to statistical learning | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/stats/clean/2021/03/28/principal-component-analysis.html",
            "relUrl": "/ml/stats/clean/2021/03/28/principal-component-analysis.html",
            "date": " • Mar 28, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Git(Hub) essentials",
            "content": "Sharing part of a repo with collaborator . Problem . I have a private repo that contains code as well as confidential information on a project. I want to share the code, but not the confidential information, with a collaborator. . Solution .",
            "url": "https://fabiangunzinger.github.io/blog/git/2021/03/28/github-essentials.html",
            "relUrl": "/git/2021/03/28/github-essentials.html",
            "date": " • Mar 28, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "vim essentials",
            "content": "Setup . I’ve remaped the Caps Look key to . | . Mental models . One keystroke to move, one to execute: e.g. the dot-formula (PV p. 11) | Chunk your undos; all changes in single insert-mode session count as a single change, so go in and out of insert mode strategically. | . Modes . Normal mode . * to highlight word under cursor. Use n and N to cycle forwards and backwards through matches. | &lt;C-a&gt; and &lt;C-x&gt; to add and subtract from the next number. | . Operators . Operator + motion = action. E.g. dl deletes character to the right, diw the word under the cursor, dap the current paragraph. Similarly, gUap converts the current paragraph to uppercase. | . Trigger | Effect | . c | Change | . d | Delete | . y | Yank into register | . g~ | Swap case | . gu | Make lowercase | . gU | Make uppercase | . &gt; | Shift right | . &lt; | Shift left | . = | Autoindent | . ! | Filter {motion} lines through an external program | . Act, repeat, reverse . Intent | Act | Repeat | Reverse | . Make a change | {edit} | . | u | . Scan line for next character | f{char}/t{char} | ; | , | . Scan line for previous character | F{char}/T{char} | ; | , | . Scan document for next match | /pattern | n | N | . Scan document for previous match | ?pattern | n | N | . Perform substitution | :s/old/new | &amp; | u | . Execute a sequence of changes | qx{change}q | @x | u | . Compound commands . Compound command | Equivalent in longhand | . C | c$ | . s | cl | . S | ^C | . I | ^i | . A | $a | . o | A | . O | ko | . Insert mode . Useful keystrokes: . &lt;C-w&gt; to delete last few words without leaving insert mode | &lt;C-o&gt;zz to move current line to middle of screen without leaving insert mode | . Keystroke | Action . | Delete back one character (backspace) | Delete back one word | Delete back one line | Switch to Insert Normal mode (to execute a single Normal Mode command) {register} | Paste content from address (use 0 for last yanked text) = | Perform calculation in place r, R | Enter replace mode for single replacement or until exit ## Visual mode ## Command-line mode ### Ex commands - basic syntax: [range]command # Sources - [Practical Vim (PV)](https://pragprog.com/titles/dnvim2/practical-vim-second-edition/)",
            "url": "https://fabiangunzinger.github.io/blog/vim/2021/03/27/vim-essentials.html",
            "relUrl": "/vim/2021/03/27/vim-essentials.html",
            "date": " • Mar 27, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Tree based methods",
            "content": "Trees Are an easy and intuitive way to split data within a sample.  | Problem, they are not good at predicting out-of-sample (with different datasets)   | . | Random Forests  . Random forests remedy this by combining the simplicity of decision trees with flexibility, which leads to a large improvement in predictive accuracy.  | How to make a random forest:  Create bootstrapped sample from data (i.e. sample observations of the original sample with replacement)  | Create a decision tree using only a subset of randomly selected variables at each step (e.g. only 2 out of 4 for root, then 2 out of remaining 3 at next node, ect.)  | Repeat above two steps many times (e.g. 1000) to build many trees (and build a random forest)  | . | To predict outcome for new observation, do the following:  Feed data into each tree in the forest and keep score of the predictions (either Yes or No for each tree). The outcome with the most scores is the prediction.  | . | The process is called “Bagging” because we Bootstrap the data and rely on the AGGregate to make a decision.  | How can we test how good a tree is at out-of sample prediction without having another sample?  Bootstrapping relies on randomly sampling from data with replacement, hence, not all observations will be used to create a tree.  | The unused observations are called the “Out-of-bag Dataset”. We can use these test whether our Forest is any good at predicting.  | We simply take the out-of-bag dataset from each tree, run through the entire Forest and check whether the Forest accurately classifies the observation. We then repeat this for each out-of-bag dataset.  | The proportion of incorrectly classified out-of-bag samples is called the “out-of-bag error”.  | . | The out-of-bag error is what helps us determine how many variables to use when building our random trees above. The algorithm builds different forests with different numbers of variables (typically starting with the square-root of the total number of variables – e.g. 2 if we have 4 variables – and then calculating a few above and below that) and then picks the one with the smallest out-of-bag error.   | . | Ada boosts  . When building random forests, trees vary in their depth.  | When using Ada boost to create a Random Forest, each tree is usually just one node and two leaves. (A tree with one node and two leaves is a “stump”). So, Ada boost produces a Forest of Stumps.  | Because a stump only makes use of a single variable, they are generally poor predictors.  | Main ideas of ada boost  Take Forest of Stumps  | Stumps have different weights (mounts of say) in the calculation of the out-of-bag error (with the weights being proportional to the gravity of the prediction errors they make. Loosely speaking, for how many observations they get the prediction wrong).   | Each stump takes the errors of the previous stump into account (it does this by treating as more important those observations that the previous stump misclassified).   | . | Process  Create first Stump using the variable that best classifies outcomes  | Then calculate classification error  | The size of that error determines the weight this stump gets in the overall classification (i.e. in the Forest of Stumps).  | The next stump will be build using the variable that best classifies outcomes in a dataset that over-emphasizes the observations that the previous stump misclassified.  | As in a Random Forst, we run all obseravtions through all Stumps and keep track of the classification. Instead of adding up the Yes and No, we add up the amount of say of the Yes Stumps and No Stumps and classify as Yes if total amount of say of yes Stumps is larger.  | . | . | Gradient boosting (most used configuration)  . Comparison to Ada boost  Like Ada boost builds fixed size trees, but they can be larger than a Stump (in our specification, we use trees with a depth of 5)  | GB also scales trees, but all by same amount  | Also builds tree based on error of previous tree  | . | Algorithm  Predict based on average and calculate (pseudo residuals)  | Then build a tree to predict residuals  | Scale predicted residual by the learning rate (we use 0.1) and add to original prediction.  | Calculate new pseudo residuals and build new tree to predict.  | Add scaled predictions to the previous prediction (i.e. to the original prediction and the previous scaled prediction).  | Keep going like this until additional trees no longer improve prediction or hit number of max trees.  | . | . | Initial prediction is log of odds: log(number in HE/number not in HE) and convert to a probability using the logistic function (e^logodds / 1 + e^logodds). If probability &gt; 0.5, prediction is “Yes” for all observations. Else is no  . | Calculate pseudo residuals as actual - predicted (e.g. 1 - 0.7)  | Build tree to predict residuals | .",
            "url": "https://fabiangunzinger.github.io/blog/python/ml/stats/2021/03/21/tree-based-methods.html",
            "relUrl": "/python/ml/stats/2021/03/21/tree-based-methods.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Logistic regression",
            "content": "Advantages over linear regression . For more than two outcome values (e.g. a, b, c): . We can use linear regression and classify outcomes as 1, 2, 3, but we then have to assume that (1) there is a natural ordering between the values, and (2) the distances between them are the same, which is often not true. . | Furthermore, we can get predictions outside the interval [0, 1], which is nonsensical. . | With only two possible outcomes, only the latter is a problem. . | . | . The (general) logistic function is a curve with equation: . $$f(x) = frac{L}{1 + e^{-k(x - x_0)}},$$ . where $L$ is the asymptotic upper value of the curve, $e$ is the base of the natural logarithm, $k$ is the steepness of the curve, and $x_0$ is its midpoint. . The standard logistic function, with $L = k = 1$ and $x_0 = 0$ maps values from minus infinity to plus infinity to between 0 and 1. . def general_logistic(x, L, k, x0): return L / (1 + np.exp(-k*(x - x0))) x = np.linspace(-domain, domain, 100) fig, (ax0, ax1) = plt.subplots(1, 2, sharey=True, figsize=(10, 4)) ax0.plot(x, general_logistic(x, 1, 1, 0)) ax0.axvline(0, color=&#39;orange&#39;, linestyle=&#39;dashed&#39;) ax0.set(title=&#39;Standard logistic function&#39;, xlabel=&#39;$x$&#39;, ylabel=&#39;$y$&#39;) ax1.plot(x, general_logistic(x, 1, 4, 0)) ax1.axvline(0, color=&#39;orange&#39;, linestyle=&#39;dashed&#39;) ax1.set(title=&#39;Steep logistic function&#39;, xlabel=&#39;$x$&#39;, ylabel=&#39;$y$&#39;); .",
            "url": "https://fabiangunzinger.github.io/blog/python/stats/2021/03/21/logistic-regression.html",
            "relUrl": "/python/stats/2021/03/21/logistic-regression.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Linear regression",
            "content": "import numpy as np import pandas as pd import seaborn as sns import statsmodels.api as sm . Notes: . When will regression do well: when the underlying model is linear. Check KNN vs linear regression comparison. | . Example . path = &#39;https://www.statlearning.com/s/Advertising.csv&#39; cols = [&#39;TV&#39;, &#39;radio&#39;, &#39;newspaper&#39;, &#39;sales&#39;] df = (pd.read_csv(path, usecols=cols) .rename(lambda col: col.lower(), axis=1) .reset_index() .rename(columns={&#39;index&#39;: &#39;market&#39;}) ) print(df.shape) df.head(3) . (200, 5) . market tv radio newspaper sales . 0 0 | 230.1 | 37.8 | 69.2 | 22.1 | . 1 1 | 44.5 | 39.3 | 45.1 | 10.4 | . 2 2 | 17.2 | 45.9 | 69.3 | 9.3 | . tidy = (df.melt(id_vars=[&#39;market&#39;, &#39;sales&#39;], var_name=&#39;channel&#39;, value_name=&#39;budget&#39;)) sns.lmplot(x=&#39;budget&#39;, y=&#39;sales&#39;, hue=&#39;channel&#39;, col=&#39;channel&#39;, sharex=False, data=tidy); . Estimating coefficients . mod = sm.OLS.from_formula(&#39;sales ~ tv + radio + newspaper&#39;, data=df) res = mod.fit() print(res.summary()) . OLS Regression Results ============================================================================== Dep. Variable: sales R-squared: 0.897 Model: OLS Adj. R-squared: 0.896 Method: Least Squares F-statistic: 570.3 Date: Mon, 15 Mar 2021 Prob (F-statistic): 1.58e-96 Time: 15:43:14 Log-Likelihood: -386.18 No. Observations: 200 AIC: 780.4 Df Residuals: 196 BIC: 793.6 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] Intercept 2.9389 0.312 9.422 0.000 2.324 3.554 tv 0.0458 0.001 32.809 0.000 0.043 0.049 radio 0.1885 0.009 21.893 0.000 0.172 0.206 newspaper -0.0010 0.006 -0.177 0.860 -0.013 0.011 ============================================================================== Omnibus: 60.414 Durbin-Watson: 2.084 Prob(Omnibus): 0.000 Jarque-Bera (JB): 151.241 Skew: -1.327 Prob(JB): 1.44e-33 Kurtosis: 6.332 Cond. No. 454. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . Evaluating accuracy of coefficient estimates . Hypothesis testing | . Evaluating accuracy of model . Residual standard error . The RSE is an estimate of the irreducible error, $ epsilon$, and thus meansures how much, on average, the response will deviate from the true regression line. . | The RSE is 3.25, which implies that our estimates deviate about 3.25 from the actual values (this would be true even if we knew the population parameters, as the RSE is an estimate of the error standard deviation). Given the average value of sales, the percentage error is about 12 percent. Whether this is a lot or not depends on the application. . | Becaue the RSE is an absolute measure of lack of fit, expressed in units of y, it&#39;s not always easy to interpret whether a given RSE is small or large. . | . rse = np.sqrt(res.scale) x_mean = df.sales.mean() print(rse) print(rse/ x_mean) print(res.rsquared) . 1.685510373414744 0.1202004188564624 0.8972106381789522 . $R^2$ . $R^2$, which is a relative measure of lack of fit, and measures the percentage of variance in y that the model can explain (and is thus always between 0 and 1). In the simple linear regression setting, $R^2 = Cor(X, Y)^2$. . | A low $R^2$ can mean that the true relationship is non-linear or that the error variance is very high or both. What constitutes &quot;low&quot; depends on the application. . | In the model above, more than 90 percent of the variation is explained by the set of explanatory variables. . | . Multiple linear regression . Estimating the coefficients . Questions of interest . Is there a relationship between the response and predictors? . To test whether at least one of the predictors is useful in predicting the response, we can look at the reported F statistic. | . res.fvalue, res.f_pvalue . (570.2707036590942, 1.575227256092437e-96) . To test whether a subset of parameters is useful, we can run our own F-test. To manually test for all parameters, we can use: | . a = np.identity(len(res.params))[1:] res.f_test(a) . &lt;class &#39;statsmodels.stats.contrast.ContrastResults&#39;&gt; &lt;F test: F=array([[570.27070366]]), p=1.5752272560925203e-96, df_denom=196, df_num=3&gt; . Which is equivalent to the statistic provided in the output. To test the (joint) usefulness of radio and newspaper, we can use: | . a = np.identity(len(res.params))[[2, 3]] res.f_test(a) . &lt;class &#39;statsmodels.stats.contrast.ContrastResults&#39;&gt; &lt;F test: F=array([[272.04067681]]), p=2.829486915701129e-57, df_denom=196, df_num=2&gt; . Remember: the F statistic is valuable because irrespective of $p$, there is only a 5 percent change that the p-value is below 0.05. In contrast, individual predictors each have that probability, so for a large number of predictors, it&#39;s very likely that we observe significant ones solely due to chance. | . Are all of the predictors or only a subset useful in explaining y? . Application . rse = np.sqrt(res.scale) x_mean = df.sales.mean() print(rse) print(rse/ x_mean) print(res.rsquared) . 1.685510373414744 0.1202004188564624 0.8972106381789522 . The RSE is 3.25, which implies that our estimates deviate about 3.25 from the actual values (this would be true even if we knew the population parameters, as the RSE is an estimate of the error standard deviation). Given the average value of sales, the percentage error is about 12 percent. Whether this is a lot or not depends on the application. . | Becaue the RSE is an absolute measure of lack of fit, expressed in units of y, it&#39;s not always easy to interpret whether a given RSE is small or large. . | $R^2$, which is a relative measure of lack of fit, and measures the percentage of variance in y that the model can explain (and is thus always between 0 and 1). In the simple linear regression setting, $R^2 = Cor(X, Y)^2$. . | A low $R^2$ can mean that the true relationship is non-linear or that the error variance is very high or both. What constitutes &quot;low&quot; depends on the application. . | In the model above, more than 90 percent of the variation is explained by the set of explanatory variables. . | . res.fvalue, res.f_pvalue . (570.2707036590942, 1.575227256092437e-96) . a = np.identity(len(res.params))[[2, 3]] res.f_test(a) . &lt;class &#39;statsmodels.stats.contrast.ContrastResults&#39;&gt; &lt;F test: F=array([[272.04067681]]), p=2.829486915701129e-57, df_denom=196, df_num=2&gt; . Remember: the F statistic is valuable because irrespective of $p$, there is only a 5 percent change that the p-value is below 0.05. In contrast, individual predictors each have that probability, so for a large number of predictors, it&#39;s very likely that we observe significant ones solely due to chance. . Sources . The hundred-page machine learning book | An introduction to statistical learning | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/stats/2021/03/21/linear-regression.html",
            "relUrl": "/ml/stats/2021/03/21/linear-regression.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "Linear discriminant analysis",
            "content": "import numpy as np import pandas as pd import seaborn as sns import statsmodels.api as sm . Advantages over logistic regression: . More stable if responses are clearly separated. . | More stable if featurs are all approximately normally distributed. . | Often preferrable if response can take more than two classes. . | . Sources . An introduction to statistical learning | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/stats/2021/03/21/linear-discriminant-analysis.html",
            "relUrl": "/ml/stats/2021/03/21/linear-discriminant-analysis.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "Predicting happiness of Cypriots",
            "content": "Examples from the hands on machine learning book by Aurélien Geron. . import pandas as pd import numpy as np import matplotlib.pyplot as plt . Load data . def load_bli(): bli = pd.read_csv(&#39;./data/oecd-bli.csv&#39;, thousands=&#39;,&#39;) bli = bli.iloc[:, [0, 1, 3, 14]] bli = bli.rename(columns={&#39;LOCATION&#39;: &#39;country_code&#39;}) bli = bli.pivot_table(&#39;Value&#39;, [&#39;country_code&#39;, &#39;Country&#39;], &#39;Indicator&#39;) bli = bli[[&#39;Life satisfaction&#39;]] bli = bli.rename(columns={&#39;Life satisfaction&#39;: &#39;life_satisfaction&#39;}) return bli.reset_index().rename(columns=str.lower) def load_gdp(): gdppc = pd.read_csv(&#39;./data/oecd-gdppc.csv&#39;, thousands=&#39;,&#39;) gdppc = gdppc[gdppc.TIME == 2019] gdppc = gdppc.rename(columns=str.lower) gdppc = gdppc.rename(columns={&#39;location&#39;: &#39;country_code&#39;, &#39;value&#39;: &#39;gdppc&#39;}) return gdppc[[&#39;country_code&#39;, &#39;gdppc&#39;]] def make_dataset(bli, gdppc): bli = bli.merge(gdppc) bli = bli.sort_values(&#39;gdppc&#39;, ascending=False) drop_idx = [0] keep_idx = list(set(bli.index) - set(drop_idx)) return bli.iloc[keep_idx] bli = load_bli() gdppc = load_gdp() data = make_dataset(bli, gdppc) X = data[[&#39;gdppc&#39;]] y = data[&#39;life_satisfaction&#39;] data.head() . country_code country life_satisfaction gdppc . 17 IRL | Ireland | 7.05 | 90140.764256 | . 4 CHE | Switzerland | 7.50 | 70485.014502 | . 28 NOR | Norway | 7.62 | 69169.203192 | . 38 USA | United States | 7.00 | 65126.619982 | . 27 NLD | Netherlands | 7.50 | 59419.863105 | . Plot relationship of interest . import seaborn as sns sns.set_style(&#39;whitegrid&#39;) data.plot(kind=&#39;scatter&#39;, x=&#39;gdppc&#39;, y=&#39;life_satisfaction&#39;, c=&#39;blue&#39;); . Model and predict . from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsRegressor cyprus_gdp = [[22_587]] lr = LinearRegression() lr.fit(X, y) print(f&#39;Linear regression: {lr.predict(cyprus_gdp)[0]:.1f}&#39;) knn1 = KNeighborsRegressor(n_neighbors=1) knn1.fit(X, y) print(f&#39;KNN, k=1: {knn1.predict(cyprus_gdp)[0]:.1f}&#39;) knn3 = KNeighborsRegressor(n_neighbors=3) knn3.fit(X, y) print(f&#39;KNN, k=3: {knn3.predict(cyprus_gdp)[0]:.1f}&#39;) . Linear regression: 5.9 KNN, k=1: 6.5 KNN, k=3: 6.2 .",
            "url": "https://fabiangunzinger.github.io/blog/python/stats/2021/03/21/handson-examples.html",
            "relUrl": "/python/stats/2021/03/21/handson-examples.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "Bayesian statistics",
            "content": "import numpy as np from scipy import stats np.random.seed(1) # for repeatability F_true = 1000 # true flux, say number of photons measured in 1 second N = 50 # number of measurements F = stats.poisson(F_true).rvs(N) # N measurements of the flux e = np.sqrt(F) # errors on Poisson counts estimated via square root . import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.errorbar(F, np.arange(N), xerr=e, fmt=&#39;ok&#39;, ecolor=&#39;gray&#39;, alpha=0.5) ax.vlines([F_true], 0, N, linewidth=5, alpha=0.2) ax.set_xlabel(&quot;Flux&quot;);ax.set_ylabel(&quot;measurement number&quot;); . w = 1 / e ** 2 print(&quot;&quot;&quot; F_true = {0} F_est = {1:.0f} +/- {2:.0f} (based on {3} measurements) &quot;&quot;&quot;.format(F_true, (w * F).sum() / w.sum(), w.sum() ** -0.5, N)) . F_true = 1000 F_est = 998 +/- 4 (based on 50 measurements) .",
            "url": "https://fabiangunzinger.github.io/blog/stats/ml/2021/03/21/bayesian_stats.html",
            "relUrl": "/stats/ml/2021/03/21/bayesian_stats.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "Jupyter",
            "content": "https://www.youtube.com/watch?v=pJ05omgQCMw&amp;list=PLewNEVDy7gq2g8vdvniTaFj0WsaN4QCk_&amp;index=2&amp;t=396s . !jupyter kernelspec list # !jupyter kernelspec list --json # show additional details . Available kernels: basics /Users/fgu/Library/Jupyter/kernels/basics blog /Users/fgu/Library/Jupyter/kernels/blog foods /Users/fgu/Library/Jupyter/kernels/foods habits /Users/fgu/Library/Jupyter/kernels/habits limo /Users/fgu/Library/Jupyter/kernels/limo london /Users/fgu/Library/Jupyter/kernels/london myenv /Users/fgu/Library/Jupyter/kernels/myenv pottering /Users/fgu/Library/Jupyter/kernels/pottering python3.9 /Users/fgu/Library/Jupyter/kernels/python3.9 tracker /Users/fgu/Library/Jupyter/kernels/tracker python3 /Users/fgu/miniconda3/envs/blog/share/jupyter/kernels/python3 . !which python . /Users/fgu/miniconda3/envs/blog/bin/python . !which -a python . /Users/fgu/miniconda3/envs/blog/bin/python /usr/bin/python . Sources . Fluent Python | Python Cookbook | Learning Python | The Hitchhiker&#39;s Guide to Python | Effective Python | Python for Data Analysis | Python Data Science Handbook | Pandas cookbook | Numpy docs | .",
            "url": "https://fabiangunzinger.github.io/blog/python/jupyter/tools/2021/03/18/jupyter.html",
            "relUrl": "/python/jupyter/tools/2021/03/18/jupyter.html",
            "date": " • Mar 18, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "A data exploration checklist",
            "content": "A step-by-step recipe for getting to know a new dataset. . import pandas as pd . Read and have a quick look . df = pd.read_csv(&#39;data/competitor_prices.csv&#39;) df.head() . Date Product_id Competitor_id Competitor_Price . 0 25/11/2013 | 4.0 | C | 74.95 | . 1 25/11/2013 | 4.0 | D | 74.95 | . 2 25/11/2013 | 4.0 | E | 75.00 | . 3 25/11/2013 | 4.0 | F | 99.95 | . 4 26/11/2013 | 4.0 | C | 74.95 | . df.describe() . Product_id Competitor_Price . count 15132.000000 | 15132.000000 | . mean 248.535223 | 85.339813 | . std 115.916096 | 48.460998 | . min 4.000000 | 2.300000 | . 25% 143.000000 | 49.950000 | . 50% 251.000000 | 75.000000 | . 75% 355.000000 | 108.000000 | . max 421.000000 | 500.000000 | . df.Competitor_id.describe() . count 15132 unique 7 top D freq 8092 Name: Competitor_id, dtype: object . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 15395 entries, 0 to 15394 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 Date 15132 non-null object 1 Product_id 15132 non-null float64 2 Competitor_id 15132 non-null object 3 Competitor_Price 15132 non-null float64 dtypes: float64(2), object(2) memory usage: 481.2+ KB . Missing values . https://github.com/ResidentMario/missingno . Sources . Fluent Python | Python Cookbook | Learning Python | The Hitchhiker&#39;s Guide to Python | Effective Python | Python for Data Analysis | Python Data Science Handbook | Pandas cookbook | Numpy docs | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/03/17/exploration-checklist.html",
            "relUrl": "/python/pandas/2021/03/17/exploration-checklist.html",
            "date": " • Mar 17, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "Penalised regression",
            "content": "import numpy as np import pandas as pd import seaborn as sns import statsmodels.api as sm . Both are methods to penalise large coefficients. . | Why would we want to do this? Because we know from bias-variance trade-off -- the U-Shaped curve in complexity-MSE space -- that an increase in bias will result in a reduction of the variance. Now, on some points on that curve, increasing bias by a little might lead to a comparatively large reduction in variance, and thus reduce MSE. Exploiting this possibility is the idea behind penalising large coefficients: biasing coefficients towards zero will introduce a bit of bias, but we hope to get rewarded by lower variance so that overall model performance improves. . | . Difference between approaches . Ridge uses the sum of L2 norms as the penalty: the sum of the squared coefficients. . | Lasso used the sum of L1 norms: the sum of absolute coefficients. . | With lasso, some coefficients might become exactly zero and thus drop out of the model, which is why Lasso is considered a model selection approach. . | The reason for this is shape of the penalties in coefficient size - penalty space: when we square penalties as in Ridge, that gives us a smooth U-shaped parabola that&#39;s fairly flat at teh bottom, meaning that as we move towards zero coefficient size, the reduction in the penalty gets increasingly small and will eventually hit a point where a further reduction in coefficient size is not worth it. In contrast, using absolute penalties leads to a triangle shaped curve with equal (absolute) slope on all points except zero. This means that the marginal gain of moving coefficient size to zero stays constant, so that, for some coefficients, going all the way might be worth it. . | . Bayesian intuition behind difference in performance . Lasso and ridge regression implicitly assume a world where most coefficients are very small (or zero, in the case of lasso) and don&#39;t contribute much to our prediction capacity and only a few are helpful. As opposted to an alternative world where we think most variables we can measure contribute a little bit in equal part to our model&#39;s predictive power. Laplace vs gaussian vs uniform distribution of true coefficient values. . | If the world is truly Laplacian, then biasing our model towards it by using penalised regression will improve the model&#39;s performance. If the world is uniform, then penalised regression will perform worse than linear regression. . | . Sources . The hundred-page machine learning book | An introduction to statistical learning | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/stats/2021/03/15/penalised-regression.html",
            "relUrl": "/ml/stats/2021/03/15/penalised-regression.html",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "Python data science idioms",
            "content": "When working with data, certain basic steps occur over and over. This is the place where I document my current best-practices. . from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Reorder dataframe columns . first = [&#39;date&#39;, &#39;yw&#39;, &#39;pcsector&#39;] rest = set(df.columns) - set(first) df = df[first + list(rest)] . Select a subset of users from a panel dataset based on user-level criteria . I have a folder of files which I want to clean and append. How to do this? | I want to convert a df column to datatime, how to do this (at read, using np, using Pandas)? What are tradeoffs? | . Misc. . Use ast.literal_eval() instead of eval() . Basically, because eval is very dangerous and would happile evaluate a string like os.system(rm -rf /), while ast.literal_eval will only evaluate Python literals. . Sources . Fluent Python | Python Cookbook | Learning Python | The Hitchhiker&#39;s Guide to Python | Effective Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/03/14/idioms.html",
            "relUrl": "/python/2021/03/14/idioms.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "Fluent Pandas",
            "content": "Various data exercises for regular practice. . import numpy as np import pandas as pd import seaborn as sns . Planets . Create a table that shows the number of planets discovered by each method in each decade . df = sns.load_dataset(&#39;planets&#39;) print(df.shape) df.head(3) . (1035, 6) . method number orbital_period mass distance year . 0 Radial Velocity | 1 | 269.300 | 7.10 | 77.40 | 2006 | . 1 Radial Velocity | 1 | 874.774 | 2.21 | 56.95 | 2008 | . 2 Radial Velocity | 1 | 763.000 | 2.60 | 19.84 | 2011 | . decades = df.year // 10 * 10 decades = decades.astype(str) + &#39;s&#39; decades.name = &#39;decade&#39; . df.pivot_table(&#39;number&#39;, columns=decades, index=&#39;method&#39;, aggfunc=&#39;sum&#39;).fillna(0) . decade 1980s 1990s 2000s 2010s . method . Astrometry 0.0 | 0.0 | 0.0 | 2.0 | . Eclipse Timing Variations 0.0 | 0.0 | 5.0 | 10.0 | . Imaging 0.0 | 0.0 | 29.0 | 21.0 | . Microlensing 0.0 | 0.0 | 12.0 | 15.0 | . Orbital Brightness Modulation 0.0 | 0.0 | 0.0 | 5.0 | . Pulsar Timing 0.0 | 9.0 | 1.0 | 1.0 | . Pulsation Timing Variations 0.0 | 0.0 | 1.0 | 0.0 | . Radial Velocity 1.0 | 52.0 | 475.0 | 424.0 | . Transit 0.0 | 0.0 | 64.0 | 712.0 | . Transit Timing Variations 0.0 | 0.0 | 0.0 | 9.0 | . df.groupby([&#39;method&#39;, decades]).number.sum().unstack().fillna(0) . decade 1980s 1990s 2000s 2010s . method . Astrometry 0.0 | 0.0 | 0.0 | 2.0 | . Eclipse Timing Variations 0.0 | 0.0 | 5.0 | 10.0 | . Imaging 0.0 | 0.0 | 29.0 | 21.0 | . Microlensing 0.0 | 0.0 | 12.0 | 15.0 | . Orbital Brightness Modulation 0.0 | 0.0 | 0.0 | 5.0 | . Pulsar Timing 0.0 | 9.0 | 1.0 | 1.0 | . Pulsation Timing Variations 0.0 | 0.0 | 1.0 | 0.0 | . Radial Velocity 1.0 | 52.0 | 475.0 | 424.0 | . Transit 0.0 | 0.0 | 64.0 | 712.0 | . Transit Timing Variations 0.0 | 0.0 | 0.0 | 9.0 | . Sort and filter . Sort by closeness of column CCC to myval . df = pd.DataFrame({&#39;a&#39;: [4, 5, 6, 7], &#39;b&#39;: [10, 20, 30, 40], &#39;c&#39;: [100, 50, -30, -50]}) df . a b c . 0 4 | 10 | 100 | . 1 5 | 20 | 50 | . 2 6 | 30 | -30 | . 3 7 | 40 | -50 | . myval = 34 df.loc[(df.c - myval).abs().argsort()] . a b c . 1 5 | 20 | 50 | . 2 6 | 30 | -30 | . 0 4 | 10 | 100 | . 3 7 | 40 | -50 | . Reminder of what happens here: . a = (df.c - myval).abs() b = a.argsort() a, b . (0 66 1 16 2 64 3 84 Name: c, dtype: int64, 0 1 1 2 2 0 3 3 Name: c, dtype: int64) . argsort returns a series of indexes, so that df[a] returns an ordered dataframe. The first elemnt in b thus refers to the index of the smallest element in a. . Sources . Data School - Data science best practice with Pandas | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/03/14/fluent-pandas.html",
            "relUrl": "/python/pandas/2021/03/14/fluent-pandas.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "AWS",
            "content": "Basic AWS interaction patterns. . todo: . Add different ways to manage sessions: botocore, awswrangler, pandas read/write storage_options | . import platform import botocore import pandas as pd import s3fs . Setup . There are multiple ways to access your AWS account. I store config and credential files in ~/.aws as discussed here. AWS access methods find these files automatically so I don&#39;t have to worry about anything. . List bucket content . bucket = &#39;fgu-mdb&#39; fs = s3fs.S3FileSystem() fs.ls(bucket) . [&#39;fgu-mdb/data_000.csv&#39;] . Read from S3 . fp = f&#39;s3://fgu-mdb/data_000.csv&#39; df = pd.read_csv(fp, sep=&#39;|&#39;) df.shape . (1000, 27) . Read with custom profile . AWS credential and config files allow you to store multiple profiles. Below I access data using a different profile depending on what machine I run the code on. . When Pandas reads files from S3 as above, it uses botocore under the hood and uses the default profile. To change the profile, we need to use botocore directly and set up a session. . profile = &#39;linux-profile&#39; if platform.system() == &#39;Darwin&#39;: profile = &#39;mac-profile&#39; session = botocore.session.Session(profile=profile) fs = s3fs.S3FileSystem(session=session) path = &#39;path-to-file&#39; df = pd.read_csv(fs.open(fp)) .",
            "url": "https://fabiangunzinger.github.io/blog/python/tools/2021/03/14/aws.html",
            "relUrl": "/python/tools/2021/03/14/aws.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "Pandas essentials",
            "content": "import numpy as np import pandas as pd import seaborn as sns . Load a sample dataset . cols = { &#39;user_id&#39;: &#39;user&#39;, &#39;transaction_date&#39;: &#39;date&#39;, &#39;amount&#39;: &#39;amount&#39;, &#39;transaction_description&#39;: &#39;desc&#39;, &#39;merchant_name&#39;: &#39;merchant&#39;, &#39;gender&#39;: &#39;gender&#39;, &#39;year_of_birth&#39;: &#39;yob&#39;, &#39;salary_range&#39;: &#39;salary&#39;, } def randomise_date(series): &quot;&quot;&quot;Add noise to years for additional anonymisation.&quot;&quot;&quot; series = series[~(series.dt.month.eq(2) &amp; series.dt.day.eq(29))] return pd.to_datetime({ &#39;year&#39;: series.dt.year - np.random.randint(0, 5, size=len(series)), &#39;month&#39;: series.dt.month, &#39;day&#39;: series.dt.day }) fp = &#39;./data/sample.parquet&#39; df = pd.read_parquet(fp, columns=cols).rename(columns=cols) df[&#39;date&#39;] = randomise_date(df.date) print(df.shape) df.head(3) . (157287, 8) . user date amount desc merchant gender yob salary . 0 777 | 2010-01-03 | 3.03 | aviva pa - d/d | aviva | m | 1969.0 | 20k to 30k | . 1 777 | 2010-01-03 | 6.68 | 9572 31dec11 , tesco stores 3345 , warrington ... | tesco | m | 1969.0 | 20k to 30k | . 2 777 | 2012-01-03 | 10.27 | 9572 30dec11 , mcdonalds , restaurant , winwic... | mcdonalds | m | 1969.0 | 20k to 30k | . Time series . groupby vs resample . Basically, resample fills in missing period values while groupby doesn&#39;t. . idx = pd.date_range(&#39;2020&#39;, freq=&#39;2d&#39;, periods=3) data = pd.DataFrame({&#39;col&#39;: range(len(idx))}, index=idx) data . col . 2020-01-01 0 | . 2020-01-03 1 | . 2020-01-05 2 | . data.resample(&#39;d&#39;).sum() . col . 2020-01-01 0 | . 2020-01-02 0 | . 2020-01-03 1 | . 2020-01-04 0 | . 2020-01-05 2 | . data.groupby(level=0).sum() . col . 2020-01-01 0 | . 2020-01-03 1 | . 2020-01-05 2 | . data . col . 2020-01-01 0 | . 2020-01-03 1 | . 2020-01-05 2 | . Timedeltas . df.date.max() . Timestamp(&#39;2020-07-31 00:00:00&#39;) . d = df.date.max() - df.date.min() print(d) d.days . 4585 days 00:00:00 . 4585 . Date offsets . Period differences create Date offsets. . d = df.date.max().to_period(&#39;M&#39;) - df.date.min().to_period(&#39;M&#39;) print(d) print(type(d)) d.n . &lt;150 * MonthEnds&gt; &lt;class &#39;pandas._libs.tslibs.offsets.MonthEnd&#39;&gt; . 150 . Aggregate . count vs size . g = df.groupby(&#39;user&#39;) # number of rows per group as a series display(g.size().head(3)) # non-missing observations per group for each variable g.count().head(3) . user 777 6302 14777 2101 20777 6452 dtype: int64 . date amount desc merchant gender yob salary . user . 777 6291 | 6302 | 6302 | 6302 | 6302 | 6302 | 6302 | . 14777 2101 | 2101 | 2101 | 2101 | 2101 | 2101 | 0 | . 20777 6443 | 6452 | 6452 | 6452 | 6452 | 6452 | 0 | . Filter . Different approaches to filter data in decreasing order of preference . cutoff = 30_000 a = df.loc[df.amount &gt; cutoff] b = df.query(&#39;amount &gt; @cutoff&#39;) c = df[df.amount &gt; cutoff] all(a == b) == all(b == c) . True . Categories . Manual sort order . df = pd.DataFrame({ &#39;id&#39;:[1, 2, 3, 4, 5], &#39;quality&#39;: [&#39;good&#39;, &#39;excellent&#39;, &#39;very good&#39;, &#39;excellent&#39;, &#39;good&#39;] }) df.sort_values(&#39;quality&#39;) . id quality . 1 2 | excellent | . 3 4 | excellent | . 0 1 | good | . 4 5 | good | . 2 3 | very good | . from pandas.api.types import CategoricalDtype quality_cat = CategoricalDtype([&#39;good&#39;, &#39;very good&#39;, &#39;excellent&#39;], ordered=True) df[&#39;quality&#39;] = df.quality.astype(quality_cat) df.sort_values(&#39;quality&#39;) . id quality . 0 1 | good | . 4 5 | good | . 2 3 | very good | . 1 2 | excellent | . 3 4 | excellent | . Dates and times . Parsing string dates . Using dateutil . from dateutil.parser import parse date = &#39;1 Nov 2020&#39; print(parse(date)) parse(date).month . 2020-11-01 00:00:00 . 11 . Inside Pandas . print(pd.Timestamp(date)) pd.Timestamp(date).month . 2020-11-01 00:00:00 . 11 . Date and period ranges . idx = pd.period_range(&#39;2018-1&#39;, &#39;2019-1&#39;, freq=&#39;Q-DEC&#39;) s = pd.Series(np.random.randn(len(idx)), index=idx) print(s) s.asfreq(&#39;d&#39;, how=&#39;start&#39;) . 2018Q1 -0.210888 2018Q2 0.217048 2018Q3 0.093228 2018Q4 -0.280792 2019Q1 -1.017585 Freq: Q-DEC, dtype: float64 . 2018-01-01 -0.210888 2018-04-01 0.217048 2018-07-01 0.093228 2018-10-01 -0.280792 2019-01-01 -1.017585 Freq: D, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;M&#39;, kind=&#39;period&#39;).mean() . 2000-01 -0.129504 2000-02 -0.040099 2000-03 0.210304 2000-04 -0.038681 Freq: M, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, freq=&#39;H&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;d&#39;).ohlc() . open high low close . 2000-01-01 -0.148350 | 2.901749 | -2.153478 | -0.657941 | . 2000-01-02 -0.964828 | 1.569833 | -1.415382 | 0.399700 | . 2000-01-03 -0.545781 | 1.263261 | -1.940718 | -1.940718 | . 2000-01-04 -0.406149 | 1.658944 | -1.393457 | -0.656099 | . 2000-01-05 -1.839502 | 0.957588 | -1.839502 | -0.092540 | . Grouping . Create a dictionary from groups based on column types: . df = sns.load_dataset(&#39;iris&#39;) pieces = dict(list(df.groupby(&#39;species&#39;))) pieces[&#39;setosa&#39;].head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . Mappings . apply vs map vs applymap . apply applies a function along an axis of a dataframe or on series values | map applies a correspondance to each value in a series | applymap applies a function to each element in a dataframe | . data = df.loc[:2, [&#39;gender&#39;, &#39;merchant&#39;]] gender = {&#39;m&#39;: &#39;male&#39;, &#39;f&#39;: &#39;female&#39;} data . gender merchant . 0 m | aviva | . 1 m | tesco | . 2 m | mcdonalds | . data.apply(lambda x: x.map(gender)) . gender merchant . 0 male | NaN | . 1 male | NaN | . 2 male | NaN | . data.gender.map(gender) . 0 male 1 male 2 male Name: gender, dtype: object . data.applymap(gender.get) . gender merchant . 0 male | None | . 1 male | None | . 2 male | None | . get turns a dictionary into a function that takes a key and returns its corresponding value if the key is in the dictionary and a default value otherwise. . Creating new columns based on existing ones using mappings . The below is a straightforward adaptation from the cookbook: . df = pd.DataFrame({&#39;AAA&#39;: [1, 2, 1, 3], &#39;BBB&#39;: [1, 1, 4, 2], &#39;CCC&#39;: [2, 1, 3, 1]}) source_cols = [&#39;AAA&#39;, &#39;BBB&#39;] new_cols = [str(c) + &#39;_cat&#39; for c in source_cols] cats = {1: &#39;One&#39;, 2: &#39;Two&#39;, 3: &#39;Three&#39;} dd = df.copy() dd[new_cols] = df[source_cols].applymap(cats.get) dd . AAA BBB CCC AAA_cat BBB_cat . 0 1 | 1 | 2 | One | One | . 1 2 | 1 | 1 | Two | One | . 2 1 | 4 | 3 | One | None | . 3 3 | 2 | 1 | Three | Two | . But it made me wonder why applymap required the use of the get method while we can map values of a series like so: . s = pd.Series([1, 2, 3, 1]) s.map(cats) . 0 One 1 Two 2 Three 3 One dtype: object . or so . s.map(cats.get) . 0 One 1 Two 2 Three 3 One dtype: object . The answer is simple: applymap requires a function as argument, while map takes functions or mappings. . One limitation of the cookbook solution above is that is doesn&#39;t seem to allow for default values (notice that 4 gets substituted with &quot;None&quot;). . One way around this is the following: . df[new_cols] = df[source_cols].applymap(lambda x: cats.get(x, &#39;Hello&#39;)) df . AAA BBB CCC AAA_cat BBB_cat . 0 1 | 1 | 2 | One | One | . 1 2 | 1 | 1 | Two | One | . 2 1 | 4 | 3 | One | Hello | . 3 3 | 2 | 1 | Three | Two | . Sources . Python for Data Analysis | Python Data Science Handbook (PDSH) | Pandas cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/03/12/pandas-essentials.html",
            "relUrl": "/python/pandas/2021/03/12/pandas-essentials.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "Misc learning",
            "content": "import numpy as np import scipy as sp import statsmodels.api as sm from statsmodels.formula.api import ols import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import seaborn as sns # sns.set_context(&quot;poster&quot;) # sns.set(rc={&#39;figure.figsize&#39;: (16, 9.)}) sns.set_style(&quot;whitegrid&quot;) import pandas as pd pd.set_option(&#39;display.max_rows&#39;, 50) pd.set_option(&#39;display.max_columns&#39;, 120) pd.set_option(&#39;max_colwidth&#39;, None) from sklearn.model_selection import train_test_split from sklearn.linear_model import RidgeClassifier from sklearn.linear_model import LogisticRegression from xgboost import XGBClassifier from sklearn.metrics import plot_roc_curve from sklearn.metrics import plot_precision_recall_curve from stargazer.stargazer import Stargazer from IPython.core.display import HTML from habits.cleaning import list_s3_files from habits.processing import read_o2_data, classify_users, make_yX, print_info from habits.modelling import make_stargazer, make_roc %load_ext autoreload %autoreload 2 . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-2df7d6552618&gt; in &lt;module&gt; 29 from IPython.core.display import HTML 30 &gt; 31 from habits.cleaning import list_s3_files 32 from habits.processing import read_o2_data, classify_users, make_yX, print_info 33 from habits.modelling import make_stargazer, make_roc ModuleNotFoundError: No module named &#39;habits&#39; . Creating datetime indices . from dateutil.parser import parse parse(&#39;3 Apr 2020&#39;).month parse(&#39;3.4.2020&#39;).month parse(&#39;3.4.2020&#39;, dayfirst=True).month . 4 . 3 . 4 . dates = pd.date_range(start=&#39;1/1/2000&#39;, freq=&#39;A-DEC&#39;, periods = 100) values = np.random.randn(100) ts = pd.Series(values, index=dates) ts.head(10) . 2000-12-31 0.843433 2001-12-31 -0.391251 2002-12-31 0.149087 2003-12-31 0.086131 2004-12-31 -2.308920 2005-12-31 -0.569420 2006-12-31 0.033575 2007-12-31 0.449340 2008-12-31 0.846790 2009-12-31 0.633025 Freq: A-DEC, dtype: float64 . idx = pd.period_range(&#39;2018-1&#39;, &#39;2019-12&#39;, freq=&#39;Q-DEC&#39;) s = pd.Series(np.random.randn(len(idx)), index=idx) s.asfreq(&#39;d&#39;, how=&#39;start&#39;).asfreq(&#39;Q&#39;) . 2018Q1 -0.205351 2018Q2 -0.673207 2018Q3 -0.872625 2018Q4 2.045383 2019Q1 -0.696708 2019Q2 -0.798782 2019Q3 -1.904917 2019Q4 -0.436799 Freq: Q-DEC, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;M&#39;, kind=&#39;period&#39;).mean() . 2000-01 0.014726 2000-02 0.056242 2000-03 -0.016878 2000-04 -0.690987 Freq: M, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, freq=&#39;H&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;d&#39;).ohlc() . open high low close . 2000-01-01 -1.945039 | 2.231004 | -2.751270 | 0.272018 | . 2000-01-02 0.335163 | 2.173964 | -0.895301 | 0.995886 | . 2000-01-03 0.771501 | 1.399791 | -2.465960 | -0.030100 | . 2000-01-04 1.765987 | 1.765987 | -1.698415 | -0.056522 | . 2000-01-05 0.215849 | 1.192958 | 0.215849 | 0.786069 | . s.resample(&#39;min&#39;).asfreq().ffill() . 2000-01-01 00:00:00 -1.945039 2000-01-01 00:01:00 -1.945039 2000-01-01 00:02:00 -1.945039 2000-01-01 00:03:00 -1.945039 2000-01-01 00:04:00 -1.945039 ... 2000-01-05 02:56:00 1.192958 2000-01-05 02:57:00 1.192958 2000-01-05 02:58:00 1.192958 2000-01-05 02:59:00 1.192958 2000-01-05 03:00:00 0.786069 Freq: T, Length: 5941, dtype: float64 . data = df.reset_index(level=0).head(100).sort_index()[[&#39;amount&#39;]] data . amount . transaction_date . 2012-08-29 12.00 | . 2012-08-30 13.50 | . 2012-08-30 7.44 | . 2012-08-30 7.44 | . 2012-08-30 13.50 | . ... ... | . 2012-12-27 135.00 | . 2012-12-27 3.60 | . 2012-12-27 8.50 | . 2012-12-27 135.00 | . 2012-12-27 8.50 | . 100 rows × 1 columns . IPython / notebook shortcuts . clean_nb = !ls *2* clean_nb . [&#39;2.0-fgu-clean-and-split-data.ipynb&#39;] . %%timeit a = range(1000) . 174 ns ± 4.23 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) . %%writefile pythoncode.py import numpy def append_if_not_exists(arr, x): if x not in arr: arr.append(x) def some_useless_slow_function(): arr = list() for i in range(10000): x = numpy.random.randint(0, 10000) append_if_not_exists(arr, x) . Writing pythoncode.py . %pycat pythoncode.py . import numpy def append_if_not_exists(arr, x): if x not in arr: arr.append(x) def some_useless_slow_function(): arr = list() for i in range(10000): x = numpy.random.randint(0, 10000) append_if_not_exists(arr, x) . . I can transfer variables with any content from one notebook to the next (useful if you run a number of notebooks in sequence, for instance, and the ouput of one serves as the input of another). . var_to_pass_on = dfu.head() %store var_to_pass_on . Stored &#39;var_to_pass_on&#39; (DataFrame) . %store -r var_to_pass_on var_to_pass_on . user_id year_of_birth user_registration_date salary_range postcode gender soa_lower soa_middle user_uid . 0 3706 | 1967-01-01 | 2012-09-30 | NaN | XXXX 0 | M | NaN | NaN | 3706-0 | . 1 1078 | 1964-01-01 | 2011-11-29 | 20K to 30K | M25 9 | M | E01005038 | E02001043 | 1078-0 | . 2 232 | 1965-01-01 | 2010-09-09 | 30K to 40K | CM4 0 | M | E01021551 | E02004495 | 232-0 | . 3 6133 | 1968-01-01 | 2012-10-21 | 10K to 20K | SK12 1 | M | E01018665 | E02003854 | 6133-0 | . 4 7993 | 1961-01-01 | 2012-10-29 | 20K to 30K | LS17 8 | M | E01011556 | E02002344 | 7993-0 | . !conda list | grep pandas . pandas 1.0.1 py37h6c726b0_0 pandas-flavor 0.2.0 py_0 conda-forge . Using R and Python together . # %conda install tzlocal # %conda install simplegeneric . import rpy2 . # pandas2ri.activate() . %reload_ext rpy2.ipython . %R require(ggplot2) . R[write to console]: Loading required package: ggplot2 . array([0], dtype=int32) . import pandas as pd df = pd.DataFrame({ &#39;Letter&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;c&#39;], &#39;X&#39;: [4, 3, 5, 2, 1, 7, 7, 5, 9], &#39;Y&#39;: [0, 4, 3, 6, 7, 10, 11, 9, 13], &#39;Z&#39;: [1, 2, 3, 1, 2, 3, 1, 2, 3] }) . %%R -i df ggplot(data = df) + geom_point(aes(x = X, y= Y, color = Letter, size = Z)) . R[write to console]: Error in ggplot(data = df) : could not find function &#34;ggplot&#34; Calls: &lt;Anonymous&gt; -&gt; &lt;Anonymous&gt; -&gt; withVisible R[write to console]: In addition: R[write to console]: Warning messages: R[write to console]: 1: R[write to console]: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : R[write to console]: R[write to console]: there is no package called ‘ggplot’ R[write to console]: 2: R[write to console]: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : R[write to console]: R[write to console]: there is no package called ‘ggplot2’ . Error in ggplot(data = df) : could not find function &#34;ggplot&#34; Calls: &lt;Anonymous&gt; -&gt; &lt;Anonymous&gt; -&gt; withVisible . Printing virtually anything . happy_squirrels = !ls /Users/fgu/Library/Mobile Documents/com~apple~CloudDocs/fab/photos/squirrels/ . from IPython.display import display, Image for s in happy_squirrels: display(Image(&#39;/Users/fgu/Library/Mobile Documents/com~apple~CloudDocs/fab/photos/squirrels/&#39; + s, width=100)) . Write code to and load code from file . %%writefile test.py print(&#39;Hello&#39;) . Overwriting test.py . %pycat test.py . print(&#39;Hello&#39;) . Data from wide to long and back . data = pd.read_csv(&#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/macrodata.csv&#39;) data.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name = &#39;date&#39;) columns = pd.Index([&#39;realgdp&#39;, &#39;cpi&#39;, &#39;unemp&#39;, &#39;infl&#39;], name=&#39;item&#39;) df = data.reindex(columns=columns) df.index = periods.to_timestamp(&#39;D&#39;, &#39;End&#39;) dfl = df.stack().reset_index().rename(columns={0:&#39;value&#39;}) dfl . date item value . 0 1959-03-31 23:59:59.999999999 | realgdp | 2710.349 | . 1 1959-03-31 23:59:59.999999999 | cpi | 28.980 | . 2 1959-03-31 23:59:59.999999999 | unemp | 5.800 | . 3 1959-03-31 23:59:59.999999999 | infl | 0.000 | . 4 1959-06-30 23:59:59.999999999 | realgdp | 2778.801 | . ... ... | ... | ... | . 807 2009-06-30 23:59:59.999999999 | infl | 3.370 | . 808 2009-09-30 23:59:59.999999999 | realgdp | 12990.341 | . 809 2009-09-30 23:59:59.999999999 | cpi | 216.385 | . 810 2009-09-30 23:59:59.999999999 | unemp | 9.600 | . 811 2009-09-30 23:59:59.999999999 | infl | 3.560 | . 812 rows × 3 columns . dfl.set_index([&#39;date&#39;, &#39;item&#39;]).unstack() . value . item cpi infl realgdp unemp . date . 1959-03-31 23:59:59.999999999 28.980 | 0.00 | 2710.349 | 5.8 | . 1959-06-30 23:59:59.999999999 29.150 | 2.34 | 2778.801 | 5.1 | . 1959-09-30 23:59:59.999999999 29.350 | 2.74 | 2775.488 | 5.3 | . 1959-12-31 23:59:59.999999999 29.370 | 0.27 | 2785.204 | 5.6 | . 1960-03-31 23:59:59.999999999 29.540 | 2.31 | 2847.699 | 5.2 | . ... ... | ... | ... | ... | . 2008-09-30 23:59:59.999999999 216.889 | -3.16 | 13324.600 | 6.0 | . 2008-12-31 23:59:59.999999999 212.174 | -8.79 | 13141.920 | 6.9 | . 2009-03-31 23:59:59.999999999 212.671 | 0.94 | 12925.410 | 8.1 | . 2009-06-30 23:59:59.999999999 214.469 | 3.37 | 12901.504 | 9.2 | . 2009-09-30 23:59:59.999999999 216.385 | 3.56 | 12990.341 | 9.6 | . 203 rows × 4 columns . dfl.pivot(&#39;date&#39;, &#39;item&#39;, &#39;value&#39;) . item cpi infl realgdp unemp . date . 1959-03-31 23:59:59.999999999 28.980 | 0.00 | 2710.349 | 5.8 | . 1959-06-30 23:59:59.999999999 29.150 | 2.34 | 2778.801 | 5.1 | . 1959-09-30 23:59:59.999999999 29.350 | 2.74 | 2775.488 | 5.3 | . 1959-12-31 23:59:59.999999999 29.370 | 0.27 | 2785.204 | 5.6 | . 1960-03-31 23:59:59.999999999 29.540 | 2.31 | 2847.699 | 5.2 | . ... ... | ... | ... | ... | . 2008-09-30 23:59:59.999999999 216.889 | -3.16 | 13324.600 | 6.0 | . 2008-12-31 23:59:59.999999999 212.174 | -8.79 | 13141.920 | 6.9 | . 2009-03-31 23:59:59.999999999 212.671 | 0.94 | 12925.410 | 8.1 | . 2009-06-30 23:59:59.999999999 214.469 | 3.37 | 12901.504 | 9.2 | . 2009-09-30 23:59:59.999999999 216.385 | 3.56 | 12990.341 | 9.6 | . 203 rows × 4 columns . Misc. . idx = pd.IndexSlice df.loc[idx[:, &quot;2014&quot;], :] . from scipy import stats s = np.arange(5) std = np.std(s) mean = np.mean(s) manual_z = (s - mean) / std scipy_z = stats.zscore(s) manual_z == scipy_z . array([ True, True, True, True, True]) . np.arange(32).reshape((4, 8)) . array([[ 0, 1, 2, 3, 4, 5, 6, 7], [ 8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]) . import random print(&quot;one&quot;) if random.randint(0, 1) else print(&quot;zero&quot;) . zero . ndraws = 1000 draws = np.random.randint(0, 2, ndraws) steps = np.where(draws &gt; 0, 1, -1) walk = steps.cumsum() # Find how long it took to make 10 steps in either direction idx = (np.abs(walk) &gt;= 10).argmax() print(&quot;It took {} steps to get to {}&quot;.format(idx, walk[idx])) . It took 57 steps to get to -10 . nwalks = 5000 ndraws = 1000 draws = np.random.randint(0, 2, size=(nwalks, ndraws)) steps = np.where(draws &gt; 0, 1, -1) walk = steps.cumsum(axis=1) # Find number of walks that cross 30 and the average crossing time hit30 = (np.abs(walk) &gt;= 30).any(1) crossing_times = (np.abs(walk[hit30]) &gt;= 30).argmax(1) print( &quot;{} walks cross 30, taking {} steps on average.&quot;.format( hit30.sum(), crossing_times.mean() ) ) . 3385 walks cross 30, taking 498.33353028064994 steps on average. . Grouping . df = pd.DataFrame({&#39;key1&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], &#39;key2&#39;: [&#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;], &#39;data1&#39;: np.random.randn(5), &#39;data2&#39;: np.random.randn(5)}) df . key1 key2 data1 data2 . 0 a | one | -0.722346 | 1.350070 | . 1 b | two | -0.864278 | 0.652911 | . 2 c | one | 0.875035 | 0.838726 | . 3 d | two | 1.420677 | -0.464896 | . 4 e | one | -0.789309 | 0.148121 | . for group, data in df.groupby(&#39;key1&#39;): print(group) print(data) . a key1 key2 data1 data2 0 a one -0.722346 1.35007 b key1 key2 data1 data2 1 b two -0.864278 0.652911 c key1 key2 data1 data2 2 c one 0.875035 0.838726 d key1 key2 data1 data2 3 d two 1.420677 -0.464896 e key1 key2 data1 data2 4 e one -0.789309 0.148121 . pieces = dict(list(df.groupby(&#39;key1&#39;))) pieces[&#39;a&#39;] . key1 key2 data1 data2 . 0 a | one | -0.722346 | 1.35007 | . grouped = df.groupby(df.dtypes, axis=1) for dtype, data in grouped: print(dtype) print(data) . float64 data1 data2 0 -0.722346 1.350070 1 -0.864278 0.652911 2 0.875035 0.838726 3 1.420677 -0.464896 4 -0.789309 0.148121 object key1 key2 0 a one 1 b two 2 c one 3 d two 4 e one . Solution to my common indexing problem (index with shorter boolean series) . Problem: I want to index my df based on a boolean series that is shorter than the length of the df. E.g. I have a subset of users that fulfill a condition and want to keep these only. . data = o2.sample(frac=0.05) . high_spender = data.groupby(&quot;user_id&quot;).amount.sum() &gt; 350 . . todrop = high_spender[~high_spender].index.values data.drop(todrop, level=0).sort_index() . transaction_id transaction_description amount auto_tag merchant_name account_type . user_id transaction_date . 20912 2013-12-18 12152457 | card purchase o2 uk ref xxx xxxxxx0000 bcc | 11.990000 | mobile | o2 | current | . 2014-04-17 17466459 | direct debit o2 gedxxxxx154 ddr | 44.160000 | mobile | o2 | current | . 2015-01-19 54324542 | direct debit o2 gedxxxx9154 ddr | 38.380001 | mobile | o2 | current | . 2015-03-24 80422755 | direct debit o2 gedxxxx9097 ddr | 34.799999 | mobile | o2 | current | . 2015-09-23 96952964 | counter credit &lt;mdbremoved&gt; o2 bgc | 54.500000 | mobile | o2 | current | . 2016-04-25 134730196 | direct debit o2 gedxxxx9097 ddr | 40.990002 | mobile | o2 | current | . 2016-05-17 138708140 | direct debit o2 gedxxxx1717 ddr | 30.000000 | mobile | o2 | current | . 2016-08-17 154735556 | direct debit o2 gedxxxx9154 ddr | 34.270000 | mobile | o2 | current | . 2016-10-07 163699913 | counter credit &lt;mdbremoved&gt; o2/ &lt;mdbremoved&gt; bgc | 68.059998 | mobile | o2 | current | . 160992 2014-09-29 38730708 | o2 uk cd 9531 | 25.000000 | mobile | o2 | current | . 2015-01-19 54406685 | o2 uk cd 9531 | 175.000000 | mobile | o2 | current | . 2015-01-19 54406686 | o2 uk cd 9531 | 202.539993 | mobile | o2 | current | . high_spender . user_id 8 False 659 False 1078 False 1146 False 2324 False ... 420102 False 421678 False 423912 False 424865 False 425830 False Name: amount, Length: 210, dtype: bool . hs = high_spender[high_spender].index.values mask = data.index.isin(hs, level=0) data[mask].sort_index() . transaction_id transaction_description amount auto_tag merchant_name account_type . user_id transaction_date . 20912 2013-12-18 12152457 | card purchase o2 uk ref xxx xxxxxx0000 bcc | 11.990000 | mobile | o2 | current | . 2014-04-17 17466459 | direct debit o2 gedxxxxx154 ddr | 44.160000 | mobile | o2 | current | . 2015-01-19 54324542 | direct debit o2 gedxxxx9154 ddr | 38.380001 | mobile | o2 | current | . 2015-03-24 80422755 | direct debit o2 gedxxxx9097 ddr | 34.799999 | mobile | o2 | current | . 2015-09-23 96952964 | counter credit &lt;mdbremoved&gt; o2 bgc | 54.500000 | mobile | o2 | current | . 2016-04-25 134730196 | direct debit o2 gedxxxx9097 ddr | 40.990002 | mobile | o2 | current | . 2016-05-17 138708140 | direct debit o2 gedxxxx1717 ddr | 30.000000 | mobile | o2 | current | . 2016-08-17 154735556 | direct debit o2 gedxxxx9154 ddr | 34.270000 | mobile | o2 | current | . 2016-10-07 163699913 | counter credit &lt;mdbremoved&gt; o2/ &lt;mdbremoved&gt; bgc | 68.059998 | mobile | o2 | current | . 160992 2014-09-29 38730708 | o2 uk cd 9531 | 25.000000 | mobile | o2 | current | . 2015-01-19 54406685 | o2 uk cd 9531 | 175.000000 | mobile | o2 | current | . 2015-01-19 54406686 | o2 uk cd 9531 | 202.539993 | mobile | o2 | current | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/03/10/learning.html",
            "relUrl": "/python/pandas/2021/03/10/learning.html",
            "date": " • Mar 10, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "Web frameworks",
            "content": "Every time I visit a website, an application called a web server who usually resides on a machine also called a web server sends HTML to my browser. HTML (Hypertext markup language) is used by browsers to describe the content and structure of a website. . | The essence of every web application is to send HTML to a browser. . | How does a web server know what data it needs to send? It sends what I request using the HTTP protocol. (HTTP means hypertext transfer protocol; a protocol is a universally agreed data format and sequence of steps to enable communication between two parties.) . | So, a web application receives HTTP requests (e.g. get or post) and responds with an HTTP request, usually in the form of the HTML for the requested page. . | . Main sources . Jeff Knupp post | Robert Chang post | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/02/27/web-frameworks.html",
            "relUrl": "/python/2021/02/27/web-frameworks.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "Splitting a large file in Python",
            "content": "https://www.wefearchange.org/2013/05/resource-management-in-python-33-or.html . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/02/27/splitting-file.html",
            "relUrl": "/python/2021/02/27/splitting-file.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "Regular expressions",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . path = &#39;/Users/fgu/tmp/data_777.csv&#39; df = pd.read_csv(path, sep=&#39;|&#39;) . a = &#39;b&#39; f&#39;he {a}&#39; . &#39;he b&#39; . [str(n) for n in range(1, 10)] . [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] . with open(path) as f: next(f) line = f.readline() pattern = &#39;&quot; d+&quot; |&quot;(?P&lt;user_id&gt; d+)&quot;&#39; match = re.match(pattern, line) print(line) print(match) print(match.group(&#39;user_id&#39;)) . &#34;688293&#34;|&#34;777&#34;|&#34;2011-07-20&#34;|&#34;1969&#34;|&#34;20K to 30K&#34;|&#34;WA1 4&#34;|&#34;E01012553&#34;|&#34;E02002603&#34;|&#34;M&#34;|&#34;2012-01-25&#34;|&#34;262916&#34;|&#34;NatWest Bank&#34;|&#34;Current&#34;|&#34;364.22&#34;|&#34;9572 24jan12 , tcs bowdon , bowdon gb - pos&#34;|&#34;Debit&#34;|&#34;25.03&#34;|&#34;No Tag&#34;|&#34;No Tag&#34;|&#34;No Tag&#34;|&#34;No Merchant&#34;|&#34;Unknown Merchant&#34;|&#34;2011-07-20&#34;|&#34;2020-07-21 20:32:00&#34;|&#34;2014-07-18&#34;|&#34;2017-10-24&#34;|&#34;U&#34; &lt;re.Match object; span=(0, 14), match=&#39;&#34;688293&#34;|&#34;777&#34;&#39;&gt; 777 . re.Match.group . def colname_cleaner(df): &quot;&quot;&quot;Convert column names to stripped lowercase with underscores.&quot;&quot;&quot; df.columns = df.columns.str.lower().str.strip() return df def str_cleaner(df): &quot;&quot;&quot;Convert string values to stripped lowercase.&quot;&quot;&quot; str_cols = df.select_dtypes(&#39;object&#39;) for col in str_cols: df[col] = df[col].str.lower().str.strip() return df movies = (data.movies() .pipe(colname_cleaner) .pipe(str_cleaner)) movies.head(2) . title us gross worldwide gross us dvd sales production budget release date mpaa rating running time min distributor source major genre creative type director rotten tomatoes rating imdb rating imdb votes . 0 the land girls | 146083.0 | 146083.0 | NaN | 8000000.0 | jun 12 1998 | r | NaN | gramercy | None | None | None | None | NaN | 6.1 | 1071.0 | . 1 first love, last rites | 10876.0 | 10876.0 | NaN | 300000.0 | aug 07 1998 | r | NaN | strand | None | drama | None | None | NaN | 6.9 | 207.0 | . import re . Finding a single pattern in text . pattern = &#39;hello&#39; text = &#39;hello world it is a beautiful day.&#39; match = re.search(pattern, text) match.start(), match.end(), match.group() . (0, 5, &#39;hello&#39;) . In Pandas . movies.title.str.extract(&#39;(love)&#39;) . 0 . 0 NaN | . 1 love | . 2 NaN | . 3 NaN | . 4 NaN | . ... ... | . 3196 NaN | . 3197 NaN | . 3198 NaN | . 3199 NaN | . 3200 NaN | . 3201 rows × 1 columns . contains(): Test if pattern or regex is contained within a string of a Series or Index. | match(): Determine if each string starts with a match of a regular expression. | fullmatch(): | extract(): Extract capture groups in the regex pat as columns in a DataFrame. | extractall(): Returns all matches (not just the first match). | find(): | findall(): | replace(): | . movies.title.replace(&#39;girls&#39;, &#39;hello&#39;) . 0 the land girls 1 first love, last rites 2 i married a strange person 3 let&#39;s talk about sex 4 slam ... 3196 zack and miri make a porno 3197 zodiac 3198 zoom 3199 the legend of zorro 3200 the mask of zorro Name: title, Length: 3201, dtype: object . Let&#39;s drop all movies by distributors with &quot;Pictures&quot; and &quot;Universal&quot; in their title. . names = [&#39;Universal&#39;, &#39;Pictures&#39;] pattern = &#39;|&#39;.join(names) mask = movies.distributor.str.contains(pattern, na=True) result = movies[~mask] result.head(2) . title us_gross worldwide_gross us_dvd_sales production_budget release_date mpaa_rating running_time_min distributor source major_genre creative_type director rotten_tomatoes_rating imdb_rating imdb_votes . 0 The Land Girls | 146083.0 | 146083.0 | NaN | 8000000.0 | Jun 12 1998 | R | NaN | Gramercy | None | None | None | None | NaN | 6.1 | 1071.0 | . 1 First Love, Last Rites | 10876.0 | 10876.0 | NaN | 300000.0 | Aug 07 1998 | R | NaN | Strand | None | Drama | None | None | NaN | 6.9 | 207.0 | . names = [&#39;Universal&#39;, &#39;Pictures&#39;] pattern = &#39; |&#39;.join(names) neg_pattern = f&#39;[^{pattern}]&#39; neg_pattern mask = movies.distributor.str.contains(neg_pattern, na=False) result2 =movies[mask] . neg_pattern . &#39;[^Universal |Pictures]&#39; . result == result2 . ValueError Traceback (most recent call last) &lt;ipython-input-114-1dac580d3c6f&gt; in &lt;module&gt; -&gt; 1 result == result2 ~/miniconda3/envs/habits/lib/python3.7/site-packages/pandas/core/ops/__init__.py in f(self, other) 837 if not self._indexed_same(other): 838 raise ValueError( --&gt; 839 &#34;Can only compare identically-labeled DataFrame objects&#34; 840 ) 841 new_data = dispatch_to_series(self, other, op, str_rep) ValueError: Can only compare identically-labeled DataFrame objects . def drop_card_repayments(df): &quot;&quot;&quot;Drop card repayment transactions from current accounts.&quot;&quot;&quot; tags = [&#39;credit card repayment&#39;, &#39;credit card payment&#39;, &#39;credit card&#39;] pattern = &#39;|&#39;.join(tags) mask = df.auto_tag.str.contains(pattern) &amp; df.account_type.eq(&#39;current&#39;) return df[~mask] . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/02/27/regex.html",
            "relUrl": "/python/2021/02/27/regex.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "Profiling",
            "content": "We should forget about small efficiencies, say about 97% of the time:premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. -- Donald Knuth Takeaway: optimise where it matters. And to know where it matters, you need to profile your code. . from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Intro . What&#39;s efficient code? . Fast (minimal completion time) | Has no unnecessary memory overhead (minimal resource consumption) | . Profiling runtime . Optimise slow lines inside slow function. | . Finding slow functions (IPython implementation): time.time() decorator (%time, or %timeit for more precision) | cProfile (%prun) | Snakeviz is helpful for cProfile results visualisation | . | . Finding slow lines: line_profiler (%lprun) | . | . Best practices: . Check overall CPU usage during profiling (e.g. use activity monitor on Mac) to make sure that no other processes are influencing my results (e.g. Dropbox update). . | Form a hypothesis about what parts of the code are slow and then compare to the profiling results to improve your intuition over time. . | Start with quick and dirty profiling to zoom into the relevant area (e.g. use a timer decorator, use %time instead of %timeit in Jupyter) before doing more costly profiling. . | | . | . Useful: . https://www.machinelearningplus.com/python/cprofile-how-to-profile-your-python-code/ | . Things to know . cProfile() doesn&#39;t seem to work with multiprocessing. | . Sources . High Performance Python | Effective Python | Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/02/27/profiling.html",
            "relUrl": "/python/pandas/2021/02/27/profiling.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "Pandas essentials",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . df = pd.read_parquet(SAMPLEDATA) print(df.shape) df.head(3) . (157287, 22) . user_id transaction_date amount transaction_description merchant_name tag gender up_tag account_id year_of_birth merchant_business_line salary_range latest_balance account_type credit_debit transaction_id bank postcode ym account_created user_registration_date account_last_refreshed . 0 777 | 2012-01-03 | 3.03 | aviva pa - d/d | aviva | life insurance | m | life insurance | 262916 | 1969.0 | aviva | 20k to 30k | 364.22 | current | debit | 688262 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 1 777 | 2012-01-03 | 6.68 | 9572 31dec11 , tesco stores 3345 , warrington gb - pos | tesco | food, groceries, household | m | food, groceries, household | 262916 | 1969.0 | tesco supermarket | 20k to 30k | 364.22 | current | debit | 688263 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 2 777 | 2012-01-03 | 10.27 | 9572 30dec11 , mcdonalds , restaurant , winwick road gb - pos | mcdonalds | dining and drinking | m | dining and drinking | 262916 | 1969.0 | mcdonalds | 20k to 30k | 364.22 | current | debit | 688264 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . df.iloc[0, 0] = np.nan df.dropna() . user_id transaction_date amount transaction_description merchant_name tag gender up_tag account_id year_of_birth merchant_business_line salary_range latest_balance account_type credit_debit transaction_id bank postcode ym account_created user_registration_date account_last_refreshed . 1 777.0 | 2012-01-03 | 6.68 | 9572 31dec11 , tesco stores 3345 , warrington gb - pos | tesco | food, groceries, household | m | food, groceries, household | 262916 | 1969.0 | tesco supermarket | 20k to 30k | 364.22 | current | debit | 688263 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 2 777.0 | 2012-01-03 | 10.27 | 9572 30dec11 , mcdonalds , restaurant , winwick road gb - pos | mcdonalds | dining and drinking | m | dining and drinking | 262916 | 1969.0 | mcdonalds | 20k to 30k | 364.22 | current | debit | 688264 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 3 777.0 | 2012-01-03 | 12.00 | 9572 31dec11 , tesco stores 3345 , warrington gb - pos | tesco | food, groceries, household | m | food, groceries, household | 262916 | 1969.0 | tesco supermarket | 20k to 30k | 364.22 | current | debit | 688265 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 4 777.0 | 2012-01-03 | 400.00 | &lt;mdbremoved&gt; - s/o | no merchant | other account | m | other account | 262916 | 1969.0 | non merchant mbl | 20k to 30k | 364.22 | current | debit | 688261 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 5 777.0 | 2012-01-04 | 8.74 | 9572 03jan12 , tesco-stores sacat, sale gb - pos | tesco | food, groceries, household | m | food, groceries, household | 262916 | 1969.0 | tesco supermarket | 20k to 30k | 364.22 | current | debit | 688267 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 157282 579777.0 | 2020-07-01 | 85.00 | &lt;mdbremoved&gt; | no merchant | transfers | m | transfers | 1655945 | 1986.0 | non merchant mbl | 20k to 30k | 6684.43 | current | debit | 793743159 | halifax personal banking | m4 6 | 202007 | 2020-04-07 | 2020-04-07 | 2020-07-05 06:34:00 | . 157283 579777.0 | 2020-07-01 | 474.54 | halifax | halifax | personal loan | m | personal loan | 1655945 | 1986.0 | halifax | 20k to 30k | 6684.43 | current | debit | 793743154 | halifax personal banking | m4 6 | 202007 | 2020-04-07 | 2020-04-07 | 2020-07-05 06:34:00 | . 157284 579777.0 | 2020-07-01 | 750.00 | &lt;mdbremoved&gt; | no merchant | transfers | m | transfers | 1655945 | 1986.0 | non merchant mbl | 20k to 30k | 6684.43 | current | debit | 793743157 | halifax personal banking | m4 6 | 202007 | 2020-04-07 | 2020-04-07 | 2020-07-05 06:34:00 | . 157285 579777.0 | 2020-07-02 | -27.98 | payment received - thank you | no merchant | credit card | m | credit card | 1655946 | 1986.0 | account provider | 20k to 30k | -30.80 | credit card | credit | 794507339 | halifax personal banking | m4 6 | 202007 | 2020-04-07 | 2020-04-07 | 2020-07-05 06:34:00 | . 157286 579777.0 | 2020-07-02 | 60.00 | bulb energy | no merchant | energy - gas, elec, other | m | energy - gas, elec, other | 1655945 | 1986.0 | no merchant business line | 20k to 30k | 6684.43 | current | debit | 794296457 | halifax personal banking | m4 6 | 202007 | 2020-04-07 | 2020-04-07 | 2020-07-05 06:34:00 | . 26490 rows × 22 columns . Questions . True of False . Fluency exercises . Topics . Groupby . Create a dictionary from groups based on column types: . df = sns.load_dataset(&#39;iris&#39;) pieces = dict(list(df.groupby(&#39;species&#39;))) pieces[&#39;setosa&#39;][:3] . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . df.dtypes . Topics . Sources . Fluent Python | Python Cookbook | Learning Python | The Hitchhiker&#39;s Guide to Python | Effective Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/02/27/pandas-essentials.html",
            "relUrl": "/python/pandas/2021/02/27/pandas-essentials.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "Misc learning",
            "content": "import numpy as np import scipy as sp import statsmodels.api as sm from statsmodels.formula.api import ols import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import seaborn as sns # sns.set_context(&quot;poster&quot;) # sns.set(rc={&#39;figure.figsize&#39;: (16, 9.)}) sns.set_style(&quot;whitegrid&quot;) import pandas as pd pd.set_option(&#39;display.max_rows&#39;, 50) pd.set_option(&#39;display.max_columns&#39;, 120) pd.set_option(&#39;max_colwidth&#39;, None) from sklearn.model_selection import train_test_split from sklearn.linear_model import RidgeClassifier from sklearn.linear_model import LogisticRegression from xgboost import XGBClassifier from sklearn.metrics import plot_roc_curve from sklearn.metrics import plot_precision_recall_curve from stargazer.stargazer import Stargazer from IPython.core.display import HTML from habits.cleaning import list_s3_files from habits.processing import read_o2_data, classify_users, make_yX, print_info from habits.modelling import make_stargazer, make_roc %load_ext autoreload %autoreload 2 . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-2df7d6552618&gt; in &lt;module&gt; 29 from IPython.core.display import HTML 30 &gt; 31 from habits.cleaning import list_s3_files 32 from habits.processing import read_o2_data, classify_users, make_yX, print_info 33 from habits.modelling import make_stargazer, make_roc ModuleNotFoundError: No module named &#39;habits&#39; . Creating datetime indices . from dateutil.parser import parse parse(&#39;3 Apr 2020&#39;).month parse(&#39;3.4.2020&#39;).month parse(&#39;3.4.2020&#39;, dayfirst=True).month . 4 . 3 . 4 . dates = pd.date_range(start=&#39;1/1/2000&#39;, freq=&#39;A-DEC&#39;, periods = 100) values = np.random.randn(100) ts = pd.Series(values, index=dates) ts.head(10) . 2000-12-31 0.843433 2001-12-31 -0.391251 2002-12-31 0.149087 2003-12-31 0.086131 2004-12-31 -2.308920 2005-12-31 -0.569420 2006-12-31 0.033575 2007-12-31 0.449340 2008-12-31 0.846790 2009-12-31 0.633025 Freq: A-DEC, dtype: float64 . idx = pd.period_range(&#39;2018-1&#39;, &#39;2019-12&#39;, freq=&#39;Q-DEC&#39;) s = pd.Series(np.random.randn(len(idx)), index=idx) s.asfreq(&#39;d&#39;, how=&#39;start&#39;).asfreq(&#39;Q&#39;) . 2018Q1 -0.205351 2018Q2 -0.673207 2018Q3 -0.872625 2018Q4 2.045383 2019Q1 -0.696708 2019Q2 -0.798782 2019Q3 -1.904917 2019Q4 -0.436799 Freq: Q-DEC, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;M&#39;, kind=&#39;period&#39;).mean() . 2000-01 0.014726 2000-02 0.056242 2000-03 -0.016878 2000-04 -0.690987 Freq: M, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, freq=&#39;H&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;d&#39;).ohlc() . open high low close . 2000-01-01 -1.945039 | 2.231004 | -2.751270 | 0.272018 | . 2000-01-02 0.335163 | 2.173964 | -0.895301 | 0.995886 | . 2000-01-03 0.771501 | 1.399791 | -2.465960 | -0.030100 | . 2000-01-04 1.765987 | 1.765987 | -1.698415 | -0.056522 | . 2000-01-05 0.215849 | 1.192958 | 0.215849 | 0.786069 | . s.resample(&#39;min&#39;).asfreq().ffill() . 2000-01-01 00:00:00 -1.945039 2000-01-01 00:01:00 -1.945039 2000-01-01 00:02:00 -1.945039 2000-01-01 00:03:00 -1.945039 2000-01-01 00:04:00 -1.945039 ... 2000-01-05 02:56:00 1.192958 2000-01-05 02:57:00 1.192958 2000-01-05 02:58:00 1.192958 2000-01-05 02:59:00 1.192958 2000-01-05 03:00:00 0.786069 Freq: T, Length: 5941, dtype: float64 . data = df.reset_index(level=0).head(100).sort_index()[[&#39;amount&#39;]] data . amount . transaction_date . 2012-08-29 12.00 | . 2012-08-30 13.50 | . 2012-08-30 7.44 | . 2012-08-30 7.44 | . 2012-08-30 13.50 | . ... ... | . 2012-12-27 135.00 | . 2012-12-27 3.60 | . 2012-12-27 8.50 | . 2012-12-27 135.00 | . 2012-12-27 8.50 | . 100 rows × 1 columns . IPython / notebook shortcuts . clean_nb = !ls *2* clean_nb . [&#39;2.0-fgu-clean-and-split-data.ipynb&#39;] . %%timeit a = range(1000) . 174 ns ± 4.23 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) . %%writefile pythoncode.py import numpy def append_if_not_exists(arr, x): if x not in arr: arr.append(x) def some_useless_slow_function(): arr = list() for i in range(10000): x = numpy.random.randint(0, 10000) append_if_not_exists(arr, x) . Writing pythoncode.py . %pycat pythoncode.py . import numpy def append_if_not_exists(arr, x): if x not in arr: arr.append(x) def some_useless_slow_function(): arr = list() for i in range(10000): x = numpy.random.randint(0, 10000) append_if_not_exists(arr, x) . . I can transfer variables with any content from one notebook to the next (useful if you run a number of notebooks in sequence, for instance, and the ouput of one serves as the input of another). . var_to_pass_on = dfu.head() %store var_to_pass_on . Stored &#39;var_to_pass_on&#39; (DataFrame) . %store -r var_to_pass_on var_to_pass_on . user_id year_of_birth user_registration_date salary_range postcode gender soa_lower soa_middle user_uid . 0 3706 | 1967-01-01 | 2012-09-30 | NaN | XXXX 0 | M | NaN | NaN | 3706-0 | . 1 1078 | 1964-01-01 | 2011-11-29 | 20K to 30K | M25 9 | M | E01005038 | E02001043 | 1078-0 | . 2 232 | 1965-01-01 | 2010-09-09 | 30K to 40K | CM4 0 | M | E01021551 | E02004495 | 232-0 | . 3 6133 | 1968-01-01 | 2012-10-21 | 10K to 20K | SK12 1 | M | E01018665 | E02003854 | 6133-0 | . 4 7993 | 1961-01-01 | 2012-10-29 | 20K to 30K | LS17 8 | M | E01011556 | E02002344 | 7993-0 | . !conda list | grep pandas . pandas 1.0.1 py37h6c726b0_0 pandas-flavor 0.2.0 py_0 conda-forge . Using R and Python together . # %conda install tzlocal # %conda install simplegeneric . import rpy2 . # pandas2ri.activate() . %reload_ext rpy2.ipython . %R require(ggplot2) . R[write to console]: Loading required package: ggplot2 . array([0], dtype=int32) . import pandas as pd df = pd.DataFrame({ &#39;Letter&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;c&#39;], &#39;X&#39;: [4, 3, 5, 2, 1, 7, 7, 5, 9], &#39;Y&#39;: [0, 4, 3, 6, 7, 10, 11, 9, 13], &#39;Z&#39;: [1, 2, 3, 1, 2, 3, 1, 2, 3] }) . %%R -i df ggplot(data = df) + geom_point(aes(x = X, y= Y, color = Letter, size = Z)) . R[write to console]: Error in ggplot(data = df) : could not find function &#34;ggplot&#34; Calls: &lt;Anonymous&gt; -&gt; &lt;Anonymous&gt; -&gt; withVisible R[write to console]: In addition: R[write to console]: Warning messages: R[write to console]: 1: R[write to console]: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : R[write to console]: R[write to console]: there is no package called ‘ggplot’ R[write to console]: 2: R[write to console]: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : R[write to console]: R[write to console]: there is no package called ‘ggplot2’ . Error in ggplot(data = df) : could not find function &#34;ggplot&#34; Calls: &lt;Anonymous&gt; -&gt; &lt;Anonymous&gt; -&gt; withVisible . Printing virtually anything . happy_squirrels = !ls /Users/fgu/Library/Mobile Documents/com~apple~CloudDocs/fab/photos/squirrels/ . from IPython.display import display, Image for s in happy_squirrels: display(Image(&#39;/Users/fgu/Library/Mobile Documents/com~apple~CloudDocs/fab/photos/squirrels/&#39; + s, width=100)) . Write code to and load code from file . %%writefile test.py print(&#39;Hello&#39;) . Overwriting test.py . %pycat test.py . print(&#39;Hello&#39;) . Data from wide to long and back . data = pd.read_csv(&#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/macrodata.csv&#39;) data.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name = &#39;date&#39;) columns = pd.Index([&#39;realgdp&#39;, &#39;cpi&#39;, &#39;unemp&#39;, &#39;infl&#39;], name=&#39;item&#39;) df = data.reindex(columns=columns) df.index = periods.to_timestamp(&#39;D&#39;, &#39;End&#39;) dfl = df.stack().reset_index().rename(columns={0:&#39;value&#39;}) dfl . date item value . 0 1959-03-31 23:59:59.999999999 | realgdp | 2710.349 | . 1 1959-03-31 23:59:59.999999999 | cpi | 28.980 | . 2 1959-03-31 23:59:59.999999999 | unemp | 5.800 | . 3 1959-03-31 23:59:59.999999999 | infl | 0.000 | . 4 1959-06-30 23:59:59.999999999 | realgdp | 2778.801 | . ... ... | ... | ... | . 807 2009-06-30 23:59:59.999999999 | infl | 3.370 | . 808 2009-09-30 23:59:59.999999999 | realgdp | 12990.341 | . 809 2009-09-30 23:59:59.999999999 | cpi | 216.385 | . 810 2009-09-30 23:59:59.999999999 | unemp | 9.600 | . 811 2009-09-30 23:59:59.999999999 | infl | 3.560 | . 812 rows × 3 columns . dfl.set_index([&#39;date&#39;, &#39;item&#39;]).unstack() . value . item cpi infl realgdp unemp . date . 1959-03-31 23:59:59.999999999 28.980 | 0.00 | 2710.349 | 5.8 | . 1959-06-30 23:59:59.999999999 29.150 | 2.34 | 2778.801 | 5.1 | . 1959-09-30 23:59:59.999999999 29.350 | 2.74 | 2775.488 | 5.3 | . 1959-12-31 23:59:59.999999999 29.370 | 0.27 | 2785.204 | 5.6 | . 1960-03-31 23:59:59.999999999 29.540 | 2.31 | 2847.699 | 5.2 | . ... ... | ... | ... | ... | . 2008-09-30 23:59:59.999999999 216.889 | -3.16 | 13324.600 | 6.0 | . 2008-12-31 23:59:59.999999999 212.174 | -8.79 | 13141.920 | 6.9 | . 2009-03-31 23:59:59.999999999 212.671 | 0.94 | 12925.410 | 8.1 | . 2009-06-30 23:59:59.999999999 214.469 | 3.37 | 12901.504 | 9.2 | . 2009-09-30 23:59:59.999999999 216.385 | 3.56 | 12990.341 | 9.6 | . 203 rows × 4 columns . dfl.pivot(&#39;date&#39;, &#39;item&#39;, &#39;value&#39;) . item cpi infl realgdp unemp . date . 1959-03-31 23:59:59.999999999 28.980 | 0.00 | 2710.349 | 5.8 | . 1959-06-30 23:59:59.999999999 29.150 | 2.34 | 2778.801 | 5.1 | . 1959-09-30 23:59:59.999999999 29.350 | 2.74 | 2775.488 | 5.3 | . 1959-12-31 23:59:59.999999999 29.370 | 0.27 | 2785.204 | 5.6 | . 1960-03-31 23:59:59.999999999 29.540 | 2.31 | 2847.699 | 5.2 | . ... ... | ... | ... | ... | . 2008-09-30 23:59:59.999999999 216.889 | -3.16 | 13324.600 | 6.0 | . 2008-12-31 23:59:59.999999999 212.174 | -8.79 | 13141.920 | 6.9 | . 2009-03-31 23:59:59.999999999 212.671 | 0.94 | 12925.410 | 8.1 | . 2009-06-30 23:59:59.999999999 214.469 | 3.37 | 12901.504 | 9.2 | . 2009-09-30 23:59:59.999999999 216.385 | 3.56 | 12990.341 | 9.6 | . 203 rows × 4 columns . HDF5 . Following this video. . import h5py import numpy as np . Create a HDF5 file object, which works kind of like a Python dictionary. . f = h5py.File(&#39;demo.hdf5&#39;) . data = np.arange(10) data . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . f[&#39;array&#39;] = data . dset = f[&#39;array&#39;] . dset . &lt;HDF5 dataset &#34;array&#34;: shape (10,), type &#34;&lt;i8&#34;&gt; . dset[:] . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . dset[[1, 2, 5]] . array([1, 2, 5]) . dset.attrs . &lt;Attributes of HDF5 object at 4425393432&gt; . Atributes again have dictionarry structure, so can add attribute like so: . dset.attrs[&#39;sampling frequency&#39;] = &#39;Every other week between 1 Jan 2001 and 7 Feb 2010&#39; dset.attrs[&#39;PI&#39;] = &#39;Fabian&#39; . list(dset.attrs.items()) for i in dset.attrs.items(): print(i) . (&#39;sampling frequency&#39;, &#39;Every other week between 1 Jan 2001 and 7 Feb 2010&#39;) (&#39;PI&#39;, &#39;Fabian&#39;) . f.close() . f = h5py.File(&#39;demo.hdf5&#39;) . list(f.keys()) . [&#39;array&#39;] . dset = f[&#39;array&#39;] . hdf5 files are organised in a hierarchy - that&#39;s what &quot;h&quot; stands for. . dset.name . &#39;/array&#39; . root = f[&#39;/&#39;] . list(root.keys()) . [&#39;array&#39;] . f[&#39;dataset&#39;] = data . f[&#39;full/dataset&#39;] = data . grp = f[&#39;full&#39;] . &#39;dataset&#39; in grp . True . list(grp.keys()) . [&#39;dataset&#39;] . dset2 = f.create_dataset(&#39;/full/bigger&#39;, (10000, 1000, 1000, 1000), dtype=&#39;f&#39;, compression=&#39;gzip&#39;) . list(f[&#39;full&#39;].keys()) . [&#39;bigger&#39;, &#39;dataset&#39;] . Misc. . idx = pd.IndexSlice df.loc[idx[:, &quot;2014&quot;], :] . from scipy import stats s = np.arange(5) std = np.std(s) mean = np.mean(s) manual_z = (s - mean) / std scipy_z = stats.zscore(s) manual_z == scipy_z . array([ True, True, True, True, True]) . np.arange(32).reshape((4, 8)) . array([[ 0, 1, 2, 3, 4, 5, 6, 7], [ 8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]) . import random print(&quot;one&quot;) if random.randint(0, 1) else print(&quot;zero&quot;) . zero . ndraws = 1000 draws = np.random.randint(0, 2, ndraws) steps = np.where(draws &gt; 0, 1, -1) walk = steps.cumsum() # Find how long it took to make 10 steps in either direction idx = (np.abs(walk) &gt;= 10).argmax() print(&quot;It took {} steps to get to {}&quot;.format(idx, walk[idx])) . It took 57 steps to get to -10 . nwalks = 5000 ndraws = 1000 draws = np.random.randint(0, 2, size=(nwalks, ndraws)) steps = np.where(draws &gt; 0, 1, -1) walk = steps.cumsum(axis=1) # Find number of walks that cross 30 and the average crossing time hit30 = (np.abs(walk) &gt;= 30).any(1) crossing_times = (np.abs(walk[hit30]) &gt;= 30).argmax(1) print( &quot;{} walks cross 30, taking {} steps on average.&quot;.format( hit30.sum(), crossing_times.mean() ) ) . 3385 walks cross 30, taking 498.33353028064994 steps on average. . Grouping . df = pd.DataFrame({&#39;key1&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], &#39;key2&#39;: [&#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;], &#39;data1&#39;: np.random.randn(5), &#39;data2&#39;: np.random.randn(5)}) df . key1 key2 data1 data2 . 0 a | one | -0.722346 | 1.350070 | . 1 b | two | -0.864278 | 0.652911 | . 2 c | one | 0.875035 | 0.838726 | . 3 d | two | 1.420677 | -0.464896 | . 4 e | one | -0.789309 | 0.148121 | . for group, data in df.groupby(&#39;key1&#39;): print(group) print(data) . a key1 key2 data1 data2 0 a one -0.722346 1.35007 b key1 key2 data1 data2 1 b two -0.864278 0.652911 c key1 key2 data1 data2 2 c one 0.875035 0.838726 d key1 key2 data1 data2 3 d two 1.420677 -0.464896 e key1 key2 data1 data2 4 e one -0.789309 0.148121 . pieces = dict(list(df.groupby(&#39;key1&#39;))) pieces[&#39;a&#39;] . key1 key2 data1 data2 . 0 a | one | -0.722346 | 1.35007 | . grouped = df.groupby(df.dtypes, axis=1) for dtype, data in grouped: print(dtype) print(data) . float64 data1 data2 0 -0.722346 1.350070 1 -0.864278 0.652911 2 0.875035 0.838726 3 1.420677 -0.464896 4 -0.789309 0.148121 object key1 key2 0 a one 1 b two 2 c one 3 d two 4 e one . Solution to my common indexing problem (index with shorter boolean series) . Problem: I want to index my df based on a boolean series that is shorter than the length of the df. E.g. I have a subset of users that fulfill a condition and want to keep these only. . data = o2.sample(frac=0.05) . high_spender = data.groupby(&quot;user_id&quot;).amount.sum() &gt; 350 . . todrop = high_spender[~high_spender].index.values data.drop(todrop, level=0).sort_index() . transaction_id transaction_description amount auto_tag merchant_name account_type . user_id transaction_date . 20912 2013-12-18 12152457 | card purchase o2 uk ref xxx xxxxxx0000 bcc | 11.990000 | mobile | o2 | current | . 2014-04-17 17466459 | direct debit o2 gedxxxxx154 ddr | 44.160000 | mobile | o2 | current | . 2015-01-19 54324542 | direct debit o2 gedxxxx9154 ddr | 38.380001 | mobile | o2 | current | . 2015-03-24 80422755 | direct debit o2 gedxxxx9097 ddr | 34.799999 | mobile | o2 | current | . 2015-09-23 96952964 | counter credit &lt;mdbremoved&gt; o2 bgc | 54.500000 | mobile | o2 | current | . 2016-04-25 134730196 | direct debit o2 gedxxxx9097 ddr | 40.990002 | mobile | o2 | current | . 2016-05-17 138708140 | direct debit o2 gedxxxx1717 ddr | 30.000000 | mobile | o2 | current | . 2016-08-17 154735556 | direct debit o2 gedxxxx9154 ddr | 34.270000 | mobile | o2 | current | . 2016-10-07 163699913 | counter credit &lt;mdbremoved&gt; o2/ &lt;mdbremoved&gt; bgc | 68.059998 | mobile | o2 | current | . 160992 2014-09-29 38730708 | o2 uk cd 9531 | 25.000000 | mobile | o2 | current | . 2015-01-19 54406685 | o2 uk cd 9531 | 175.000000 | mobile | o2 | current | . 2015-01-19 54406686 | o2 uk cd 9531 | 202.539993 | mobile | o2 | current | . high_spender . user_id 8 False 659 False 1078 False 1146 False 2324 False ... 420102 False 421678 False 423912 False 424865 False 425830 False Name: amount, Length: 210, dtype: bool . hs = high_spender[high_spender].index.values mask = data.index.isin(hs, level=0) data[mask].sort_index() . transaction_id transaction_description amount auto_tag merchant_name account_type . user_id transaction_date . 20912 2013-12-18 12152457 | card purchase o2 uk ref xxx xxxxxx0000 bcc | 11.990000 | mobile | o2 | current | . 2014-04-17 17466459 | direct debit o2 gedxxxxx154 ddr | 44.160000 | mobile | o2 | current | . 2015-01-19 54324542 | direct debit o2 gedxxxx9154 ddr | 38.380001 | mobile | o2 | current | . 2015-03-24 80422755 | direct debit o2 gedxxxx9097 ddr | 34.799999 | mobile | o2 | current | . 2015-09-23 96952964 | counter credit &lt;mdbremoved&gt; o2 bgc | 54.500000 | mobile | o2 | current | . 2016-04-25 134730196 | direct debit o2 gedxxxx9097 ddr | 40.990002 | mobile | o2 | current | . 2016-05-17 138708140 | direct debit o2 gedxxxx1717 ddr | 30.000000 | mobile | o2 | current | . 2016-08-17 154735556 | direct debit o2 gedxxxx9154 ddr | 34.270000 | mobile | o2 | current | . 2016-10-07 163699913 | counter credit &lt;mdbremoved&gt; o2/ &lt;mdbremoved&gt; bgc | 68.059998 | mobile | o2 | current | . 160992 2014-09-29 38730708 | o2 uk cd 9531 | 25.000000 | mobile | o2 | current | . 2015-01-19 54406685 | o2 uk cd 9531 | 175.000000 | mobile | o2 | current | . 2015-01-19 54406686 | o2 uk cd 9531 | 202.539993 | mobile | o2 | current | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/02/27/learning.html",
            "relUrl": "/python/pandas/2021/02/27/learning.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post43": {
            "title": "Iterators and generators",
            "content": "Writing my own iterator pattern . (From the Python Cookbook recipee 4.3) . def frange(start, stop, increment): x = start while x &lt; stop: yield x x += increment rng = frange(1, 10, 2) next(rng) next(rng) . 3 . list(rng) . [5, 7, 9] . I&#39;m a little confused by this still. . def aritprog(begin, step, end=None): result = type(begin + step)(begin) forever = end is None index = 0 while forever or result &lt; end: yield result index += 1 result = begin + step * index a = aritprog(0, 5, 20) for a in a: print(a) . 0 5 10 15 . a = iter([1, 2, 3]) . import inspect def gen(x): yield x a = gen(5) print(inspect.getgeneratorstate(a)) next(a) print(inspect.getgeneratorstate(a)) try: next(a) except StopIteration: print(inspect.getgeneratorstate(a)) . GEN_CREATED GEN_SUSPENDED GEN_CLOSED . Using generators for line-by-line data processing for large files . Example here, see &quot;Case Study: Generators in a Database Conversion Utility&quot; at end of Chap 14 in Fluent Python for context. . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/02/27/iterators-and-generators.html",
            "relUrl": "/python/2021/02/27/iterators-and-generators.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post44": {
            "title": "Python data science idioms",
            "content": "When working with data, certain basic steps occur over and over. This is the place where I document my current best-practices. . from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Reorder dataframe columns . first = [&#39;date&#39;, &#39;yw&#39;, &#39;pcsector&#39;] rest = set(df.columns) - set(first) df = df[first + list(rest)] . Select a subset of users from a panel dataset based on user-level criteria . I have a folder of files which I want to clean and append. How to do this? | I want to convert a df column to datatime, how to do this (at read, using np, using Pandas)? What are tradeoffs? | . Sources . Fluent Python | Python Cookbook | Learning Python | The Hitchhiker&#39;s Guide to Python | Effective Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/02/27/idioms.html",
            "relUrl": "/python/2021/02/27/idioms.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post45": {
            "title": "Fast groupby operations",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext line_profiler %load_ext autoreload %autoreload 2 . path = &#39;/Users/fgu/tmp/mdb/data_777.parquet&#39; df = pd.read_parquet(path) print(df.shape) df.head() . (157287, 22) . user_id transaction_date amount transaction_description merchant_name tag gender up_tag account_id year_of_birth merchant_business_line salary_range latest_balance account_type credit_debit transaction_id bank postcode ym account_created user_registration_date account_last_refreshed . 0 777 | 2012-01-03 | 3.03 | aviva pa - d/d | aviva | life insurance | m | life insurance | 262916 | 1969.0 | aviva | 20k to 30k | 364.22 | current | debit | 688262 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 1 777 | 2012-01-03 | 6.68 | 9572 31dec11 , tesco stores 3345 , warrington gb - pos | tesco | food, groceries, household | m | food, groceries, household | 262916 | 1969.0 | tesco supermarket | 20k to 30k | 364.22 | current | debit | 688263 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 2 777 | 2012-01-03 | 10.27 | 9572 30dec11 , mcdonalds , restaurant , winwick road gb - pos | mcdonalds | dining and drinking | m | dining and drinking | 262916 | 1969.0 | mcdonalds | 20k to 30k | 364.22 | current | debit | 688264 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 3 777 | 2012-01-03 | 12.00 | 9572 31dec11 , tesco stores 3345 , warrington gb - pos | tesco | food, groceries, household | m | food, groceries, household | 262916 | 1969.0 | tesco supermarket | 20k to 30k | 364.22 | current | debit | 688265 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . 4 777 | 2012-01-03 | 400.00 | &lt;mdbremoved&gt; - s/o | no merchant | other account | m | other account | 262916 | 1969.0 | non merchant mbl | 20k to 30k | 364.22 | current | debit | 688261 | natwest bank | wa1 4 | 201201 | 2011-07-20 | 2011-07-20 | 2020-07-21 20:32:00 | . Basics . Boolean comparisons . df = sns.load_dataset(&#39;iris&#39;) . %timeit df.sepal_width &gt; 3 . 105 µs ± 2.61 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) . %timeit df.sepal_width.values &gt; 0 . 6.11 µs ± 181 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) . Given the above, the below is rather surprising: . %timeit df[df.sepal_width &gt; 3] . 335 µs ± 23.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . %timeit df[df.sepal_width.values &gt; 3] . 148 µs ± 6.14 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) . Group means . From https://cmdlinetips.com/2019/05/how-to-implement-pandas-groupby-operation-with-numpy/ . df = sns.load_dataset(&#39;iris&#39;) . %timeit df.groupby(&#39;species&#39;).sepal_length.mean() . 481 µs ± 11.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . %%timeit spec = df.species.values sl = df.sepal_length.values groups = df.species.unique() [(group, np.mean(sl[spec == group])) for group in groups] . 111 µs ± 7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2021/02/27/fast-groupby.html",
            "relUrl": "/python/pandas/2021/02/27/fast-groupby.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post46": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://fabiangunzinger.github.io/blog/jupyter/2021/02/27/example-post.html",
            "relUrl": "/jupyter/2021/02/27/example-post.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post47": {
            "title": "Error handling",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Only printing message . def divide(a, b): try: return a / b except ZeroDivisionError as e: print(e) divide(4, 0) . division by zero . Raise original error . def divide(a, b): try: return a / b except ZeroDivisionError as e: raise e divide(4, 0) . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-11-4ca87b7cf0dc&gt; in &lt;module&gt; 5 raise e 6 -&gt; 7 divide(4, 0) &lt;ipython-input-11-4ca87b7cf0dc&gt; in divide(a, b) 3 return a / b 4 except ZeroDivisionError as e: -&gt; 5 raise e 6 7 divide(4, 0) &lt;ipython-input-11-4ca87b7cf0dc&gt; in divide(a, b) 1 def divide(a, b): 2 try: -&gt; 3 return a / b 4 except ZeroDivisionError as e: 5 raise e ZeroDivisionError: division by zero . Raise different error type to be clearer that invalid value was supplied . def divide(a, b): try: return a / b except ZeroDivisionError as e: raise ValueError(&#39;Invalid inputs&#39;) divide(4, 0) . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-10-1fa9c1ba7c1d&gt; in divide(a, b) 2 try: -&gt; 3 return a / b 4 except ZeroDivisionError as e: ZeroDivisionError: division by zero During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) &lt;ipython-input-10-1fa9c1ba7c1d&gt; in &lt;module&gt; 5 raise ValueError(&#39;Invalid inputs&#39;) 6 -&gt; 7 divide(4, 0) &lt;ipython-input-10-1fa9c1ba7c1d&gt; in divide(a, b) 3 return a / b 4 except ZeroDivisionError as e: -&gt; 5 raise ValueError(&#39;Invalid inputs&#39;) 6 7 divide(4, 0) ValueError: Invalid inputs . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/02/27/error-handling.html",
            "relUrl": "/python/2021/02/27/error-handling.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post48": {
            "title": "Data visualisation",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . Matplotlib . Subplots from columns in df . Adapted from this post . with plt.style.context(&#39;seaborn-deep&#39;): fig, axes = plt.subplots(2, 2, figsize=(8, 8)) for ax, col in zip(axes.flatten(), accounts.columns): # zip does the magic! s = accounts[col] ax.hist(s, alpha=0.7) med, p75 = s.quantile([.5, .75]) ax.axvline(med, color=&#39;g&#39;, label=&#39;50th pct: &#39; + format(med, &#39;.0f&#39;)) ax.axvline(p75, color=&#39;orange&#39;, label=&#39;75th pct: &#39; + format(p75, &#39;.0f&#39;)) ax.legend() . NameError Traceback (most recent call last) &lt;ipython-input-1-3a95d9d76c1b&gt; in &lt;module&gt; -&gt; 1 with plt.style.context(&#39;seaborn-deep&#39;): 2 fig, axes = plt.subplots(2, 2, figsize=(8, 8)) 3 4 for ax, col in zip(axes.flatten(), accounts.columns): # zip does the magic! 5 s = accounts[col] NameError: name &#39;plt&#39; is not defined . Effect of holidays on births . From Python Data Science Handbook . file = &#39;https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv&#39; df = pd.read_csv(file).dropna() print(df.shape) df.head() . (15067, 5) . year month day gender births . 0 1969 | 1 | 1.0 | F | 4046 | . 1 1969 | 1 | 1.0 | M | 4440 | . 2 1969 | 1 | 2.0 | F | 4454 | . 3 1969 | 1 | 2.0 | M | 4548 | . 4 1969 | 1 | 3.0 | F | 4548 | . # Eliminate outliers using sigma-clipping pcts = np.percentile(df.births, [25, 50, 75]) mu, sigma = pcts[1], 0.74 * (pcts[2] - pcts[0]) df = df.query(&#39;(births &gt; @mu - 5 * @sigma) &amp; (births &lt; @mu + 5 * @sigma)&#39;) # Create datetime index df.index = pd.to_datetime(df.year * 10_000 + df.month * 100 + df.day, format=&#39;%Y%m%d&#39;) # Make pivot table with month-day index and mean of births column day_means = df.pivot_table(&#39;births&#39;, [df.index.month, df.index.day]) # Turn pivot index into year-month-day index (for leap year) for plotting from datetime import date day_means.index = [date(2020, month, day) for month, day in day_means.index] . fig, ax = plt.subplots(figsize=(12, 4)) style = dict(color=&#39;cornflowerblue&#39;, linewidth=4, style=&#39;-&#39;) day_means.plot(ax=ax, legend=None, **style) # Format ax.set(ylim=(3600, None), ylabel=&#39;Mean number of births&#39;) ax.xaxis.set_major_locator(mpl.dates.MonthLocator()) ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(bymonthday=15)) ax.xaxis.set_major_formatter(plt.NullFormatter()) ax.xaxis.set_minor_formatter(mpl.dates.DateFormatter(&#39;%h&#39;)) # Add text ax.text(&#39;2020-01-01&#39;, 3900, &quot;New year&#39;s day&quot;, ha=&#39;left&#39;) ax.text(&#39;2020-07-04&#39;, 4200, &#39;Independence day&#39;, ha=&#39;center&#39;) ax.text(&#39;2020-12-25&#39;, 3700, &#39;Christmas&#39;, ha=&#39;right&#39;); . Seatle cycling data . From JVDP&#39;s blog . file = &#39;https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD&#39; data = pd.read_csv(file, skiprows=1, names=[&#39;date&#39;, &#39;total&#39;, &#39;east&#39;, &#39;west&#39;], parse_dates=True, index_col=&#39;date&#39;) . data.describe() . total east west . count 67118.000000 | 67118.000000 | 67118.000000 | . mean 112.912527 | 51.559835 | 61.352692 | . std 144.160880 | 66.522811 | 89.768937 | . min 0.000000 | 0.000000 | 0.000000 | . 25% 14.000000 | 6.000000 | 7.000000 | . 50% 60.000000 | 28.000000 | 30.000000 | . 75% 147.000000 | 69.000000 | 74.000000 | . max 1097.000000 | 698.000000 | 850.000000 | . data.plot(); . weekly = data.resample(&#39;W&#39;).sum() weekly.plot(style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . daily = data.resample(&#39;D&#39;).sum() daily.rolling(window=30, center=True).sum().plot(style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . by_time = data.groupby(data.index.time).sum() hourly_ticks = 3 * 60 * 60 * np.arange(8) by_time.plot(xticks=hourly_ticks, style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . by_weekday = data.groupby(data.index.dayofweek).sum() by_weekday.index = [&#39;Mo&#39;, &#39;Tu&#39;, &#39;We&#39;, &#39;Th&#39;, &#39;Fr&#39;, &#39;Sa&#39;, &#39;So&#39;] by_weekday.plot(style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . mpl.get_configdir() . &#39;/Users/fgu/.matplotlib&#39; . wknd = np.where(data.index.dayofweek &gt; 4, &#39;weekend&#39;, &#39;weekday&#39;) hourly = data.groupby([wknd, data.index.time]).sum() fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 5)) hourly.loc[&#39;weekday&#39;].plot(ax=ax1, title=&#39;Weekdays&#39;, xticks=hourly_ticks, style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]) hourly.loc[&#39;weekend&#39;].plot(ax=ax0, title=&#39;Weekends&#39;, xticks=hourly_ticks, style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . USA.gov data from Bitly . From Python for Data Analysis . import pandas as pd import numpy as np import seaborn as sns sns.set() %config InlineBackend.figure_format =&#39;retina&#39; . path = &#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/bitly_usagov/example.txt&#39; df = pd.read_json(path, lines=True) df.head() . a c nk tz gr g h l al hh r u t hc cy ll _heartbeat_ kw . 0 Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... | US | 1.0 | America/New_York | MA | A6qOVH | wfLQtf | orofrog | en-US,en;q=0.8 | 1.usa.gov | http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/... | http://www.ncbi.nlm.nih.gov/pubmed/22415991 | 1.331923e+09 | 1.331823e+09 | Danvers | [42.576698, -70.954903] | NaN | NaN | . 1 GoogleMaps/RochesterNY | US | 0.0 | America/Denver | UT | mwszkS | mwszkS | bitly | NaN | j.mp | http://www.AwareMap.com/ | http://www.monroecounty.gov/etc/911/rss.php | 1.331923e+09 | 1.308262e+09 | Provo | [40.218102, -111.613297] | NaN | NaN | . 2 Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ... | US | 1.0 | America/New_York | DC | xxr3Qb | xxr3Qb | bitly | en-US | 1.usa.gov | http://t.co/03elZC4Q | http://boxer.senate.gov/en/press/releases/0316... | 1.331923e+09 | 1.331920e+09 | Washington | [38.9007, -77.043098] | NaN | NaN | . 3 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8)... | BR | 0.0 | America/Sao_Paulo | 27 | zCaLwp | zUtuOu | alelex88 | pt-br | 1.usa.gov | direct | http://apod.nasa.gov/apod/ap120312.html | 1.331923e+09 | 1.331923e+09 | Braz | [-23.549999, -46.616699] | NaN | NaN | . 4 Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... | US | 0.0 | America/New_York | MA | 9b6kNl | 9b6kNl | bitly | en-US,en;q=0.8 | bit.ly | http://www.shrewsbury-ma.gov/selco/ | http://www.shrewsbury-ma.gov/egov/gallery/1341... | 1.331923e+09 | 1.273672e+09 | Shrewsbury | [42.286499, -71.714699] | NaN | NaN | . Let&#39;s plot the most occuring time-zones. . counts = df.tz.str.replace(&#39;^$&#39;, &#39;Unknown&#39;).fillna(&#39;Missing&#39;).value_counts()[:10] sns.barplot(counts.values, counts.index); . Now, let&#39;s split the bars by operating system. . pd.Series.reverse = lambda self: self[::-1] # Cool trick from here: https://stackoverflow.com/a/46624694 df[&#39;os&#39;] = np.where(df.a.str.contains(&#39;Mac&#39;), &#39;Mac&#39;, &#39;Not Mac&#39;) agg_counts = (df.replace(&#39;^$&#39;, &#39;Unknown&#39;, regex=True) .groupby([&#39;tz&#39;, &#39;os&#39;]) .size() .unstack() .fillna(0)) indexer = agg_counts.sum(1).argsort() data = agg_counts.take(indexer[-10:]).reverse().stack() data.name = &#39;totals&#39; data = data.reset_index() sns.barplot(x=&#39;totals&#39;, y=&#39;tz&#39;, hue=&#39;os&#39;, data=data); . MovieLens 1M dataset . From Python for Data Analysis . !ls data/ml-1m . README movies.dat ratings.dat users.dat . import pandas as pd . path = &#39;data/ml-1m/&#39; unames = [&#39;user_id&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;occupation&#39;, &#39;zip&#39;] users = pd.read_table(path + &#39;users.dat&#39;, sep=&#39;::&#39;, engine=&#39;python&#39;, names=unames) rnames = [&#39;user_id&#39;, &#39;movie_id&#39;, &#39;rating&#39;, &#39;timestamp&#39;] ratings = pd.read_table(path + &#39;ratings.dat&#39;, sep=&#39;::&#39;, engine=&#39;python&#39;, names=rnames) mnames = [&#39;movie_id&#39;, &#39;title&#39;, &#39;genres&#39;] movies = pd.read_table(path + &#39;movies.dat&#39;, sep=&#39;::&#39;, engine=&#39;python&#39;, names=mnames) data = pd.merge(pd.merge(users, ratings), movies) data.head() . user_id gender age occupation zip movie_id rating timestamp title genres . 0 1 | F | 1 | 10 | 48067 | 1193 | 5 | 978300760 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 1 2 | M | 56 | 16 | 70072 | 1193 | 5 | 978298413 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 2 12 | M | 25 | 12 | 32793 | 1193 | 4 | 978220179 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 3 15 | M | 25 | 7 | 22903 | 1193 | 4 | 978199279 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 4 17 | M | 50 | 1 | 95350 | 1193 | 5 | 978158471 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . Calculate average ratings by gender . mean_ratings = data.pivot_table(values=&#39;rating&#39;, index=&#39;title&#39;, columns=&#39;gender&#39;, aggfunc=&#39;mean&#39;) mean_ratings.head() . gender F M . title . $1,000,000 Duck (1971) 3.375000 | 2.761905 | . &#39;Night Mother (1986) 3.388889 | 3.352941 | . &#39;Til There Was You (1997) 2.675676 | 2.733333 | . &#39;burbs, The (1989) 2.793478 | 2.962085 | . ...And Justice for All (1979) 3.828571 | 3.689024 | . Keep only movies with at least 200 ratings . ratings_count = data.groupby(&#39;title&#39;).size() active_titles = ratings_count[ratings_count &gt; 200].index mean_ratings = mean_ratings.loc[active_titles] # mean_ratings = mean_ratings.reindex(active_titles) # alternative . Above was mainly to practice, what I actually want is to exclude movies with fewer than 200 ratings from the very start . rating_count = data.groupby(&#39;title&#39;).size() active_movies = rating_count[rating_count &gt; 200].index data = data[data.title.isin(active_movies)] data.head() . user_id gender age occupation zip movie_id rating timestamp title genres . 0 1 | F | 1 | 10 | 48067 | 1193 | 5 | 978300760 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 1 2 | M | 56 | 16 | 70072 | 1193 | 5 | 978298413 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 2 12 | M | 25 | 12 | 32793 | 1193 | 4 | 978220179 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 3 15 | M | 25 | 7 | 22903 | 1193 | 4 | 978199279 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 4 17 | M | 50 | 1 | 95350 | 1193 | 5 | 978158471 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . Calculate ratings difference by gender (again, just for fun, and to compare to above result) . mean_ratings2 = data.pivot_table(&#39;rating&#39;, &#39;title&#39;, &#39;gender&#39;, &#39;mean&#39;) all(mean_ratings2 == mean_ratings) . True . Look at top movis by gender . mean_ratings.sort_values(&#39;F&#39;, ascending=False).head() . gender F M . title . Close Shave, A (1995) 4.644444 | 4.473795 | . Wrong Trousers, The (1993) 4.588235 | 4.478261 | . General, The (1927) 4.575758 | 4.329480 | . Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) 4.572650 | 4.464589 | . Wallace &amp; Gromit: The Best of Aardman Animation (1996) 4.563107 | 4.385075 | . mean_ratings.sort_values(&#39;M&#39;, ascending=False).head() . gender F M . title . Godfather, The (1972) 4.314700 | 4.583333 | . Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954) 4.481132 | 4.576628 | . Shawshank Redemption, The (1994) 4.539075 | 4.560625 | . Raiders of the Lost Ark (1981) 4.332168 | 4.520597 | . Usual Suspects, The (1995) 4.513317 | 4.518248 | . Calculate rating differences . mean_ratings[&#39;diff&#39;] = np.abs(mean_ratings[&#39;F&#39;] - mean_ratings[&#39;M&#39;]) mean_ratings.sort_values(&#39;diff&#39;, ascending=False).head() . gender F M diff . title . Dirty Dancing (1987) 3.790378 | 2.959596 | 0.830782 | . Good, The Bad and The Ugly, The (1966) 3.494949 | 4.221300 | 0.726351 | . To Wong Foo, Thanks for Everything! Julie Newmar (1995) 3.486842 | 2.795276 | 0.691567 | . Kentucky Fried Movie, The (1977) 2.878788 | 3.555147 | 0.676359 | . Jumpin&#39; Jack Flash (1986) 3.254717 | 2.578358 | 0.676359 | . Find movies with the most rating disagreement among all viwers . data.groupby(&#39;title&#39;).rating.std().sort_values(ascending=False).head() . title Plan 9 from Outer Space (1958) 1.455998 Texas Chainsaw Massacre, The (1974) 1.332448 Dumb &amp; Dumber (1994) 1.321333 Blair Witch Project, The (1999) 1.316368 Natural Born Killers (1994) 1.307198 Name: rating, dtype: float64 . Baby names . From Python for Data Analysis . !head data/names/yob1880.txt . Mary,F,7065 Anna,F,2604 Emma,F,2003 Elizabeth,F,1939 Minnie,F,1746 Margaret,F,1578 Ida,F,1472 Alice,F,1414 Bertha,F,1320 Sarah,F,1288 . import re files = !ls data/names/yob* pieces = [] columns = [&#39;name&#39;, &#39;sex&#39;, &#39;births&#39;] for file in files: frame = pd.read_csv(file, names=columns) year = int(re.findall(&#39; d+&#39;, file)[0]) frame[&#39;year&#39;] = year pieces.append(frame) names = pd.concat(pieces, ignore_index=True) names.head() . name sex births year . 0 Mary | F | 7065 | 1880 | . 1 Anna | F | 2604 | 1880 | . 2 Emma | F | 2003 | 1880 | . 3 Elizabeth | F | 1939 | 1880 | . 4 Minnie | F | 1746 | 1880 | . years = range(1880, 2019) pieces = [] columns = [&#39;name&#39;, &#39;gender&#39;, &#39;births&#39;] for year in years: path = &#39;data/names/yob%d.txt&#39; % year frame = pd.read_csv(path, names=columns) frame[&#39;year&#39;] = year pieces.append(frame) names = pd.concat(pieces, ignore_index=True) names.head() . name gender births year . 0 Mary | F | 7065 | 1880 | . 1 Anna | F | 2604 | 1880 | . 2 Emma | F | 2003 | 1880 | . 3 Elizabeth | F | 1939 | 1880 | . 4 Minnie | F | 1746 | 1880 | . Plot number of girls and boys born over time . names.pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;gender&#39;, &#39;sum&#39;).plot(); . Add a proportion column . def add_prop(group): group[&#39;prop&#39;] = group.births / group.births.sum() return group names = names.groupby([&#39;gender&#39;, &#39;year&#39;]).apply(add_prop) names.head() . name gender births year prop . 0 Mary | F | 7065 | 1880 | 0.077642 | . 1 Anna | F | 2604 | 1880 | 0.028617 | . 2 Emma | F | 2003 | 1880 | 0.022012 | . 3 Elizabeth | F | 1939 | 1880 | 0.021309 | . 4 Minnie | F | 1746 | 1880 | 0.019188 | . Check that prop sums to 1 for each year-gender group . names.groupby([&#39;gender&#39;, &#39;year&#39;]).prop.sum() . gender year F 1880 1.0 1881 1.0 1882 1.0 1883 1.0 1884 1.0 ... M 2014 1.0 2015 1.0 2016 1.0 2017 1.0 2018 1.0 Name: prop, Length: 278, dtype: float64 . Keep only top 1000 names per gender and year . def top1000(group): return group.sort_values(by=&#39;births&#39;, ascending=False)[:1000] top1000 = names.groupby([&#39;gender&#39;, &#39;year&#39;]).apply(top1000).reset_index(drop=True) . Let&#39;s look at the number of births per year for common names . subset = [&#39;John&#39;, &#39;Harry&#39;, &#39;Mary&#39;, &#39;Marilyn&#39;] (names.pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;name&#39;, &#39;sum&#39;)[subset] .plot(subplots=True, figsize=(12,10), title=&#39;Number of births per year&#39;)); . Plot suggest that common names have become less popular. This could be either because people use other names instead, or becasue people just use more names overall. Let&#39;s look into this. First by looking at the proportion of birhts for the top 1000 names. . (top1000.pivot_table(&#39;prop&#39;, &#39;year&#39;, &#39;gender&#39;, &#39;sum&#39;) .plot(title=&#39;Proportion of top 1000 names of all births&#39;, figsize=(6, 5), yticks=np.linspace(0, 1.2, 13))); . It&#39;s clear from the above plot that the top 1000 names are becoming a smaller proportion of all names over time, indicating that naming diversity is increasing. To corroborate this, let&#39;s look at the number of names that account for 50 percent of all births in each year for each sex. . boys2018 = top1000[(top1000.year == 2018) &amp; (top1000.gender == &#39;M&#39;)] boys2018.sort_values(&#39;prop&#39;, ascending=False).prop.cumsum().searchsorted(.5) + 1 . 149 . def get_quantile_count(group, q=0.5): group = group.sort_values(&#39;prop&#39;, ascending=False) return group.prop.cumsum().searchsorted(.5) + 1 diversity = top1000.groupby([&#39;gender&#39;, &#39;year&#39;]).apply(get_quantile_count).unstack(level=0) diversity.plot(figsize=(8, 6)); . Explore the last-letter revolution . import matplotlib.pyplot as plt def get_last_letter(name): return name[-1] names[&#39;last_letter&#39;] = names.name.map(get_last_letter) table = names.pivot_table(&#39;births&#39;, &#39;last_letter&#39;, [&#39;sex&#39;, &#39;year&#39;], &#39;sum&#39;) subtable = table.reindex(columns=[1960, 1990, 2018], level=&#39;year&#39;) subtable = subtable / subtable.sum() subtable fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,10)) subtable[&#39;M&#39;].plot(kind=&#39;bar&#39;, ax=ax1) subtable[&#39;F&#39;].plot(kind=&#39;bar&#39;, ax=ax2); . For boys names, d, n, and y have changed markedly in popularity over the past six decads. Let&#39;s look at this more closely. . table[&#39;M&#39;].reindex([&#39;d&#39;, &#39;n&#39;, &#39;y&#39;]).T.plot(figsize=(8, 6), linewidth=5); . Leslie-like names have evolved from being boy to being girl names . def normalise(df): return df.div(df.sum(1), axis=&#39;rows&#39;) (names[names.name.str.lower().str.contains(&#39;^lesl&#39;)] .pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;sex&#39;, &#39;sum&#39;) .pipe(normalise) .plot(figsize=(8, 6), linewidth=5)); . Evolution of Molly and Fabian . names . name sex births year last_letter . 0 Mary | F | 7065 | 1880 | y | . 1 Anna | F | 2604 | 1880 | a | . 2 Emma | F | 2003 | 1880 | a | . 3 Elizabeth | F | 1939 | 1880 | h | . 4 Minnie | F | 1746 | 1880 | e | . ... ... | ... | ... | ... | ... | . 1957041 Zylas | M | 5 | 2018 | s | . 1957042 Zyran | M | 5 | 2018 | n | . 1957043 Zyrie | M | 5 | 2018 | e | . 1957044 Zyron | M | 5 | 2018 | n | . 1957045 Zzyzx | M | 5 | 2018 | x | . 1957046 rows × 5 columns . (names[names.name.isin([&#39;Molly&#39;, &#39;Fabian&#39;])] .pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;name&#39;, &#39;sum&#39;) .plot(figsize=(10, 8), linewidth=8)); . Baby names in Switzerland . ls data/ch-names/ . f.xlsx m.xlsx . genders = [&#39;f&#39;, &#39;m&#39;] def rename_cols(df, names): df.columns = names return df pieces = [] columns = [&#39;name&#39;, &#39;year&#39;, &#39;births&#39;, &#39;rank&#39;] for gender in genders: path = &#39;data/ch-names/%s.xlsx&#39; % gender data = (pd.read_excel(path, header=[2, 3], index_col=0, skipfooter=5) .stack(level=0) .reset_index() .pipe(rename_cols, columns) .assign(gender=gender)) pieces.append(data) names = pd.concat(pieces, ignore_index=True) names . name year births rank gender . 0 Emma | 1998 | 88 | 78 | f | . 1 Emma | 1999 | 80 | 85 | f | . 2 Emma | 2000 | 127 | 46 | f | . 3 Emma | 2001 | 116 | 46 | f | . 4 Emma | 2002 | 147 | 36 | f | . ... ... | ... | ... | ... | ... | . 41995 Rúben | 2014 | 5 | 1071 | m | . 41996 Rúben | 2015 | 4 | 1263 | m | . 41997 Rúben | 2016 | 6 | 972 | m | . 41998 Rúben | 2017 | 15 | 515 | m | . 41999 Rúben | 2018 | 7 | 924 | m | . 42000 rows × 5 columns . files = !ls data/ch-names/* pieces = [] columns = [&#39;name&#39;, &#39;year&#39;, &#39;births&#39;] for file in files: frame = (pd.read_excel(file, header=[2, 3], index_col=0, skipfooter=5) .stack(level=[0]).reset_index().drop(&#39;Rang&#39;, axis=1)) frame.columns = columns gender = file[-6] frame[&#39;gender&#39;] = gender pieces.append(frame) table = pd.concat(pieces, ignore_index=True) table.head() . name year births gender . 0 Emma | 1998 | 88 | f | . 1 Emma | 1999 | 80 | f | . 2 Emma | 2000 | 127 | f | . 3 Emma | 2001 | 116 | f | . 4 Emma | 2002 | 147 | f | . Add proportions and rank columns . def calc_prop(group): group[&#39;prop&#39;] = group.births / group.births.sum() * 100 return group table = table.groupby([&#39;year&#39;, &#39;gender&#39;]).apply(calc_prop) table[&#39;rank&#39;] = table.groupby([&#39;year&#39;, &#39;gender&#39;]).births.rank(method=&#39;min&#39;) . Most popular names by year . def top_n(group, n=5): return group.sort_values(&#39;births&#39;, ascending=False)[:n] num_names = 10 years = [1998] (table.groupby([&#39;year&#39;, &#39;gender&#39;]) .apply(top_n, num_names) .drop([&#39;year&#39;, &#39;gender&#39;], axis=1) .reset_index(level=2, drop=True) .loc[years]) . name births prop rank . year gender . 1998 m Luca | 648 | 2.214173 | 1000.0 | . m David | 528 | 1.804141 | 999.0 | . m Simon | 511 | 1.746053 | 998.0 | . m Marco | 435 | 1.486366 | 997.0 | . m Joel | 424 | 1.448780 | 996.0 | . m Michael | 407 | 1.390692 | 995.0 | . m Lukas | 392 | 1.339438 | 994.0 | . m Nicolas | 375 | 1.281350 | 993.0 | . m Fabian | 368 | 1.257432 | 992.0 | . m Kevin | 350 | 1.195927 | 991.0 | . w Laura | 607 | 2.405675 | 1000.0 | . w Celine | 414 | 1.640774 | 999.0 | . w Sarah | 407 | 1.613031 | 998.0 | . w Jessica | 391 | 1.549620 | 997.0 | . w Lea | 375 | 1.486208 | 996.0 | . w Michelle | 357 | 1.414870 | 995.0 | . w Sara | 344 | 1.363348 | 994.0 | . w Vanessa | 320 | 1.268231 | 993.0 | . w Lara | 308 | 1.220672 | 992.0 | . w Julia | 299 | 1.185003 | 991.0 | . What&#39;s going on with girl names ending in &#39;a&#39;? . table[table.name.str.endswith(&#39;a&#39;)] . 0 False 1 False 2 False 3 False 4 False ... 41995 False 41996 False 41997 False 41998 False 41999 False Name: name, Length: 42000, dtype: bool . girl_names = [&#39;Emma&#39;, &#39;Ivy&#39;, &#39;Audrey&#39;, &#39;Vivien&#39;] . def get_last_letter(name): return name[-1] table[&#39;last_letter&#39;] = table.name.map(get_last_letter) last_letters = table[table.gender == &#39;w&#39;].pivot_table(&#39;births&#39;, &#39;last_letter&#39;, &#39;year&#39;, &#39;sum&#39;) last_letters = last_letters / last_letters.sum() last_letters.T[&#39;a&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a32573f50&gt; . Popularity of names over time . import matplotlib.pyplot as plt def plot_births(names, axis, values=&#39;prop&#39;): (table[(table.name.isin(names))] .pivot_table(values, &#39;year&#39;, &#39;name&#39;, &#39;sum&#39;) .plot(ax=axis, xticks=range(1998, 2019), rot=45, linewidth=7)) fig, (left, right) = plt.subplots(1, 2, figsize=(20, 8)) girl_names = [&#39;Emma&#39;] boy_names = [&#39;Liam&#39;, &#39;Leo&#39;, &#39;Theo&#39;, &#39;Fabian&#39;] plot_births(girl_names, left, values=&#39;births&#39;) plot_births(boy_names, right, values=&#39;births&#39;) . USDA Food database . From Python for Data Analysis . ls data/usda_foods.json . chap14-examples.ipynb data/ . import json db = json.load(open(&#39;data/usda_foods.json&#39;)) len(db) . db[0].keys() . dict_keys([&#39;id&#39;, &#39;description&#39;, &#39;tags&#39;, &#39;manufacturer&#39;, &#39;group&#39;, &#39;portions&#39;, &#39;nutrients&#39;]) . pd.DataFrame(db[0][&#39;nutrients&#39;]) . value units description group . 0 25.180 | g | Protein | Composition | . 1 29.200 | g | Total lipid (fat) | Composition | . 2 3.060 | g | Carbohydrate, by difference | Composition | . 3 3.280 | g | Ash | Other | . 4 376.000 | kcal | Energy | Energy | . ... ... | ... | ... | ... | . 157 1.472 | g | Serine | Amino Acids | . 158 93.000 | mg | Cholesterol | Other | . 159 18.584 | g | Fatty acids, total saturated | Other | . 160 8.275 | g | Fatty acids, total monounsaturated | Other | . 161 0.830 | g | Fatty acids, total polyunsaturated | Other | . 162 rows × 4 columns . Produce df with info variables . info_keys = [&#39;description&#39;, &#39;id&#39;, &#39;manufacturer&#39;, &#39;group&#39;] new_col_names = {&#39;description&#39;: &#39;food&#39;, &#39;group&#39;: &#39;fgroup&#39;} info = pd.DataFrame(db, columns=info_keys) info = info.rename(columns=new_col_names) info.head() . food id manufacturer fgroup . 0 Cheese, caraway | 1008 | | Dairy and Egg Products | . 1 Cheese, cheddar | 1009 | | Dairy and Egg Products | . 2 Cheese, edam | 1018 | | Dairy and Egg Products | . 3 Cheese, feta | 1019 | | Dairy and Egg Products | . 4 Cheese, mozzarella, part skim milk | 1028 | | Dairy and Egg Products | . Create a df with all the nutrient info for each food . new_col_names = {&#39;description&#39;: &#39;nutrient&#39;, &#39;group&#39;: &#39;ngroup&#39;} pieces = [] for rec in db: nuts = pd.DataFrame(rec[&#39;nutrients&#39;]) nuts[&#39;id&#39;] = rec[&#39;id&#39;] pieces.append(nuts) nutrients = pd.concat(pieces, ignore_index=True) nutrients = nutrients.rename(columns=new_col_names) nutrients.head() . value units nutrient ngroup id . 0 25.18 | g | Protein | Composition | 1008 | . 1 29.20 | g | Total lipid (fat) | Composition | 1008 | . 2 3.06 | g | Carbohydrate, by difference | Composition | 1008 | . 3 3.28 | g | Ash | Other | 1008 | . 4 376.00 | kcal | Energy | Energy | 1008 | . Combine info and nutrient dfs . foods = pd.merge(info, nutrients).drop_duplicates() foods.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 375176 entries, 0 to 389354 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 food 375176 non-null object 1 id 375176 non-null int64 2 manufacturer 293054 non-null object 3 fgroup 375176 non-null object 4 value 375176 non-null float64 5 units 375176 non-null object 6 nutrient 375176 non-null object 7 ngroup 375176 non-null object dtypes: float64(1), int64(1), object(6) memory usage: 25.8+ MB . Plot nutrient content by food group . nutrient = &#39;Carbohydrate, by difference&#39; (foods[foods.nutrient.isin([nutrient])] .groupby(&#39;fgroup&#39;) .value .quantile(.5) .sort_values() .plot(kind=&#39;barh&#39;, figsize=(8, 6)) .set(xlabel=&#39;Median %s content&#39; % nutrient, ylabel=&#39;&#39;)); . Find the food with the maxium nutritional content for each nutrient . get_max = lambda x: x.loc[x.value.idxmax()] foods.groupby(&#39;nutrient&#39;).apply(get_max).head() . food id manufacturer fgroup value units nutrient ngroup . nutrient . Adjusted Protein Baking chocolate, unsweetened, squares | 19078 | | Sweets | 12.900 | g | Adjusted Protein | Composition | . Alanine Gelatins, dry powder, unsweetened | 19177 | | Sweets | 8.009 | g | Alanine | Amino Acids | . Alcohol, ethyl Alcoholic beverage, distilled, all (gin, rum, ... | 14533 | | Beverages | 42.500 | g | Alcohol, ethyl | Other | . Arginine Seeds, sesame flour, low-fat | 12033 | | Nut and Seed Products | 7.436 | g | Arginine | Amino Acids | . Ash Desserts, rennin, tablets, unsweetened | 19225 | | Sweets | 72.500 | g | Ash | Other | . FEC 2012 presidential election campaign contributions . From Python for Data Analysis . columns = {&#39;cand_nm&#39;:&#39;candidate&#39;, &#39;contbr_city&#39;:&#39;city&#39;, &#39;contbr_occupation&#39;:&#39;occupation&#39;, &#39;contb_receipt_amt&#39;:&#39;amount&#39;, &#39;contb_receipt_dt&#39;:&#39;date&#39;} parties = {&#39;Bachmann, Michelle&#39;: &#39;r&#39;, &#39;Romney, Mitt&#39;: &#39;r&#39;, &#39;Obama, Barack&#39;: &#39;d&#39;, &quot;Roemer, Charles E. &#39;Buddy&#39; III&quot;: &#39;r&#39;, &#39;Pawlenty, Timothy&#39;: &#39;r&#39;, &#39;Johnson, Gary Earl&#39;: &#39;r&#39;, &#39;Paul, Ron&#39;: &#39;r&#39;, &#39;Santorum, Rick&#39;: &#39;r&#39;, &#39;Cain, Herman&#39;: &#39;r&#39;, &#39;Gingrich, Newt&#39;: &#39;r&#39;, &#39;McCotter, Thaddeus G&#39;: &#39;r&#39;, &#39;Huntsman, Jon&#39;: &#39;r&#39;, &#39;Perry, Rick&#39;: &#39;r&#39;} path = &#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/fec/P00000001-ALL.csv&#39; fec = (pd.read_csv(path) [columns.keys()] .rename(columns=columns) .assign(party = lambda df: df.candidate.map(parties))) . /Users/fgu/miniconda3/envs/basics/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . fec.head() . candidate city occupation amount date party . 0 Bachmann, Michelle | MOBILE | RETIRED | 250.0 | 20-JUN-11 | r | . 1 Bachmann, Michelle | MOBILE | RETIRED | 50.0 | 23-JUN-11 | r | . 2 Bachmann, Michelle | LANETT | INFORMATION REQUESTED | 250.0 | 05-JUL-11 | r | . 3 Bachmann, Michelle | PIGGOTT | RETIRED | 250.0 | 01-AUG-11 | r | . 4 Bachmann, Michelle | HOT SPRINGS NATION | RETIRED | 300.0 | 20-JUN-11 | r | . Compare total donations . fec.groupby(&#39;party&#39;).amount.sum() . party d 1.335026e+08 r 1.652488e+08 Name: amount, dtype: float64 . Compare donations by occupation . occ_mapping = {&#39;INFORMATION REQUESTED&#39;:&#39;Not provided&#39;, &#39;INFORMATION REQUESTED PER BEST EFFORTS&#39;: &#39;Not provided&#39;} f = lambda x: occ_mapping.get(x, x) fec.occupation = fec.occupation.map(f) fec . candidate city occupation amount date party . 0 Bachmann, Michelle | MOBILE | RETIRED | 250.0 | 20-JUN-11 | r | . 1 Bachmann, Michelle | MOBILE | RETIRED | 50.0 | 23-JUN-11 | r | . 2 Bachmann, Michelle | LANETT | Not provided | 250.0 | 05-JUL-11 | r | . 3 Bachmann, Michelle | PIGGOTT | RETIRED | 250.0 | 01-AUG-11 | r | . 4 Bachmann, Michelle | HOT SPRINGS NATION | RETIRED | 300.0 | 20-JUN-11 | r | . ... ... | ... | ... | ... | ... | ... | . 1001726 Perry, Rick | INFO REQUESTED | Not provided | 5000.0 | 29-SEP-11 | r | . 1001727 Perry, Rick | INFO REQUESTED | BUSINESS OWNER | 2500.0 | 30-SEP-11 | r | . 1001728 Perry, Rick | INFO REQUESTED | Not provided | 500.0 | 29-SEP-11 | r | . 1001729 Perry, Rick | INFO REQUESTED | LONGWALL MAINTENANCE FOREMAN | 500.0 | 30-SEP-11 | r | . 1001730 Perry, Rick | INFO REQUESTED | Not provided | 2500.0 | 31-AUG-11 | r | . 1001731 rows × 6 columns . Discretise donations into buckets for contribution size . Seaborn . From Python Data Science Handbook . Create a simple random walk plot using default . rng = np.random.RandomState(2312) x = np.linspace(0, 10, 500) y = np.cumsum(rng.randn(500, 6), axis=0) plt.plot(x, y) plt.legend(&#39;abcdef&#39;, ncol=2, loc=&#39;lower left&#39;); . import seaborn as sns sns.set() plt.plot(x, y); plt.legend(&#39;abcdef&#39;, ncol=2, loc=&#39;lower left&#39;); . Explore marathon data . from datetime import timedelta # Read data def convert_time(s): h, m, s = map(int, s.split(&#39;:&#39;)) return timedelta(hours=h, minutes=m, seconds=s) file = &#39;https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv&#39; df = pd.read_csv(file, converters={&#39;split&#39;: convert_time, &#39;final&#39;: convert_time}) # Add times in seconds df[&#39;split_sec&#39;] = df.split.astype(int) / 1E9 df[&#39;final_sec&#39;] = df.final.astype(int) / 1E9 print(df.shape) df.head() . (37250, 6) . age gender split final split_sec final_sec . 0 33 | M | 01:05:38 | 02:08:51 | 3938.0 | 7731.0 | . 1 32 | M | 01:06:26 | 02:09:28 | 3986.0 | 7768.0 | . 2 31 | M | 01:06:49 | 02:10:42 | 4009.0 | 7842.0 | . 3 38 | M | 01:06:16 | 02:13:45 | 3976.0 | 8025.0 | . 4 31 | M | 01:06:32 | 02:13:59 | 3992.0 | 8039.0 | . with sns.axes_style(&#39;white&#39;): g = sns.jointplot(&#39;split_sec&#39;, &#39;final_sec&#39;, data=df, kind=&#39;hex&#39;) g.ax_joint.plot(np.linspace(4000, 17000), np.linspace(8000, 33000), &#39;:&#39;) . df[&#39;split_frac&#39;] = 1 - 2 * df.split_sec / df.final_sec sns.distplot(df.split_frac, kde=False) plt.axvline(0, linestyle=&#39;--&#39;); . g = sns.PairGrid(df, hue=&#39;gender&#39;, vars=[&#39;age&#39;, &#39;split_frac&#39;, &#39;final_sec&#39;, &#39;split_sec&#39;]) g.map(sns.scatterplot, alpha=0.5) g.add_legend(); . sns.kdeplot(df.split_frac[df.gender==&#39;M&#39;], label=&#39;Men&#39;, shade=True) sns.kdeplot(df.split_frac[df.gender==&#39;W&#39;], label=&#39;Women&#39;, shade=True); . sns.violinplot(&#39;gender&#39;, &#39;split_frac&#39;, data=df); . df[&#39;age_dec&#39;] = df.age.map(lambda age: age // 10 * 10) sns.violinplot(&#39;age_dec&#39;, &#39;split_frac&#39;, hue=&#39;gender&#39;, split=True, data=df); . g = sns.lmplot(&#39;final_sec&#39;, &#39;split_frac&#39;, col=&#39;gender&#39;, data=df, markers=&#39;.&#39;, scatter_kws=dict(color=&#39;cornflowerblue&#39;)) g.map(plt.axhline, y=0.1, color=&#39;k&#39;, ls=&#39;:&#39;); . Sources . Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/dataviz/2021/02/27/dataviz.html",
            "relUrl": "/python/pandas/dataviz/2021/02/27/dataviz.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post49": {
            "title": "Concurrency",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Theory . Elements: . RAM | memory controller (interface between cup and ram) | CPU | Kernel . Lowest level of software. It manages cpu resources, file system, memory resources, drivers, networking, etc. | . | Program . Code and data structures that can be executed (like newspaper) | . | . Cores (units of cpu?) . Multi core: instead of dividing up cpu time among different processes, can actually run stuff in parallel, but within each core, the process is exactly the same as with only one core. | . | Process . A logical container that tells kernel what program to run. Process has id, priority, status, memory space, etc.. Kernel uses that info to allocate cpu time to each program. Because happens at milisecond level, creates illusion of concurrency (comic analogy). | A program that competes for CPU resources (e.g. a web browser, or a bit of cleaning code) | The program in execution. Given that program is composed of multiple threads, program is program + state of all threads executing the program | . | Threads . Threads run within a single process, to allow it to do multiple things at once (like a newspaper) | They are lines of control cursing through the code and data structures of the program (called a thread of execution through the program). Akin to one reader scanning through a section of the newspaper. | Multiple threads: different lines of control cursing through the program (multiple readers reading different sections of the newspaper). | Potential conflict: threads operate on (read/update) same data structures. Solution: locks | A ... running within a program (e.g. reading data, processing, writing to disk) | . | . Type of tasks: . CPU-bound tasks: using CPU to crunch numbers . | I/O-bound tasks: waiting for some I/O operations to complete (downloading files from internet, reading and writing to file-system) . | . Theading: . Gives speed ups for I/O-bound tasks | . Multiprocessing: . Run CPU-bound tasks in prallel | . Usage . os.cpu_count() . 8 . Sources . Udacity on process vs threads | Gary explains - processes and threads | Gary explains - what is a kernel | Corey Schafer on threading | Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/02/27/concurrency.html",
            "relUrl": "/python/2021/02/27/concurrency.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post50": {
            "title": "Beautiful code",
            "content": "Sources to look at: . Hitchhiker&#39;s guide (link below) | IPython cookbook | . &quot;Terseness and obscurity are the limits where brevity should stop&quot; -- The Hitchhiker&#39;s Guide to Python . Use common design patterns . Use extra variables and functions instead of comments . Naming . Be descriptive. | Use plain and unabbreviated words. | Be concise and omit needless words (e.g. &quot;get&quot;, &quot;calculate&quot;) | . Add a docstring to each function . I roughly follow PEP-257 and Google. | Omit docstring if function is short and obvious (the one below would easily qualify...). | . def sum_numbers(num1=0, num2=1): &quot;&quot;&quot;Sum two numbers. Arguments: num1: the first number (default, 0) num2: the second number (default, 1) Return: Sum of numbers. &quot;&quot;&quot; return num1 + num2 . One function performs one task . import pandas as pd df = pd.DataFrame({&#39;data&#39;: [1, 2, 3, 4]}) # bad def calc_and_print_stats(df): mean = df.data.mean() maximum = df.data.max() print(f&#39;Mean is {mean}&#39;) print(f&#39;Max is {maximum}&#39;) calc_and_print_stats(df) . Mean is 2.5 Max is 4 . def calc_stats(df): return df.data.mean(), df.data.max() def print_stats(stats): print(f&#39;Mean is {stats[0]}&#39;) print(f&#39;Max is {stats[1]}&#39;) stats = calc_stats(df) print_stats(stats) . Mean is 2.5 Max is 4 . Principles . Based on this . 1. . Return a value | Such as True, to show that the function completed when there is nothing obvious to return. . Keep them short | Less than 50 lines as a rule of thumb. . Idempotent and pure | Idempotent functions return the same output for a given input every time, pure functions are idempotent and have no side-effects. . Main sources . The Hitchhiker&#39;s Guide to Python, code style | IPython cookbook, writing high-quality Python code | Google style guide | Jeff Knupp post | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/02/27/beautiful-code.html",
            "relUrl": "/python/2021/02/27/beautiful-code.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post51": {
            "title": "Vim troubleshooting",
            "content": "Vim troubleshooting . CSS indent not working . Vim recognised css files, but used my global indent of 4 spaces rather than a file-type specific indent of 2 spaces. | h: filetype notes that filetype detection files are located in the runtime path. Going there, I found the ftplugins folder that contains the default file settings. Looking at css.vim makes clear that it doesn’t set any tabstop settings, which explains why the defaults were used. | Googling something along the lines of “custom filetype indent vim” let me to this SO answer, which helpfully links to h: filetype-plugins. Once there, it was easy to find the relevant section, ftplugin-overrule that documents how to add custom filetype settings. This is what I did, and it worked like a charm. | .",
            "url": "https://fabiangunzinger.github.io/blog/2021/02/16/vim-troubleshooting.html",
            "relUrl": "/2021/02/16/vim-troubleshooting.html",
            "date": " • Feb 16, 2021"
        }
        
    
  
    
        ,"post52": {
            "title": "Python idioms collection",
            "content": "Python idioms collection . :q .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/02/10/idioms.html",
            "relUrl": "/python/2021/02/10/idioms.html",
            "date": " • Feb 10, 2021"
        }
        
    
  
    
        ,"post53": {
            "title": "Linux",
            "content": "Notes on useful linux functionality as reference for my future self. . Getting help and finding stuff . man &lt;command&gt; opens the manual for &lt;command&gt;. | man -k &lt;search term&gt; lists all commands with &lt;search term&gt; in the manual pages. | Search inside manual works as in vim. | . Basics . A process is a running instance of a program. | Everything is a fiele (the keyboard is read-only, the screen write only) | . File handling . file lists file type of all files in a directory. | wc returns the number of words, lines, and bytes of the input file. Options -w, -l, -c return any one of those counts, -m returns the number of characters. (Remember difference between wc -w &lt;filename&gt; and wc -w &lt; &lt;filename&gt;: former prints file name, latter redirects file content to command anonymously, so prints result only.) | cut print certain columns of input file. | sed (stream edit) offers vim-like search and replace on data. | uniq removes duplicates from data. | egrep for regex-based filtering. | . Processes . top to list most memory-intensive processes. | ps lists processes running in current terminal, use -aux option to print all running processes (use | grep to filter output). | kill [signal] &lt;process id&gt; to kill a process, use -9 signal to force if required. | ctrl-z to move current job to background, jobs to list running background jobs, fg &lt;job id&gt; to move job to foreground. | . Bash scripting . Variables . &#39; interpret all content literally, &quot; allow for variable substitution. | $( command ) saves command output into a variable. | export var makes var available to child process. | /dev/stdin reads input from pipe. | . Arithmetic . let assigns result of expression to a variable. | expr prints result of expression. | $(( expression )) returns the result of expression. | ${#var} returns the length of var in characters. | . Functions . `function_name () { }` is the basic format (there is also `function function_name {`, but I prefer this. | . Permissions . Three actions: r (read), w (write), x (execute). | Three types of people: owner or user (u), group (g), and others (o). (a) applies to all types. | Permission info is 10 characters long: first character is file type (- for file, d for directory), the remaining ones are rwx permissions for owner, group, and others, with letter indicating permission on, hyphen indicating permission off. | Changing persmission: chmod &lt;for whom&gt;&lt;add or remove&gt;&lt;permission type&gt;. Example: chmod g+w adds write permission for group, chmod u-x removes execute permission for owner, chmod a+rwx grants all permission to everyone. chmod stands for change file mode bits. | Shortcuts: Remember the following: | . Octal Binary . 0 | 000 | . 1 | 001 | . 2 | 010 | . 3 | 011 | . 4 | 100 | . 5 | 101 | . 6 | 110 | . 7 | 111 | . This is useful because we can use the binary numbers to refer to rwx and the Octal ones as shortcuts (e.g. 5 is r-x). Further using the order of users as ugo, and using one Octal shortcut for each user, we can quickly set permissions for all users (e.g. 753 is rwxr-x-wx). . | Directory permissions: r means you can read content (e.g. do ls), w means you can write (e.g. create files or subdirectories), and x means you can enter (e.g. cd). . | . Sources . Ryan’s bash-scripting tutorial | Ryan’s linux tutorial | .",
            "url": "https://fabiangunzinger.github.io/blog/tools/linux/2021/02/08/linux.html",
            "relUrl": "/tools/linux/2021/02/08/linux.html",
            "date": " • Feb 8, 2021"
        }
        
    
  
    
        ,"post54": {
            "title": "Workspace",
            "content": "title: My workspace tags: [tools] — . My workspace setup for doing data science. . Managing projects: . iterm2 | itermocil for project management | iterm2 tabls to switch between projects | . Code testing and data exploration: . Jupyter lab | . Writing code: . vim | .",
            "url": "https://fabiangunzinger.github.io/blog/2021/01/31/workspace.html",
            "relUrl": "/2021/01/31/workspace.html",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post55": {
            "title": "Unit testing",
            "content": "Resources . Pytest | Unit testing for data science | .",
            "url": "https://fabiangunzinger.github.io/blog/python/testing/2021/01/21/unit-testing.html",
            "relUrl": "/python/testing/2021/01/21/unit-testing.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post56": {
            "title": "Error handling",
            "content": "%config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . A brief reminder of different ways to handle errors in Python. . Print message only . def divide(a, b): try: return a / b except ZeroDivisionError as e: print(e) divide(4, 0) . division by zero . Raise original error (traceback and message) . def divide(a, b): try: return a / b except ZeroDivisionError as e: raise e divide(4, 0) . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-11-4ca87b7cf0dc&gt; in &lt;module&gt; 5 raise e 6 -&gt; 7 divide(4, 0) &lt;ipython-input-11-4ca87b7cf0dc&gt; in divide(a, b) 3 return a / b 4 except ZeroDivisionError as e: -&gt; 5 raise e 6 7 divide(4, 0) &lt;ipython-input-11-4ca87b7cf0dc&gt; in divide(a, b) 1 def divide(a, b): 2 try: -&gt; 3 return a / b 4 except ZeroDivisionError as e: 5 raise e ZeroDivisionError: division by zero . Raise different error type to be clearer that invalid value was supplied . def divide(a, b): try: return a / b except ZeroDivisionError as e: raise ValueError(&#39;invalid inputs; division by zero is undefined&#39;) divide(4, 0) . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-5-b41638ecf455&gt; in divide(a, b) 2 try: -&gt; 3 return a / b 4 except ZeroDivisionError as e: ZeroDivisionError: division by zero During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) &lt;ipython-input-5-b41638ecf455&gt; in &lt;module&gt; 5 raise ValueError(&#39;invalid inputs; division by zero is undefined&#39;) 6 -&gt; 7 divide(4, 0) &lt;ipython-input-5-b41638ecf455&gt; in divide(a, b) 3 return a / b 4 except ZeroDivisionError as e: -&gt; 5 raise ValueError(&#39;invalid inputs; division by zero is undefined&#39;) 6 7 divide(4, 0) ValueError: invalid inputs; division by zero is undefined . Sources . Fluent Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/01/15/error-handling.html",
            "relUrl": "/python/2021/01/15/error-handling.html",
            "date": " • Jan 15, 2021"
        }
        
    
  
    
        ,"post57": {
            "title": "hdf5",
            "content": "import h5py import numpy as np . Notes on basic hdf5 use. . Create a file . f = h5py.File(&#39;demo.hdf5&#39;, &#39;w&#39;) . data = np.arange(10) data . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . f[&#39;array&#39;] = data . dset = f[&#39;array&#39;] . dset . &lt;HDF5 dataset &#34;array&#34;: shape (10,), type &#34;&lt;i8&#34;&gt; . dset[:] . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . dset[[1, 2, 5]] . array([1, 2, 5]) . Add additional data . f[&#39;dataset&#39;] = data . f[&#39;full/dataset&#39;] = data . list(f.keys()) . [&#39;array&#39;, &#39;dataset&#39;, &#39;full&#39;] . grp = f[&#39;full&#39;] . &#39;dataset&#39; in grp . True . list(grp.keys()) . [&#39;dataset&#39;] . Create dataset . dset = f.create_dataset(&#39;/full/bigger&#39;, (10000, 1000, 1000, 1000), compression=&#39;gzip&#39;) . Set attributes . dset.attrs . &lt;Attributes of HDF5 object at 140618810188336&gt; . Atributes again have dictionarry structure, so can add attribute like so: . dset.attrs[&#39;sampling frequency&#39;] = &#39;Every other week between 1 Jan 2001 and 7 Feb 2010&#39; dset.attrs[&#39;PI&#39;] = &#39;Fabian&#39; . list(dset.attrs.items()) for i in dset.attrs.items(): print(i) . (&#39;PI&#39;, &#39;Fabian&#39;) (&#39;sampling frequency&#39;, &#39;Every other week between 1 Jan 2001 and 7 Feb 2010&#39;) . Open file . f.close() . f = h5py.File(&#39;demo.hdf5&#39;, &#39;r&#39;) . list(f.keys()) . [&#39;array&#39;, &#39;dataset&#39;, &#39;full&#39;] . dset = f[&#39;array&#39;] . hdf5 files are organised in a hierarchy - that&#39;s what the &quot;h&quot; stands for. . dset.name . &#39;/array&#39; . root = f[&#39;/&#39;] . list(root.keys()) . [&#39;array&#39;, &#39;dataset&#39;, &#39;full&#39;] . list(f[&#39;full&#39;].keys()) . [&#39;bigger&#39;, &#39;dataset&#39;] . Sources . Managing Large Datasets with Python and HDF5 - O&#39;Reilly Webcast | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2021/01/12/hdf5.html",
            "relUrl": "/python/2021/01/12/hdf5.html",
            "date": " • Jan 12, 2021"
        }
        
    
  
    
        ,"post58": {
            "title": "Software design principles",
            "content": "Fundamentals . A function should do one and only one thing (and – as a rule of thumb – be no longer than 50 lines of code). . | Don’t reapeat yourself. (Don’t copy and paste more than once.) . | . SOLID . Single-responsibility principle . A module (usually source file) should only have one reason to change – it should be responsible to a single actor that can demand changes. . | Example: an employee class that produces outputs for the finance and HR departments violates the principle, as both the CFO and the CHO might demand changes that then unintenionally affects the output seen by the other. . | Solution: Separate code that different actors depend on. . | Corollary: don’t reuse a function for two different outputs because it does the same thing, but because it does the same thing is used in all instances for outputs used by the same actor. (Above, the CFO might want to tweak how regular hours are calculated. If the same function is used for HR, this will affect the calculations that HR gets.) . | .",
            "url": "https://fabiangunzinger.github.io/blog/2021/01/04/clean-architecture.html",
            "relUrl": "/2021/01/04/clean-architecture.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post59": {
            "title": "My (n)vim setup",
            "content": "My (n)vim setup . Mostly for my future self, this post documents the main choices I made when setting up vim. . Use Neovim . I use neovim rather than vim because it seems to be the future; it’s open source, uses lua rather than vim-script, and is used by people I implicitly trust. From browsing on the web, it seems like many people are contemplating switching over, and as someone who’s just starting out, it seems like there is no good reason to carefully set up vim with an expectation to switch in the future. | . YouCompleteMe for code completion . - Often doesn’t work with Anaconda Python, and I seem to be one of those cases. Followed the suggestion in the link. . I first tried compiling with /usr/bin/python3, but this didn’t work. | I then tried /usr/local/bin/python3.9, following this, which seems to have worked. - - | . iTerm . https://iterm2.com/index.html | . Vim . Dough Black’s good vimrc . | Using help: https://vim.fandom.com/wiki/Learn_to_use_help | Idiomatic VIM | Awesome vimrc: https://github.com/amix/vimrc | Vim as Python IDE: https://realpython.com/vim-and-python-a-match-made-in-heaven/ . | :help vimrc-intro . | .",
            "url": "https://fabiangunzinger.github.io/blog/tools/vim/2020/12/27/vim-setup.html",
            "relUrl": "/tools/vim/2020/12/27/vim-setup.html",
            "date": " • Dec 27, 2020"
        }
        
    
  
    
        ,"post60": {
            "title": "Numpy essentials",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . Questions . What&#39;s the main difference between a Python list and a NumPy (or C) array? What is the main trade-off involved? | Do array slices return a view or a copy? What can this be useful for? | What are four methods to access and modify data in an array? | What two features make NumPy arrays more efficient storage containers than lists? | . Answers . The overhead: Python lists are references to objects, each of which has information like type, size, memory location, etc. NumPy arrays can only store a single data type and thus don&#39;t need this extra overhead for each element. The tradeoff is flexibility vs speed: Python lists are very flexible (can store different types) but slow, NumPy arrays can only store homogenous data but are fast. | A view, which allows you to operate on a subset of a large dataset without copying it. | Indexing, slicing, boolean indexing, fancy indexing. | They have less overhead (see above) and elements are stored in memory contiguously, which means they can be read faster. | . True of false? . Indexing and reshaping . a = np.arange(1, 10) b = a.reshape(3, 3) b[0, 0] = 99 a[0] == 1 . False . a = np.arange(3) (a.reshape(1, 3) == a[:, np.newaxis]).all() . False . Slicing . a = np.arange(10) all(a[5:1:-2] == [5, 3]) . True . Explanation: slicing works like [start:stop:stride], with start being included and stop being excluded in the result, and with defaults being, respectively, 0, length of array, and 1. If the stride is negative, then the defaults for start and stop get reversed, and start is now the endpoint of array but will still be included (it&#39;s still the start element), and stop is now the start point (but, if specified, will still be ommitted). . Fancy indexing . a = np.array([1, 2, 3, 4, 5]) idx = [3, 2, 0] all(a[idx] == [4, 3, 2]) . False . 2d fancy indexing . X = np.arange(12).reshape(3, 4) X . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . row = np.array([1, 2]) col = np.array([2, 3]) (X[row, col] == [6, 10]).all() . False . Fancy indexing and simple indexing . all(X[2, [3, 2, 1]] == [11, 10, 9]) . True . Fancy indexing and slicing . (X[1:, [1, 2]] == [[5, 6], [9, 10]]).all() . True . Fancy indexing and masking . mask = np.array([1, 0, 1]) rows = np.array([1, 2]) (X[rows[:, np.newaxis], mask] == [[5, 4, 5], [9, 8, 9]]).all() . True . x = np.zeros(3) idx = [0, 1, 1, 1] x[idx] += 1 y = np.zeros_like(x) np.add.at(y, idx, 1) (x == y).all() . False . x = np.array([4, 3, 1, 5, 2]) (x[np.argsort(x)] == np.sort(x)).all() . True . x = np.arange(1, 5) all(x.reshape(-1, 1) == x.reshape(len(x), 1)) . True . x = np.arange(2) all(x.repeat(2) == np.tile(x, 2)) . False . all(np.arange(2).repeat([1, 2]) == np.array([0, 1, 1])) . True . a = np.arange(5) i = [0, 4] a.put(i, [99, 88]) all(a.take(i) == [99, 88]) . True . a = np.array([[10, 30], [70, 90]]) imax = np.expand_dims(np.argmax(a, axis=1), axis=1) np.put_along_axis(a, imax, 99, axis=0) (a == [[10, 30], [70, 99]]).all() . False . As exercise, replace column mins with 55. . a = np.arange(6).reshape(2, 3) (a[::-1, ::-1] == [[2, 1, 0], [5, 4, 3]]).all() . False . x = np.array([3, 1, 4, 5, 2]) all(np.partition(x, 2) == x[np.argpartition(x, 2)]) . True . Summing values . Lessons: . NumPy methods perform best on NumPy arrays; builtins, on lists. | np.add.reduce() is twice as fast as np.sum() | . mylist, myrange = range(1000), np.arange(1000) . %timeit np.add.reduce(myrange) . 1.33 µs ± 3.88 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) . %timeit np.sum(myrange) . 3.39 µs ± 107 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) . %timeit sum(mylist) . 11.6 µs ± 175 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) . %timeit functools.reduce(operator.add, mylist) . 43.9 µs ± 301 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each) . Broadcasting . First, it&#39;s helpful to know how NumPy labels dimensions. The image below makes this clear. . . Image from Elegant SciPy . Next, let&#39;s look at how broadcasting works: . . Image from Python Data Science Handbook . Determining broadcasting compatibility: . Comparing dimensions from right to left, ignoring different number of dimensions, arrays are compatible if the dimension of either array is 1 or if they match. | . 1D . x = np.arange(1, 5).reshape(-1, 1) y = np.arange(2, 6).reshape(1, -1) x * y . Explanation: -1 automatically determines the dimension in which it occurs based on all other specified dimensions. When creating x above, we create an np-array of length 4 and then specify that we want a single column while specifying -1 for the row dimension, so we end up with four rows. . 2D . a = np.arange(12).reshape(4, -1) # demeaning columns a - a.mean(0) # demeaning rows row_means = a.mean(1) a - row_means[:, np.newaxis] . 3D . rng = np.random.default_rng(2312) a = rng.integers(1, 10, size=24).reshape(4, 3, 2) a . Taking means along a dimension . What does it mean to take the mean of the zeroth dimension? It means to &quot;collapse&quot; that dimension down to size 0 or to &quot;flatten&quot; the array in that dimension, and to take the mean of each &quot;stack&quot; of values that was flattened. In the image above, imagine pressing down vertically from the top of the array, flattening the zeroth dimension and ending up with a shape of (3, 2). As we do this, we take the mean of each of the 3 x 2 = 6 stacks of values that we compressed on our way down. These are the means of dimension zero. In our array a, the stack in the top right corner, [0, 0], is [1, 6, 6, 1], with a mean of 3.5, so the [0, 0] element of our new flattened shape is 9, as we can see below. . mean0 = a.mean(0) mean0 . Demeaning an axis . To demean axis 0 of a we can now simply subtract our means from the original shape. (If we were to take the mean again, each element in the resulting (3, 2) array would be 0, so demeaning worked.) . (a - mean0) . (a - mean0).mean(0) . Broadcasting along an axis . We just did this above. The reason demeaning worked is because by broadcasting rules, our smaller (3, 2) array of means got padded in position 0 to (1, 3, 2), and then broadcasted (stretched) along the first dimension to match the (4, 3, 2) shape of a. Thus, to broadcast along axis 0, we need an array of shape (3, 2) or (1, 3, 2). Similarly, to broadcast along axis 1, we need an array of shape (4, 1, 2); to broadcast along axis 2, an array of shape (4, 3, 1). To practice, let&#39;s demean axis 1. . a . mean1 = a.mean(1) mean1 . This has shape (4, 2), which means we can&#39;t demean directly. . a - mean1 . Why didn&#39;t this work? The smaller (4, 2) array got padded in position 0 to (1, 4, 2), which can&#39;t be expanded to match the (4, 3, 2) shape of a. What we need -- and we already knew this -- is an array of shape (4, 1, 2). We can produce one by simply adding a dimension to our mean array. . mean1 = mean1[:, np.newaxis, :] mean1 . Visually, in the image above, we have now separated out the two means of each of the four layers of values, and can now broadcast these along dimension 1. Another way of thinking about this is to think of each of the our layers as its own 2D shape. In that case, all we did was calculate column means by calculating means along the first axis (just as we would in a single 2D shape, except that the first axis there would be axis 0 instead of 1), and now we are broadcasting these mean values along the rows to demean the columns. . a - mean1 . Again, we can see that the mean of each of the 4 x 2 = 8 blocks of size 3 is 0, just as we&#39;d expect. . Just as an exercise, let&#39;s demean axis 2, too: . a - a.mean(2)[:, :, np.newaxis] . Setting array values . arr = np.zeros((4, 3)) arr[:] = 5 arr . columns = np.array([1, 2, 3, 4]) arr[:] = columns[:, np.newaxis] arr . arr[:2] = [[8], [9]] arr . Understanding ravel() . Basics . Ravel (meaning to untangle) flattens arrays. | ravel() returns a one dimensional array (i.e. a list) of the values of the input array in the specified order. | Basically: row major order (also C order) proceeds row-wise, column major order (also Fortran order) proceeds column-wise. | NumPy usually stores arrays in row-order, so that ravel() can produce a view without the need to produce a copy. If an array is stored differently (maybe because it was created from a slice), ravel() might have to produce a copy first. | Especially when working with higher dimensional data, the following is a helpful reminder of how order determines the result: row major order traverses data from high to low dimensions; column major, from low to high dimensions. | . a = np.array([[1, 2, 3], [4, 5, 6]]) a.ravel() . array([1, 2, 3, 4, 5, 6]) . Different order types . a.ravel(order=&#39;C&#39;) . array([1, 2, 3, 4, 5, 6]) . a.ravel(order=&#39;F&#39;) . array([1, 4, 2, 5, 3, 6]) . a.ravel(order=&#39;A&#39;) . array([1, 2, 3, 4, 5, 6]) . a.ravel(order=&#39;K&#39;) . array([1, 2, 3, 4, 5, 6]) . (a.ravel() == a.flatten()).all() and (a.flatten() == a.reshape(-1)).all() . True . Then on to https://stackoverflow.com/questions/38143717/groupby-in-python-pandas-fast-way . x = np.arange(12).reshape(3, 1, 4) x . array([[[ 0, 1, 2, 3]], [[ 4, 5, 6, 7]], [[ 8, 9, 10, 11]]]) . x.ravel() . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . x.ravel(order=&#39;F&#39;) . array([ 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11]) . ravel_multi_index() . tbc . Fluency exercises . Code snippets are from Jake Vanderplas&#39; Python Data Science Handbook. . Draw and plot a 100 points from a multivariate normal distribution and highlight a random sample of 20 . rng = np.random.default_rng(2312) mean = [0, 0] cov = [[1, 2], [2, 5]] data = rng.multivariate_normal(mean, cov, size=100) sample = rng.choice(data, 20) fmt = dict(s=150, facecolor=&#39;none&#39;, edgecolor=&#39;green&#39;) plt.scatter(data[:,0], data[:,1], alpha=0.5) plt.scatter(sample[:,0], sample[:,1], alpha=0.5, **fmt); . Manually draw a histogram of 1000 random values using only numpy functions and plt.plot(). . rng = np.random.default_rng(2312) x = rng.normal(size=1000) # sort data into 20 bins bins = np.linspace(-5, 5, 20) labels = bins.searchsorted(x) # count observations by label counts = np.zeros_like(bins) np.add.at(counts, labels, 1) plt.plot(bins, counts, drawstyle=&#39;steps&#39;, lw=5); . Write a simple selection sort algorithm . def selection_sort(x): for i in range(len(x)): swap = i + np.argmin(x[i:]) x[i], x[swap] = x[swap], x[i] return x x = np.array([1, 4, 3, 2, 5]) selection_sort(x) . array([1, 2, 3, 4, 5]) . Explain in words why a, b = b, a works. Because of Python&#39;s evaluation order when performing value assignment, its tuple creation syntax, and tuple unpacking. When performing value assignment, Python evaluates the right-hand side first, and in this case produces the tuple (b, a) because two comma-separated values are evaluated as a tuple. Next, Python assigns the tuple to the the left-hand side. Because left-hand side consists of more than one element Python attempts tupe unpacking, assigning the first value of the tuple to the first element on the left-hand side, and the second element to the second. . Use np and plt functions to calculate the k nearest neighbours of 10 random points and visualise the result . rng = np.random.default_rng(2312) x = rng.random(size=(10, 2)) # calculate squared distances squared_dist = ((x[np.newaxis, :, :] - x[:, np.newaxis, :]) ** 2).sum(-1) # identify k nearest neighbhours k = 2 nearest_partition = np.argpartition(squared_dist, k + 1) # plot points and connections to nearest neighbours plt.scatter(x[:, 0], x[:, 1]) for i in range(len(x)): for j in nearest_partition[i, :k + 1]: plt.plot(*zip(x[i], x[j]), color=&#39;green&#39;) . Understanding the solution . The most mind-bending bit for me is the calculation of the point-wise differences. To wrap my head around this, I start with a simple case: . a = np.arange(5) a . To get the element-wise differences for a one-dimensional array, it&#39;s easier to visualise what to do: turn the array into a column and row vector and then use broadcasting to calculate each of the differences (broadcasting will expand each of the vectors into a 5x5 matrix, and then perform element-wise matrix subtraction). . diffs = a[:, np.newaxis] - a[np.newaxis, :] diffs . From here, we can then square the values and retrieve the nearest neighbour for each element of the original array: . k = 2 squared_diffs = diffs ** 2 nearest_partition = np.argpartition(squared_diffs, k + 1, axis=1) nearest_partition . I still occasionally get tripped up when interpreting the above: focusing on the first row, the first element, 1, says that in the partitioned array, element 1 from the squared_diffs array will be in position zero. The next element, 0, says that element zero from squared_diffs will be in position one, and so on. With this, we can now extract the k nearest neighbours for each element in our original array. . for i in range(len(a)): neighbours = [] for j in nearest_partition[i, :k + 1]: if a[j] != a[i]: neighbours.append(a[j]) print(a[i], neighbours) . To understand how to find nearest neighbouts in three dimensions, let&#39;s start with a toy example. . a = np.arange(4).reshape(2, 2) a . Intuitively, what we want to do is build two cubes -- one built from the original array broadcasted vertically and one from the original array flipped on one of its edges and then broadcasted horizontally -- and then perform element-wise subtraction. Or, more acurately, we want to create arrays such that broadcasting builds appropriate cubes for us before it performs the element-wise subtraction. But for intuitions sake, let&#39;s build the cubes ourselves, so we can see what happens. . vertical_cube = np.broadcast_to(a[np.newaxis, :, :], (2, 2, 2)) vertical_cube . horizontal_cube = np.broadcast_to(a[:, np.newaxis, :], (2, 2, 2)) horizontal_cube . a[:, None, :] - a[:, :, None] . We can now do the same for our original array. . x . x[np.newaxis, :, :] . x[:, np.newaxis, :] . When we subtract the second from the first array, broadcasting will expand both arrays to shape (10, 10, 2), thus building our vertical (from the first array) and horizontal (from the second array) cubes on which it then performs element-wise subtraction. (We didn&#39;t actually need to create the new zeroth dimension in our first array, because broadcasting pads the shorter array with new dimensions on the left and would thus perform this step for us.) The result is what we want: the difference in each dimension for each of the ten points to all of the ten points. Once we have this, we can square the values and sum them along dimension 2 (the last dimension, hence the use of -1 in the code above), which gives us the distances between the points. . x[np.newaxis, :, :] - x[:, np.newaxis, :] . Find index of values in array . I want to find a list of indices representing the index of each row of one array in another array (From here). . x = np.array([[4, 2], [9, 3], [8, 5], [3, 3], [5, 6]]) tofind = np.array([[4, 2], [3, 3], [5, 6]]) # desired result: [0, 3, 4] . result = [] for a in tofind: i = np.argwhere([((x - a) == 0).all(1)])[0][1] result.append(i) result . np.where((x == tofind[:, np.newaxis]).all(-1))[1] . Sources . Python Data Science Handbook | Python for Data Analysis | Elegant SciPy | .",
            "url": "https://fabiangunzinger.github.io/blog/python/numpy/2020/12/14/numpy-essentials.html",
            "relUrl": "/python/numpy/2020/12/14/numpy-essentials.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post61": {
            "title": "AWS",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Loading data from S3 . import s3fs . List content of bucket . bucket = &#39;fgu-mdb&#39; fs = s3fs.S3FileSystem() fs.ls(bucket) . [&#39;fgu-mdb/data_000.csv&#39;] . Read file from S3 . file = &#39;data_000.csv&#39; df = pd.read_csv(f&#39;s3://{bucket}/{file}&#39;, sep=&#39;|&#39;) df.shape . (1000, 27) . Read data with selected profile . If there are multiple profile in aws credential file, can select desired one as below. In this case, I select different profile depending on where code is being run. Botocore is required because Pandas uses it to read s3 files, and, by default, uses the aws default profile to read data. If, as below, I instead want Pandas to read data using another profile, I need to use a botocore session. . profile = &#39;tracker-fgu&#39; if platform.system() == &#39;Linux&#39;: profile = &#39;default&#39; session = botocore.session.Session(profile=profile) fs = s3fs.S3FileSystem(session=session) fs.ls(&#39;/&#39;) path = &#39;path-to-file&#39; df = pd.read_csv(fs.open(path)) . Sources .",
            "url": "https://fabiangunzinger.github.io/blog/python/aws/2020/12/02/aws.html",
            "relUrl": "/python/aws/2020/12/02/aws.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post62": {
            "title": "Simulations",
            "content": "import numpy as np . Simulating loan repayments using Monte Carlo simulation . Entirely based on this excellent QuantEcon tutorial. . A small company makes loans of up to 25k to be repaid a year later. 75 percent of loans are repaid in full, 20 percent of loans are repaid in half, the rest are not repaid. The company discounts the future at an annual rate of 5 percent. How much would the company be willing to loan if it wants to break even on average? . We can calculate this by hand. One year from now, the average repayment is 0.75(25000) + 0.2(12500) + 0.05(0) = 21250. Because that repayment occurs a year from now while the loan is made today, the company calculates the repayment&#39;s present value as: (1/1.05)21250 = 20238.1. Assuming that loan repayment is independent of loan size, then this is the loan size for which the company will break even on average (if it makes enough loans). . We can verify this using a simulation: . def repayment_simulator(n, r=0.05, full_repayment=25_000, partial_repayment=12_500): &quot;&quot;&quot;Simulate present value of repayments for N loans.&quot;&quot;&quot; repayments = np.zeros(n) outcome = np.random.rand(n) full_repayments = outcome &lt;= 0.75 repayments[full_repayments] = full_repayment partial_repayments = (outcome &lt;= 0.95) &amp; ~full_repayments repayments[partial_repayments] = partial_repayment return (1 / (1 + r)) * repayments np.mean(repayment_simulator(10_000_000)) . 20237.311904761867 . Instead of breaking even, the company is now looking for the largest loan size that will give it a 95 percent chance of being profitable in a year it makes 250 loans. . def simulate_year(n, k): &quot;&quot;&quot;Simulate a year with N loans k times.&quot;&quot;&quot; year_avgs = np.zeros(k) for year in range(k): year_avgs[year] = np.mean(repayment_simulator(n)) return year_avgs np.percentile(simulate_year(250, 10_000), [5, 50, 95]) . array([19523.80952381, 20238.0952381 , 20904.76190476]) . The result tells us that in 95 percent of simulated years, the average repayment is larger than 19,523. Hence, if the company wants to make a profit with 95 percent probability, it should loan no more than this amount. . Sources . QuantEcon | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/11/03/simulations.html",
            "relUrl": "/python/2020/11/03/simulations.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post63": {
            "title": "Cool Python tricks",
            "content": "Coerce input to type of something else . type(&#39;s&#39;)(5) . &#39;5&#39; . Sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/10/18/cool-tricks.html",
            "relUrl": "/python/2020/10/18/cool-tricks.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post64": {
            "title": "Built in heroes",
            "content": "Title inspired by this David Beazley talk. My notes on useful built in functions. . import functools import itertools import operator . Functools . partial . print(operator.mul(2, 3)) tripple = partial(mul, 3) tripple(2) . 6 . 6 . Iterable unpacking . When looping over a list of records (maybe of unequal length), we can access each records elements directly using star expressions. (From Python Cookbook recipe 1.2.) . records = [ (&#39;foo&#39;, 1, 2), (&#39;bar&#39;, &#39;hello&#39;) ] . for record in records: print(record) . (&#39;foo&#39;, 1, 2) (&#39;bar&#39;, &#39;hello&#39;) . for a, *b in records: print(a) print(b) . foo [1, 2] bar [&#39;hello&#39;] . def do_foo(x, y): print(f&#39;foo: args are {x} and {y}.&#39;) def do_bar(x): print(f&#39;bar: arg is {x}.&#39;) for tag, *args in records: if tag == &#39;foo&#39;: do_foo(*args) elif tag == &#39;bar&#39;: do_bar(*args) . foo: args are 1 and 2. bar: arg is hello. . Creating a callable_iterator . Roll a die until a 6 is rolled . import random def roll(): return random.randint(1, 6) roll_iter = iter(roll, 6) roll_iter . &lt;callable_iterator at 0x112c75850&gt; . for r in roll_iter: print(r) . 1 2 4 2 . list(iter(roll, 4)) . [3, 1, 2, 5, 3, 5, 3] . To read file until an empty line: . with open(&#39;filepath&#39;) as f: for line in iter(f.readline, &#39;&#39;): process_line(line) . itertools.starmap() . Using starmap() to calculate a running average: . numbers = range(1, 11, 4) list(itertools.starmap(lambda a, b: b / a, enumerate(itertools.accumulate(numbers), 1))) . [1.0, 3.0, 5.0] . list(itertools.accumulate(numbers)) . [1, 6, 15] . list(enumerate(itertools.accumulate(numbers), 1)) . [(1, 1), (2, 6), (3, 15)] . Multiplying characters . name = &#39;Emily&#39; list(itertools.starmap(operator.mul, enumerate(name, 1))) . [&#39;E&#39;, &#39;mm&#39;, &#39;iii&#39;, &#39;llll&#39;, &#39;yyyyy&#39;] . functools.reduce() . import pandas as pd df = pd.DataFrame({&#39;AAA&#39;: [4, 5, 6, 7], &#39;BBB&#39;: [10, 20, 30, 40], &#39;CCC&#39;: [100, 50, -30, -50]}) . What I usually do . crit1 = df.AAA &gt; 5 crit2 = df.BBB &gt; 30 crits = crit1 &amp; crit2 df[crits] . AAA BBB CCC . 3 7 | 40 | -50 | . Alternative using functools.reduce() . import functools crit1 = df.AAA &gt; 5 crit2 = df.BBB &gt; 30 crits = [crit1, crit2] mask = functools.reduce(lambda x, y: x &amp; y, crits) df[mask] . AAA BBB CCC . 3 7 | 40 | -50 | . Understanding dropwhile() . From docs . def dropwhile(predicate, iterable): iterable = iter(iterable) for x in iterable: if not predicate(x): yield x break for x in iterable: yield x predicate = lambda x: x &lt; 5 iterable = [1, 2, 3, 6, 7, 3] list(dropwhile(predicate, iterable)) . [6, 7, 3] . What happens here? . iter() is used so that the iterable becomes an iterator (which gets emptied as it&#39;s being iterated over). . | The first for loop moves until the first element fails the condition in predicate, at which point that element is yielded and the program breakes out of that for loop, advancing to the next. . | Because of step 1, iterable now only contains all elements after the element that caused the previous for loop to break, and all of these are yielded. . | def sensemaker(predicate, iterable): iterable = iter(iterable) for x in iterable: if not predicate(x): print(&#39;First loop&#39;) print(x) break print(&#39;Second loop&#39;) for x in iterable: print(x) sensemaker(predicate, iterable) . First loop 6 Second loop 7 3 . def sensemaker(predicate, iterable): # iterable = iter(iterable) for x in iterable: if not predicate(x): print(&#39;First loop&#39;) print(x) break print(&#39;Second loop&#39;) for x in iterable: print(x) sensemaker(predicate, iterable) . First loop 6 Second loop 1 2 3 6 7 3 . If we don&#39;t turn the iterable into an iterator, it doesn&#39;t get exhausted and the second loop simply loops over all its objects. . From more itertools . import more_itertools more_itertools.take(4, more_itertools.pairwise(itertools.count())) . [(0, 1), (1, 2), (2, 3), (3, 4)] . Sources . Fluent Python | Pandas cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/10/18/builtin-heroes.html",
            "relUrl": "/python/pandas/2020/10/18/builtin-heroes.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post65": {
            "title": "Pythonic objects",
            "content": "Practicing the use of special methods. . An artificially powerful French card deck . Based on part 4 in Fluent Python. . import collections import functools import numbers import operator Card = collections.namedtuple(&#39;Card&#39;, [&#39;rank&#39;, &#39;suit&#39;]) class FrenchDeck: ranks = [str(n) for n in range(2, 11)] + list(&#39;JQKA&#39;) suits = &#39;spades diamonds clubs hearts&#39;.split() def __init__(self, cards=None): if cards is None: self._cards = [Card(rank, suit) for suit in self.suits for rank in self.ranks] else: self._cards = cards def __len__(self): return len(self._cards) # basic implementation returns a list def __getitem__(self, position): return self._cards[position] # make slicing return a FrenchDeck rather than a list # this is what requires the conditional init statement def __getitem__(self, index): cls = type(self) if isinstance(index, slice): return cls(self._cards[index]) if isinstance(index, numbers.Integral): return self._cards[index] else: msg = &#39;{cls.__name__} indices must be integers&#39; raise TypeError(msg.format(cls=cls)) def __str__(self): cards = [(card.suit, card.rank) for card in self._cards] return str(tuple(cards)) def __repr__(self): cards = [(card.suit, card.rank) for card in self._cards] return &#39;FrenchDeck({})&#39;.format(cards) shortcut_names = &#39;abc&#39; # access first three elements with abc shortcut def __getattr__(self, name): cls = type(self) if len(name) == 1: pos = self.shortcut_names.find(name) if 0 &lt;= pos &lt; len(self._cards): return self._cards[pos] msg = &#39;{.__name__!r} object has no attribute {!r}&#39; raise AttributeError(msg.format(cls, name)) # avoid attribute setting to avoid confusion with abc shortcuts def __setattr__(self, name, value): cls = type(self) if len(name) == 1: if name in self.shortcut_names: error = &#39;readonly attribute {attr_name!r}&#39; elif name.islower(): error = &quot;can&#39;t set attributes &#39;a&#39; to &#39;z&#39; in {cls_name!r}&quot; else: error = &#39;&#39; if error: msg = error.format(attr_name=name, cls_name=cls.__name__) raise AttributeError(msg) super().__setattr__(name, value) # basic comparison def __eq__(self, other): return tuple(self) == tuple(other) # more efficient comparison for large deck def __eq__(self, other): return (len(self) == len(other) and all(a == b for a, b in zip(self, other))) def __hash__(self): hashes = map(hash, self._cards) return functools.reduce(operator.xor, hashes) # totally artificial format attribute def __format__(self, fmt): cards = ((format(card.rank, fmt), card.suit) for card in deck._cards[:3]) return &#39;{}, {}, {}&#39;.format(*cards) deck = FrenchDeck() # print(deck[3:5]) # print(deck.a) # deck[2:3] ddeck = FrenchDeck() format(deck, &#39;-^5&#39;) . &#34;(&#39;--2--&#39;, &#39;spades&#39;), (&#39;--3--&#39;, &#39;spades&#39;), (&#39;--4--&#39;, &#39;spades&#39;)&#34; . FrenchDeck as a subclass of an existing ABC, collections.MutableSequence. . import collections Card = collections.namedtuple(&#39;Card&#39;, [&#39;card&#39;, &#39;suit&#39;]) class FrenchDeck2(collections.MutableSequence): cards = [str(n) for n in range(2, 11)] + list(&#39;JQKA&#39;) suits = &#39;spandes clubs diamonds hearts&#39;.split() def __init__(self): self._cards = [Card(card, suit) for suit in self.suits for card in self.cards] def __len__(self): return len(self._cards) def __getitem__(self, position): return self._cards[position] def __iter__(self): return (c for c in self._cards) def __delitem__(self, position): del self._cards[position] def __setitem__(self, key, value): self._cards[key] = value def insert(self, position, value): self._cards.insert(position, value) a = FrenchDeck2() a.insert(1, &#39;hello&#39;) len(a) a[1] = &#39;ha&#39; len(a) a[:3] list(reversed(a[:5])) . [Card(card=&#39;5&#39;, suit=&#39;spandes&#39;), Card(card=&#39;4&#39;, suit=&#39;spandes&#39;), Card(card=&#39;3&#39;, suit=&#39;spandes&#39;), &#39;ha&#39;, Card(card=&#39;2&#39;, suit=&#39;spandes&#39;)] . A helper class to get data out of a closure . From Effective Python Item 15. We want to order items in a list such that priority items are listed first, and we want a flag to indicate whether the list contained any priority items. . Using a function and nonlocal: . numbers = [4, 6, 5, 2, 7, 8, 9, 0, 1, 3] group = {3, 4, 5} def sort_priority(numbers, group): found = False def helper(x): nonlocal found if x in group: found = True return (0, x) return (1, x) numbers.sort(key=helper) return found sort_priority(numbers, group) . True . numbers . [3, 4, 5, 0, 1, 2, 6, 7, 8, 9] . This works. But Slatkin is not too keen on the nonlocal keyword except for use in very simple functions (the above would most definitely qualify), and defines a helper class to achieve the same result. . numbers = [4, 6, 5, 2, 7, 8, 9, 0, 1, 3] group = {3, 4, 5} class Sorter(object): def __init__(self, group): self.group = group self.found = False def __call__(self, x): if x in self.group: self.found = True return (0, x) return (1, x) sorter = Sorter(group) numbers.sort(key=sorter) assert sorter.found is True numbers . [3, 4, 5, 0, 1, 2, 6, 7, 8, 9] . sorter(5), sorter(7) . ((0, 5), (1, 7)) . Sources . Effective Python | Fluent Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/10/14/pythonic-objects.html",
            "relUrl": "/python/2020/10/14/pythonic-objects.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post66": {
            "title": "Mutable function parameters",
            "content": "Notes based on chapter 8 in Fluent Python. . Functions that take mutable objects as arguments require caution, because function arguments are aliases for the passed arguments. This can cause unintended behaviour in two types of situations: . When setting a mutable object as default | When aliasing a mutable object passed to the constructor | . Setting a mutable object as default . class HauntedBus(): def __init__(self, passengers=[]): self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) bus1 = HauntedBus([&#39;pete&#39;, &#39;lara&#39;, &#39;nick&#39;]) bus1.drop(&#39;nick&#39;) print(bus1.passengers) bus2 = HauntedBus() bus2.pick(&#39;heather&#39;) print(bus2.passengers) bus3 = HauntedBus() bus3.pick(&#39;katy&#39;) print(bus3.passengers) . [&#39;pete&#39;, &#39;lara&#39;] [&#39;heather&#39;] [&#39;heather&#39;, &#39;katy&#39;] . Between bus 1 and 2, all works as intended, since we passed our own list when creating bus 1. Then things get a bit weird, though: how did Heather get into bus 3? When we define the HauntedBus class, we create a single empty list that is kept in the background and will be used whenever we instantiate a new bus without a custom passenger list. Importantly, all such buses will operate on the same list. We can see this by checking the object ids of the three buses&#39; passenger lists: . assert bus1.passengers is not bus2.passengers assert bus2.passengers is bus3.passengers . This shows that while the passenger list of bus 1 and 2 are not the same object, the lists of bus 2 and 3 are. Once we know that, the above behaviour makes sense: all passenger list operations on buses without a custom list operate on the same list. Anohter way of seeing this by inspecting the default dict of HauntedBus after our operations abve. . HauntedBus.__init__.__defaults__ . ([&#39;heather&#39;, &#39;katy&#39;],) . The above shows that after the bus3.pick(&#39;katy&#39;) call above, the default list is now changed, and will be inherited by future instances of HauntedBus. . bus4 = HauntedBus() bus4.passengers . [&#39;heather&#39;, &#39;katy&#39;] . This behaviour is an example of why it matters whether we think of variables as boxes or labels. If we think that variables are boxes, then the above bevaviour doesn&#39;t make sense, since each passenger list would be its own box with its own content. But when we think of variables as labels -- the correct way to think about them in Python -- then the behaviour makes complete sense: each time we instantiate a bus without a custom passenger list, we create a new label -- of the form name-of-bus.passengers -- for the empty list we created when we loaded or created HauntedBus. . What to do to avoid the unwanted behaviour? The solution is to create a new empty list each time no list is provided. . class Bus(): def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = list(passengers) def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) bus1 = Bus() bus1.pick(&#39;tim&#39;) bus2 = Bus() bus2.passengers . [] . Aliasing a mutable object argument inside the function . The init method of the above class copies the passed passenger list by calling list(passengers). This is critical. If, instead of copying we alias the passed list, we change lists defined outside the function that are passed as arguments, which is probably not what we want. . class Bus(): def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) team = [&#39;hugh&#39;, &#39;lisa&#39;, &#39;gerd&#39;, &#39;adam&#39;, &#39;emily&#39;] bus = Bus(team) bus.drop(&#39;hugh&#39;) team . [&#39;lisa&#39;, &#39;gerd&#39;, &#39;adam&#39;, &#39;emily&#39;] . Again, the reason for this is that self.passengers is an alias for passengers, which is itself an alias for team, so that all operations we perfom on self.passengers are actually performed on team. The identity check below shows what the passengers attribute of bus is indeed the same object as the team list. . bus.passengers is team . True . To summarise: unless there is a good reason for an exception, for functions that take mutable objects as arguments do the following: . Create a new object each time a class is instantiated by using None as the default parameter, rather than creating the object at the time of the function definition. . | Make a copy of the mutable object for processing inside the function to leave the original object unchanged. . | Main sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/10/05/mutable-function-parameters.html",
            "relUrl": "/python/2020/10/05/mutable-function-parameters.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post67": {
            "title": "Documenting sample selection",
            "content": "import numpy as np import pandas as pd . Problem . I have a dataframe on which I perform a series of data selection steps. What I want is to automatically build a table for the appendix of my paper that tells me the number of users left in the data after each selection step. . Here&#39;s a mock dataset: . df = (pd.DataFrame({&#39;user_id&#39;: [1, 2, 3, 4] * 2, &#39;data&#39;: np.random.rand(8)}) .sort_values(&#39;user_id&#39;)) df . user_id data . 0 1 | 0.382558 | . 4 1 | 0.923332 | . 1 2 | 0.606272 | . 5 2 | 0.760290 | . 2 3 | 0.103158 | . 6 3 | 0.044038 | . 3 4 | 0.708062 | . 7 4 | 0.955849 | . here some selection functions: . def first_five(df): return df[:5] def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def select_sample(df): return ( df .pipe(first_five) .pipe(n_largest) ) select_sample(df) . user_id data . 4 1 | 0.923332 | . 5 2 | 0.760290 | . 1 2 | 0.606272 | . Solution . If we have a single dataframe on which to perform selection, as in the setting above, we can use a decorator and a dictionary. . As a first step, let&#39;s build a decorator that prints out the number of users after applying each function: . from functools import wraps def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() print(f&#39;{func.__name__}: {num_users}&#39;) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def select_sample(df): return ( df .pipe(first_five) .pipe(n_largest) ) select_sample(df) . first_five: 3 n_largest: 2 . user_id data . 4 1 | 0.923332 | . 5 2 | 0.760290 | . 1 2 | 0.606272 | . That&#39;s already nice. But I need those counts for the data appendix of my paper, so what I really want is to store the counts in a container that I can turn into a table. To do this, we can store the counts in a dictionary instead of printing them. . counts = dict() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def select_sample(df): return ( df .pipe(first_five) .pipe(n_largest) ) display(select_sample(df)) counts . user_id data . 7 7 | 0.961156 | . 2 2 | 0.934121 | . 0 0 | 0.771461 | . {&#39;first_five&#39;: 5, &#39;n_largest&#39;: 3} . Next, I want to add the number of users at the beginning and the end of the process (the count at the end is identical with the final step, but I think it&#39;s worth adding so readers can easily spot the final numbers). . counts = dict() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def add_user_count(df, step): num_users = df.user_id.nunique() counts.update({step: num_users}) return df def select_sample(df): return ( df .pipe(add_user_count, &#39;start&#39;) .pipe(first_five) .pipe(n_largest) .pipe(add_user_count, &#39;end&#39;) ) display(select_sample(df)) counts . user_id data . 7 7 | 0.961156 | . 2 2 | 0.934121 | . 0 0 | 0.771461 | . {&#39;start&#39;: 6, &#39;first_five&#39;: 5, &#39;n_largest&#39;: 3, &#39;end&#39;: 3} . We&#39;re nearly there. Let&#39;s turn this into a table that we can store to disk (as a Latex table, say) and automatically import in our paper. . table = pd.DataFrame(counts.items(), columns=[&#39;Processing step&#39;, &#39;Number of unique users&#39;]) table . Processing step Number of unique users . 0 start | 4 | . 1 first_five | 4 | . 2 n_largest | 3 | . 3 end | 3 | . Finally, let&#39;s make sure readers of our paper (and we ourselves a few weeks from now) actually understand what&#39;s going on at each step. . description = { &#39;start&#39;: &#39;Raw dataset&#39;, &#39;first_five&#39;: &#39;Keep first five observations&#39;, &#39;n_largest&#39;: &#39;Keep three largest datapoints&#39;, &#39;end&#39;: &#39;Final dataset&#39; } table[&#39;Processing step&#39;] = table[&#39;Processing step&#39;].map(description) table . Processing step Number of unique users . 0 Raw dataset | 4 | . 1 Keep first five observations | 4 | . 2 Keep three largest datapoints | 3 | . 3 Final dataset | 3 | . That&#39;s it. We can can now export this as a Latex table (or some other format) and automatically load it in our paper. . Multiple datasets . Instead of having a single dataframe on which to perform selection, I actually have multiple pieces of a large dataframe (because the full dataframe doesn&#39;t fit into memory). What I want is to perform the data selection on each chunk separately but have the values in the counter object add up so that -- at the end -- the counts represent the counts for the full dataset. The solution here is to use collection.Counter() instead of a dictionary. . So, my setup is akin to the following: . large_df = pd.DataFrame({&#39;user_id&#39;: list(range(12)), &#39;data&#39;: np.random.rand(12)}) large_df . user_id data . 0 0 | 0.771461 | . 1 1 | 0.091924 | . 2 2 | 0.934121 | . 3 3 | 0.170945 | . 4 4 | 0.132203 | . 5 5 | 0.854464 | . 6 6 | 0.469576 | . 7 7 | 0.961156 | . 8 8 | 0.363705 | . 9 9 | 0.560698 | . 10 10 | 0.769054 | . 11 11 | 0.019288 | . buckets = pd.cut(large_df.user_id, bins=2) raw_pieces = [data for key, data in large_df.groupby(buckets)] for piece in raw_pieces: display(piece) . user_id data . 0 0 | 0.771461 | . 1 1 | 0.091924 | . 2 2 | 0.934121 | . 3 3 | 0.170945 | . 4 4 | 0.132203 | . 5 5 | 0.854464 | . user_id data . 6 6 | 0.469576 | . 7 7 | 0.961156 | . 8 8 | 0.363705 | . 9 9 | 0.560698 | . 10 10 | 0.769054 | . 11 11 | 0.019288 | . What happens if we use a dict() as our counts object as we did above. . counts = dict() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def add_user_count(df, step=&#39;start&#39;): num_users = df.user_id.nunique() counts.update({step: num_users}) return df def select_sample(df): return ( df .pipe(add_user_count) .pipe(first_five) .pipe(n_largest) .pipe(add_user_count, &#39;end&#39;) ) selected_pieces = [] for piece in raw_pieces: selected_pieces.append(select_sample(piece)) print(counts) df = pd.concat(selected_pieces) . {&#39;start&#39;: 6, &#39;first_five&#39;: 5, &#39;n_largest&#39;: 3, &#39;end&#39;: 3} {&#39;start&#39;: 6, &#39;first_five&#39;: 5, &#39;n_largest&#39;: 3, &#39;end&#39;: 3} . The counts are replaced rather than added up, which is how updating works for a dictionary: . m = dict(a=1, b=2) n = dict(b=3, c=4) m.update(n) m . {&#39;a&#39;: 1, &#39;b&#39;: 3, &#39;c&#39;: 4} . collections.Counter() (docs) solve this problem. . import collections counts = collections.Counter() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def add_user_count(df, step=&#39;start&#39;): num_users = df.user_id.nunique() counts.update({step: num_users}) return df def select_sample(df): return ( df .pipe(add_user_count) .pipe(first_five) .pipe(n_largest) .pipe(add_user_count, &#39;end&#39;) ) selected_pieces = [] for piece in raw_pieces: selected_pieces.append(select_sample(piece)) print(counts) df = pd.concat(selected_pieces) . Counter({&#39;start&#39;: 6, &#39;first_five&#39;: 5, &#39;n_largest&#39;: 3, &#39;end&#39;: 3}) Counter({&#39;start&#39;: 12, &#39;first_five&#39;: 10, &#39;n_largest&#39;: 6, &#39;end&#39;: 6}) . Now, updating adds up the values for each key, just as we want. We can add the same formatting as we did above and are done with our table. . Background . Other cool stuff Counter() can do . o = Counter(a=1, b=2) p = Counter(b=3, c=-4) o.update(p) o . Counter({&#39;a&#39;: 1, &#39;b&#39;: 5, &#39;c&#39;: -4}) . Counters can also do cool things like this: . list(o.elements()) . [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;] . o.most_common(2) . [(&#39;b&#39;, 5), (&#39;a&#39;, 1)] . o - p . Counter({&#39;a&#39;: 1, &#39;b&#39;: 2}) . Why is counts a global variable? . Because I want all decorated functions to write to the same counter object. . Often, decorators make use of closures instead, which have access to a nonlocal variable defined inside the outermost function. Let&#39;s look at what happens if we do this for our user counter. . def user_counter(func): counts = Counter() @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) print(counts) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def largest(df): return df.loc[df.data.nlargest(3).index] def select(df): return ( df .pipe(first_five) .pipe(largest) ) result = select(df) . Counter({&#39;first_five&#39;: 4}) Counter({&#39;largest&#39;: 3}) . Now, each decorated function gets its own counter object, which is not what we want here. For more on decorator state retention options, see chapter 39 in Learning Python. . What are closures and nonlocal variables? . (Disclaimer: Just about all of the text and code on closures is taken -- sometimes verbatim -- from chapter 7 in Fluent Python. So the point here is not to produce new insight, but to absorb the material and write an easily accessible note to my future self.) . Closures are functions that have access to nonlocal arguments -- variabls that are neither local nor global, but are defined inside an outer function within which the closure was defined, and to which the closure has access. . Let&#39;s look at an example. A simple function that takes one number as an argument and returns the average of all numbers passed to it since it&#39;s definition. For this, we need a way to store all previously passed values. One way to do this is to define a class with a call method. . class Averager(): def __init__(self): self.series = [] def __call__(self, new_value): self.series.append(new_value) total = sum(self.series) return total / len(self.series) avg = Averager() avg(10), avg(20), avg(30) . (10.0, 15.0, 20.0) . Another way is to use a closure function and store the series of previously passed numbers as a free variable. . def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total / len(series) return averager avg = make_averager() avg(10), avg(20), avg(30) . (10.0, 15.0, 20.0) . This gives the same result, but is arguably simpler than defining a class. . We can improve the above function by storing previous results so that we don&#39;t have to calculate the new average from scratch at every function call. . def make_fast_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averager avg = make_fast_averager() avg(10), avg(11), avg(12) . (10.0, 10.5, 11.0) . %%timeit avg = make_averager() [avg(n) for n in range(10_000)] . 210 ms ± 2.72 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . %%timeit avg = make_fast_averager() [avg(n) for n in range(10_000)] . 1.57 ms ± 23.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . This simple change gives us a massive speedup. . Notice the nonlocal statement inside the averager function. Why do we need this? Let&#39;s see what happens if we don&#39;t specify it: . def make_fast_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total / count return averager avg = make_fast_averager() avg(10), avg(11), avg(12) . UnboundLocalError Traceback (most recent call last) &lt;ipython-input-27-9b1a9d59caaa&gt; in &lt;module&gt; 10 avg = make_fast_averager() 11 &gt; 12 avg(10), avg(11), avg(12) &lt;ipython-input-27-9b1a9d59caaa&gt; in averager(new_value) 3 total = 0 4 def averager(new_value): -&gt; 5 count += 1 6 total += new_value 7 return total / count UnboundLocalError: local variable &#39;count&#39; referenced before assignment . How come our fast averager can&#39;t find count and total even though our slow averager could find series just fine? . The answer lies in Python&#39;s variable scope rules and the difference between assigning to unmutable objects and updating mutable ones. . Whenever we assign to a variable inside a function, it is treated as a local variable. . | count += 1 is the same as count = count + 1, so we are assigning to count, which makes it a local variable (the same goes for total). We are assigning new values to count rather than updaing it because integers are immutable, so we can&#39;t update it. . | Lists are mutable, so series.append() doesn&#39;t create a new list, but merely appends to it, which doesn&#39;t count as an assignment, so that series is not treated as a local variable. . | Hence, we need to explicitly tell Python that count and total are nonlocal variables. . Main sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/30/documenting-sample-selection.html",
            "relUrl": "/python/2020/09/30/documenting-sample-selection.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post68": {
            "title": "Decorators",
            "content": "Decorators with arguments . I have a simple logger decorator. . def logger(func): calls = 0 def wrapper(*args, **kwargs): nonlocal calls calls +=1 print(f&#39;Call #{calls} of {func.__name__}&#39;) return func(*args, **kwargs) return wrapper @logger def greeter(): print(&#39;Hello&#39;) @logger def singer(): print(&#39;lalala&#39;) @logger def congratulator(): print(&#39;Congratulations!&#39;) greeter() greeter() singer() congratulator() . Call #1 of greeter Hello Call #2 of greeter Hello Call #1 of singer lalala Call #1 of congratulator Congratulations! . Now I want the ability to deactivate the logger for certain functions. So I wrap the decorator in a decorator factory, like so: . def param_logger(active=True): def decorator(func): calls = 0 def wrapper(*args, **kwargs): nonlocal calls if active: calls +=1 print(f&#39;Call #{calls} of {func.__name__}&#39;) return func(*args, **kwargs) return wrapper return decorator @param_logger() def greeter(): print(&#39;Hello&#39;) @param_logger(active=True) def singer(): print(&#39;lalala&#39;) @param_logger(active=False) def congratulator(): print(&#39;Congratulations!&#39;) greeter() greeter() singer() congratulator() . Call #1 of greeter Hello Call #2 of greeter Hello Call #1 of singer lalala Congratulations! . How does this work? I&#39;m not completely confident, actually, but this is how I explain it to myself. . In our initial logger function above, both the argument to the outer function (func) and the variable defined inside the outer function (calls) are free variables of the closure function wrapper, meaning that wrapper has access to them even though they are not bound inside wrapper. . If we remember that . @logger def singer(): print(&#39;lalala&#39;) . is equivalent to . singer = logger(singer) . then it&#39;s clear that we can get a view of the free variables of the decorated greeter variable like so: . logger(greeter).__code__.co_freevars . (&#39;calls&#39;, &#39;func&#39;) . Now, what are the free variables of param_logger? . param_logger().__code__.co_freevars . (&#39;active&#39;,) . This makes sense: active is the function argument and we do not define any additional variables inside the scope of param_logger, so given our result above, this is what we would expect. . But param_logger is a decorator factory and not a decorator, which means it produces a decorator at the time of decoration. So, what are the free variables of the decorator is produces? . Similar to above, remembering that . @param_logger() def singer(): print(&#39;lalala&#39;) . is equivalent to . singer = param_logger()(singer) . we can inspect the decorated singer function&#39;s free variables like so: . param_logger()(singer).__code__.co_freevars . (&#39;active&#39;, &#39;calls&#39;, &#39;func&#39;) . We can see that active is now an additional free variable that our wrapper function has access to, which provides us with the answer to our question: decorator factories work by producing decorators at decoration time and passing on the specified keyword to the decorated function. . A final point for those into aesthetics or coding consistency: we can tweak our decorator factory so that we can ommit the () if we pass no keyword arguments. . def logger(func=None, active=True): def decorator(func): calls = 0 def wrapper(*args, **kwargs): nonlocal calls if active: calls +=1 print(f&#39;Call #{calls} of {func.__name__}&#39;) return func(*args, **kwargs) return wrapper if func: return decorator(func) else: return decorator @logger def greeter(): print(&#39;Hello&#39;) @logger() def babler(): print(&#39;bablebalbe&#39;) @logger(active=True) def singer(): print(&#39;lalala&#39;) @logger(active=False) def congratulator(): print(&#39;Congratulations!&#39;) greeter() greeter() babler() singer() congratulator() . Call #1 of greeter Hello Call #2 of greeter Hello Call #1 of babler bablebalbe Call #1 of singer lalala Congratulations! . To understand what happens here, remember that decorating func with a decorator is equivalent to . func = decorator(func) . While decorating it with a decorator factory is equivalent to . func = decorator()(func) . The control flow inside the above decorator factory simply switches between these two cases: if logger gets a function argument, then that&#39;s akin to the first scenario, where the func argument is passed into decorator directly, and so the decorator factory returns decorator(func) to mimik this behaviour. If func is not passed, then we&#39;re in the standard decorator factory scenario above, and we simply return the decorator uncalled, just as any plain decorator factory would. . Recipe 9.6 in the Python Cookbook discusses a neat solution to the above for a registration decorator using functools.partial(), which I haven&#39;t managed to a scenario with a decorator factory. Might give it another go later. . Mistakes I often make . I often do the below: . from functools import wraps def decorator(func): @wraps def wrapper(*args, **kwargs): print(&#39;Func is called:&#39;, func.__name__) return func(*args, **kwargs) return wrapper @decorator def greeter(name): return f&#39;Hello {name}&#39; greeter(&#39;World&#39;) . AttributeError Traceback (most recent call last) &lt;ipython-input-8-d69c0930e7ac&gt; in &lt;module&gt; 12 return f&#39;Hello {name}&#39; 13 &gt; 14 greeter(&#39;World&#39;) ~/miniconda3/envs/habits/lib/python3.7/functools.py in update_wrapper(wrapper, wrapped, assigned, updated) 56 pass 57 else: &gt; 58 setattr(wrapper, attr, value) 59 for attr in updated: 60 getattr(wrapper, attr).update(getattr(wrapped, attr, {})) AttributeError: &#39;str&#39; object has no attribute &#39;__module__&#39; . What&#39;s wrong, there? @wraps should be @wraps(func). . from functools import wraps def decorator(func): @wraps(func) def wrapper(*args, **kwargs): print(&#39;Func is called:&#39;, func.__name__) return func(*args, **kwargs) return wrapper @decorator def greeter(name): return f&#39;Hello {name}&#39; greeter(&#39;World&#39;) . Func is called: greeter . &#39;Hello World&#39; . Main sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/30/decorators.html",
            "relUrl": "/python/2020/09/30/decorators.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post69": {
            "title": "String formatting",
            "content": "Basics . import string print(string.digits) print(string.ascii_lowercase) print(string.punctuation) . 0123456789 abcdefghijklmnopqrstuvwxyz !&#34;#$%&amp;&#39;()*+,-./:;&lt;=&gt;?@[ ]^_`{|}~ . Cool example from the docs: . for align, text in zip(&#39;&lt;^&gt;&#39;, [&#39;left&#39;, &#39;center&#39;, &#39;right&#39;]): print(&#39;{0:{filler}{align}30}&#39;.format(text, filler=align, align=align)) . left&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; ^^^^^^^^^^^^center^^^^^^^^^^^^ &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;right . Unpack and nicely format nested city information . Example from page 29 in Fluent Python. Code is available here. . metro_areas = [ (&#39;Tokyo&#39;, &#39;JP&#39;, 36.933, (35.689722, 139.691667)), (&#39;Delhi NCR&#39;, &#39;IN&#39;, 21.935, (28.613889, 77.208889)), (&#39;Mexico City&#39;, &#39;MX&#39;, 20.142, (19.433333, -99.133333)), (&#39;New York-Newark&#39;, &#39;US&#39;, 20.104, (40.808611, -74.020386)), (&#39;Sao Paulo&#39;, &#39;BR&#39;, 19.649, (-23.547778, -46.635833)), ] print(&#39;{:15} | {:^9} | {:^9}&#39;.format(&#39;&#39;, &#39;lat.&#39;, &#39;long.&#39;)) fmt = &#39;{:15} | {:&gt;9.4f} | {:&gt;9.4f}&#39; for name, cc, pop, (lat, long) in metro_areas: if long &lt;= 0: print(fmt.format(name, lat, long)) . | lat. | long. Mexico City | 19.4333 | -99.1333 New York-Newark | 40.8086 | -74.0204 Sao Paulo | -23.5478 | -46.6358 . Playing around with the format specification . print(&#39;=&#39;*39) print(&#39;{:15} | {:^9} | {:^9}&#39;.format(&#39; &#39;, &#39;lat.&#39;, &#39;long.&#39;)) print(&#39;-&#39;*39) fmt = &#39;{:15} | {:&gt;9.4f} | {:&gt;9.4f}&#39; for name, cc, pop, (lat, long) in metro_areas: if long &lt;= 0: print(fmt.format(name, lat, long)) print(&#39;=&#39;*39) . ======================================= | lat. | long. Mexico City | 19.4333 | -99.1333 New York-Newark | 40.8086 | -74.0204 Sao Paulo | -23.5478 | -46.6358 ======================================= . fmt = &#39;{:15} | {:0=+20.5f} | {:20.5e}&#39; for name, cc, pop, (lat, long) in metro_areas: if long &lt;= 0: print(fmt.format(name, lat, long)) . Mexico City | +0000000000019.43333 | -9.91333e+01 New York-Newark | +0000000000040.80861 | -7.40204e+01 Sao Paulo | -0000000000023.54778 | -4.66358e+01 . Main sources . String docs | Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/29/string-formatting.html",
            "relUrl": "/python/2020/09/29/string-formatting.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post70": {
            "title": "SQL",
            "content": "SQLite . Based on docs. . import os import sqlite3 PATH = &#39;/Users/fgu/tmp&#39; sqlite3.version . &#39;2.6.0&#39; . Establish a connection to database (or create and connect if it doesn&#39;t exist yet) . db_path = os.path.join(PATH, &#39;some.db&#39;) conn = sqlite3.connect(db_path) . To manupulate the database, create a cursor) object (not strictly needed for sqlite3), but I still use it for practice and to make db code portable to other databases. . Adding a table with a row of data . c = conn.cursor() # Create table c.execute(&quot;&quot;&quot;CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)&quot;&quot;&quot;) # Insert a row of data c.execute(&quot;INSERT INTO stocks VALUES (&#39;2006-01-05&#39;,&#39;BUY&#39;,&#39;RHAT&#39;,100,35.14)&quot;) # Save (commit) changes conn.commit() # Close connection conn.close() . To check that the database now contains our stocks table, list all its tables. . conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(&quot;select name from sqlite_master where type = &#39;table&#39;&quot;).fetchall() . [(&#39;stocks&#39;,)] . Retrieving data . conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(&quot;SELECT * FROM stocks&quot;).fetchall() . [(&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14)] . When adding Python variables to the query, never use string substitution directly like so: . symbol = &#39;RHAT&#39; c.execute(f&quot;select * from stocks where symbol = &#39;{symbol}&#39;&quot;).fetchall() . [(&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14)] . While this works, it&#39;s vulnerable to injection attacks. Use parameter substition instead. Either using question marks like so . symbol = (&#39;RHAT&#39;, ) c.execute(&quot;select * from stocks where symbol = ?&quot;, symbol).fetchall() . [(&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14)] . or using named placedholders like so . c.execute(&quot;select * from stocks where symbol = :symbol&quot;, {&#39;symbol&#39;: &#39;RHAT&#39;}).fetchall() . [(&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14)] . Why do I need fetchall() after cursor.execute()? . Because the curse.execute() returns an iterater object containing all query results. . Using namedtuples . EmployeeRecord = namedtuple(&#39;EmployeeRecord&#39;, &#39;name, age, title, department, paygrade&#39;) import csv for emp in map(EmployeeRecord._make, csv.reader(open(&quot;employees.csv&quot;, &quot;rb&quot;))): print emp.name, emp.title import sqlite3 conn = sqlite3.connect(&#39;/companydata&#39;) cursor = conn.cursor() cursor.execute(&#39;SELECT name, age, title, department, paygrade FROM employees&#39;) for emp in map(EmployeeRecord._make, cursor.fetchall()): print emp.name, emp.title . Using Pandas . Pandas is a very handy way to interact with databased in Python, as it makes dumping and retrieving dataframes very easy. . import pandas as pd pd.read_sql_query(&#39;SELECT * FROM stocks&#39;, conn) . date trans symbol qty price newcol . 0 2006-01-05 | BUY | RHAT | 100.0 | 35.14 | None | . SQLAlchemy . Summary of this video. .",
            "url": "https://fabiangunzinger.github.io/blog/python/sql/2020/09/23/sql.html",
            "relUrl": "/python/sql/2020/09/23/sql.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post71": {
            "title": "Exploring data",
            "content": "Based on excellent materials materials from Daniel Chen here and talk here . from imports import * %load_ext autoreload %autoreload 2 . Creating tidy data . Definition by Hadley Wickham here: . Each variable is a column | Each observation is a row | Each type of observational unit is a table. | Columns are values . pew = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/pew.csv&#39;) pew.head() . religion &lt;$10k $10-20k $20-30k $30-40k $40-50k $50-75k $75-100k $100-150k &gt;150k Don&#39;t know/refused . 0 Agnostic | 27 | 34 | 60 | 81 | 76 | 137 | 122 | 109 | 84 | 96 | . 1 Atheist | 12 | 27 | 37 | 52 | 35 | 70 | 73 | 59 | 74 | 76 | . 2 Buddhist | 27 | 21 | 30 | 34 | 33 | 58 | 62 | 39 | 53 | 54 | . 3 Catholic | 418 | 617 | 732 | 670 | 638 | 1116 | 949 | 792 | 633 | 1489 | . 4 Don’t know/refused | 15 | 14 | 15 | 11 | 10 | 35 | 21 | 17 | 18 | 116 | . pew.melt( id_vars=&#39;religion&#39;, var_name=&#39;income&#39;, value_name=&#39;count&#39; ).head() . religion income count . 0 Agnostic | &lt;$10k | 27 | . 1 Atheist | &lt;$10k | 12 | . 2 Buddhist | &lt;$10k | 27 | . 3 Catholic | &lt;$10k | 418 | . 4 Don’t know/refused | &lt;$10k | 15 | . billboard = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/billboard.csv&#39;) billboard.columns . Index([&#39;year&#39;, &#39;artist&#39;, &#39;track&#39;, &#39;time&#39;, &#39;date.entered&#39;, &#39;wk1&#39;, &#39;wk2&#39;, &#39;wk3&#39;, &#39;wk4&#39;, &#39;wk5&#39;, &#39;wk6&#39;, &#39;wk7&#39;, &#39;wk8&#39;, &#39;wk9&#39;, &#39;wk10&#39;, &#39;wk11&#39;, &#39;wk12&#39;, &#39;wk13&#39;, &#39;wk14&#39;, &#39;wk15&#39;, &#39;wk16&#39;, &#39;wk17&#39;, &#39;wk18&#39;, &#39;wk19&#39;, &#39;wk20&#39;, &#39;wk21&#39;, &#39;wk22&#39;, &#39;wk23&#39;, &#39;wk24&#39;, &#39;wk25&#39;, &#39;wk26&#39;, &#39;wk27&#39;, &#39;wk28&#39;, &#39;wk29&#39;, &#39;wk30&#39;, &#39;wk31&#39;, &#39;wk32&#39;, &#39;wk33&#39;, &#39;wk34&#39;, &#39;wk35&#39;, &#39;wk36&#39;, &#39;wk37&#39;, &#39;wk38&#39;, &#39;wk39&#39;, &#39;wk40&#39;, &#39;wk41&#39;, &#39;wk42&#39;, &#39;wk43&#39;, &#39;wk44&#39;, &#39;wk45&#39;, &#39;wk46&#39;, &#39;wk47&#39;, &#39;wk48&#39;, &#39;wk49&#39;, &#39;wk50&#39;, &#39;wk51&#39;, &#39;wk52&#39;, &#39;wk53&#39;, &#39;wk54&#39;, &#39;wk55&#39;, &#39;wk56&#39;, &#39;wk57&#39;, &#39;wk58&#39;, &#39;wk59&#39;, &#39;wk60&#39;, &#39;wk61&#39;, &#39;wk62&#39;, &#39;wk63&#39;, &#39;wk64&#39;, &#39;wk65&#39;, &#39;wk66&#39;, &#39;wk67&#39;, &#39;wk68&#39;, &#39;wk69&#39;, &#39;wk70&#39;, &#39;wk71&#39;, &#39;wk72&#39;, &#39;wk73&#39;, &#39;wk74&#39;, &#39;wk75&#39;, &#39;wk76&#39;], dtype=&#39;object&#39;) . idvars = billboard.columns[~bb.columns.str.startswith(&#39;wk&#39;)] tidy_billboard = billboard.melt( id_vars=idvars, var_name=&#39;week&#39;, value_name=&#39;rating&#39; ) tidy_billboard.head() . year artist track time date.entered week rating . 0 2000 | 2 Pac | Baby Don&#39;t Cry (Keep... | 4:22 | 2000-02-26 | wk1 | 87.0 | . 1 2000 | 2Ge+her | The Hardest Part Of ... | 3:15 | 2000-09-02 | wk1 | 91.0 | . 2 2000 | 3 Doors Down | Kryptonite | 3:53 | 2000-04-08 | wk1 | 81.0 | . 3 2000 | 3 Doors Down | Loser | 4:24 | 2000-10-21 | wk1 | 76.0 | . 4 2000 | 504 Boyz | Wobble Wobble | 3:35 | 2000-04-15 | wk1 | 57.0 | . Multiple variables stored in one column . ebola = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/country_timeseries.csv&#39;) ebola.head() . Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone Cases_Nigeria Cases_Senegal Cases_UnitedStates Cases_Spain Cases_Mali Deaths_Guinea Deaths_Liberia Deaths_SierraLeone Deaths_Nigeria Deaths_Senegal Deaths_UnitedStates Deaths_Spain Deaths_Mali . 0 1/5/2015 | 289 | 2776.0 | NaN | 10030.0 | NaN | NaN | NaN | NaN | NaN | 1786.0 | NaN | 2977.0 | NaN | NaN | NaN | NaN | NaN | . 1 1/4/2015 | 288 | 2775.0 | NaN | 9780.0 | NaN | NaN | NaN | NaN | NaN | 1781.0 | NaN | 2943.0 | NaN | NaN | NaN | NaN | NaN | . 2 1/3/2015 | 287 | 2769.0 | 8166.0 | 9722.0 | NaN | NaN | NaN | NaN | NaN | 1767.0 | 3496.0 | 2915.0 | NaN | NaN | NaN | NaN | NaN | . 3 1/2/2015 | 286 | NaN | 8157.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 3496.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 12/31/2014 | 284 | 2730.0 | 8115.0 | 9633.0 | NaN | NaN | NaN | NaN | NaN | 1739.0 | 3471.0 | 2827.0 | NaN | NaN | NaN | NaN | NaN | . tidy_ebola = ebola.melt(id_vars=[&#39;Date&#39;, &#39;Day&#39;], value_name=&#39;Cases&#39;) tidy_ebola[[&#39;Statistic&#39;, &#39;Country&#39;]] = (tidy_ebola.variable .str.split(&#39;_&#39;, expand=True)) tidy_ebola.drop(&#39;variable&#39;, axis=1, inplace=True) tidy_ebola.head() . Date Day Cases Statistic Country . 0 1/5/2015 | 289 | 2776.0 | Cases | Guinea | . 1 1/4/2015 | 288 | 2775.0 | Cases | Guinea | . 2 1/3/2015 | 287 | 2769.0 | Cases | Guinea | . 3 1/2/2015 | 286 | NaN | Cases | Guinea | . 4 12/31/2014 | 284 | 2730.0 | Cases | Guinea | . (ebola .melt(id_vars=[&#39;Date&#39;, &#39;Day&#39;], value_name=&#39;Cases&#39;) .assign(Statistic = lambda df: df.variable.str.split(&#39;_&#39;, expand=True)[0]) .assign(Country = lambda df: df.variable.str.split(&#39;_&#39;, expand=True)[1]) .drop(&#39;variable&#39;, axis=1) ).head() . Date Day Cases Statistic Country . 0 1/5/2015 | 289 | 2776.0 | Cases | Guinea | . 1 1/4/2015 | 288 | 2775.0 | Cases | Guinea | . 2 1/3/2015 | 287 | 2769.0 | Cases | Guinea | . 3 1/2/2015 | 286 | NaN | Cases | Guinea | . 4 12/31/2014 | 284 | 2730.0 | Cases | Guinea | . Variables are stored in both rows and columns . weather = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/weather.csv&#39;) weather.head() . id year month element d1 d2 d3 d4 d5 d6 d7 d8 d9 d10 d11 d12 d13 d14 d15 d16 d17 d18 d19 d20 d21 d22 d23 d24 d25 d26 d27 d28 d29 d30 d31 . 0 MX17004 | 2010 | 1 | tmax | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 27.8 | NaN | . 1 MX17004 | 2010 | 1 | tmin | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 14.5 | NaN | . 2 MX17004 | 2010 | 2 | tmax | NaN | 27.3 | 24.1 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 29.7 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 29.9 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 MX17004 | 2010 | 2 | tmin | NaN | 14.4 | 14.4 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 13.4 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 10.7 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 MX17004 | 2010 | 3 | tmax | NaN | NaN | NaN | NaN | 32.1 | NaN | NaN | NaN | NaN | 34.5 | NaN | NaN | NaN | NaN | NaN | 31.1 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . (weather .melt( id_vars=[&#39;id&#39;, &#39;year&#39;, &#39;month&#39;, &#39;element&#39;], var_name=&#39;day&#39;, value_name=&#39;temp&#39; ) .pivot_table( index=[&#39;id&#39;, &#39;year&#39;, &#39;month&#39;, &#39;day&#39;], columns=&#39;element&#39;, values=&#39;temp&#39; ) .reset_index() .assign(day = lambda df: df.day.str.extract(&#39;( d+)&#39;)) ).head() . element id year month day tmax tmin . 0 MX17004 | 2010 | 1 | 30 | 27.8 | 14.5 | . 1 MX17004 | 2010 | 2 | 11 | 29.7 | 13.4 | . 2 MX17004 | 2010 | 2 | 2 | 27.3 | 14.4 | . 3 MX17004 | 2010 | 2 | 23 | 29.9 | 10.7 | . 4 MX17004 | 2010 | 2 | 3 | 24.1 | 14.4 | . Multiple types of observational units are stored in a single table . tidy_billboard.head() tidy_bb = tidy_billboard tidy_bb.head() . year artist track time date.entered week rating . 0 2000 | 2 Pac | Baby Don&#39;t Cry (Keep... | 4:22 | 2000-02-26 | wk1 | 87.0 | . 1 2000 | 2Ge+her | The Hardest Part Of ... | 3:15 | 2000-09-02 | wk1 | 91.0 | . 2 2000 | 3 Doors Down | Kryptonite | 3:53 | 2000-04-08 | wk1 | 81.0 | . 3 2000 | 3 Doors Down | Loser | 4:24 | 2000-10-21 | wk1 | 76.0 | . 4 2000 | 504 Boyz | Wobble Wobble | 3:35 | 2000-04-15 | wk1 | 57.0 | . bb_songs = ( tidy_bb[[&#39;year&#39;, &#39;artist&#39;, &#39;track&#39;, &#39;time&#39;, &#39;date.entered&#39;]] .drop_duplicates() .assign(id = lambda df: range(len(df))) ) bb_songs.head() . year artist track time date.entered id . 0 2000 | 2 Pac | Baby Don&#39;t Cry (Keep... | 4:22 | 2000-02-26 | 0 | . 1 2000 | 2Ge+her | The Hardest Part Of ... | 3:15 | 2000-09-02 | 1 | . 2 2000 | 3 Doors Down | Kryptonite | 3:53 | 2000-04-08 | 2 | . 3 2000 | 3 Doors Down | Loser | 4:24 | 2000-10-21 | 3 | . 4 2000 | 504 Boyz | Wobble Wobble | 3:35 | 2000-04-15 | 4 | . bb_ratings = ( tidy_bb .merge(bb_songs) .loc[:, [&#39;week&#39;, &#39;rating&#39;, &#39;id&#39;]] ) bb_ratings.head() . week rating id . 0 wk1 | 87.0 | 0 | . 1 wk2 | 82.0 | 0 | . 2 wk3 | 72.0 | 0 | . 3 wk4 | 77.0 | 0 | . 4 wk5 | 87.0 | 0 | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/22/tidy-data.html",
            "relUrl": "/python/pandas/2020/09/22/tidy-data.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post72": {
            "title": "Looping like a native",
            "content": "Based on this talk. . my_list = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] for i in range(len(my_list)): v = my_list[i] print(v) . a b c . This works, but we unnecessarily use integers. Just directly print what you want to print instead. . for v in my_list: print(v) . a b c .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/22/looping.html",
            "relUrl": "/python/2020/09/22/looping.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post73": {
            "title": "Data cleaning",
            "content": "Based on excellent materials materials from Daniel Chen here and talk here . from imports import * %load_ext autoreload %autoreload 2 . Creating tidy data . Definition by Wikham here: . Each variable is a column | Each observation is a row | Each type of observational unit is a table. | Columns are values . pew = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/pew.csv&#39;) pew.head() . religion &lt;$10k $10-20k $20-30k $30-40k $40-50k $50-75k $75-100k $100-150k &gt;150k Don&#39;t know/refused . 0 Agnostic | 27 | 34 | 60 | 81 | 76 | 137 | 122 | 109 | 84 | 96 | . 1 Atheist | 12 | 27 | 37 | 52 | 35 | 70 | 73 | 59 | 74 | 76 | . 2 Buddhist | 27 | 21 | 30 | 34 | 33 | 58 | 62 | 39 | 53 | 54 | . 3 Catholic | 418 | 617 | 732 | 670 | 638 | 1116 | 949 | 792 | 633 | 1489 | . 4 Don’t know/refused | 15 | 14 | 15 | 11 | 10 | 35 | 21 | 17 | 18 | 116 | . pew.melt( id_vars=&#39;religion&#39;, var_name=&#39;income&#39;, value_name=&#39;count&#39; ).head() . religion income count . 0 Agnostic | &lt;$10k | 27 | . 1 Atheist | &lt;$10k | 12 | . 2 Buddhist | &lt;$10k | 27 | . 3 Catholic | &lt;$10k | 418 | . 4 Don’t know/refused | &lt;$10k | 15 | . billboard = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/billboard.csv&#39;) billboard.columns . Index([&#39;year&#39;, &#39;artist&#39;, &#39;track&#39;, &#39;time&#39;, &#39;date.entered&#39;, &#39;wk1&#39;, &#39;wk2&#39;, &#39;wk3&#39;, &#39;wk4&#39;, &#39;wk5&#39;, &#39;wk6&#39;, &#39;wk7&#39;, &#39;wk8&#39;, &#39;wk9&#39;, &#39;wk10&#39;, &#39;wk11&#39;, &#39;wk12&#39;, &#39;wk13&#39;, &#39;wk14&#39;, &#39;wk15&#39;, &#39;wk16&#39;, &#39;wk17&#39;, &#39;wk18&#39;, &#39;wk19&#39;, &#39;wk20&#39;, &#39;wk21&#39;, &#39;wk22&#39;, &#39;wk23&#39;, &#39;wk24&#39;, &#39;wk25&#39;, &#39;wk26&#39;, &#39;wk27&#39;, &#39;wk28&#39;, &#39;wk29&#39;, &#39;wk30&#39;, &#39;wk31&#39;, &#39;wk32&#39;, &#39;wk33&#39;, &#39;wk34&#39;, &#39;wk35&#39;, &#39;wk36&#39;, &#39;wk37&#39;, &#39;wk38&#39;, &#39;wk39&#39;, &#39;wk40&#39;, &#39;wk41&#39;, &#39;wk42&#39;, &#39;wk43&#39;, &#39;wk44&#39;, &#39;wk45&#39;, &#39;wk46&#39;, &#39;wk47&#39;, &#39;wk48&#39;, &#39;wk49&#39;, &#39;wk50&#39;, &#39;wk51&#39;, &#39;wk52&#39;, &#39;wk53&#39;, &#39;wk54&#39;, &#39;wk55&#39;, &#39;wk56&#39;, &#39;wk57&#39;, &#39;wk58&#39;, &#39;wk59&#39;, &#39;wk60&#39;, &#39;wk61&#39;, &#39;wk62&#39;, &#39;wk63&#39;, &#39;wk64&#39;, &#39;wk65&#39;, &#39;wk66&#39;, &#39;wk67&#39;, &#39;wk68&#39;, &#39;wk69&#39;, &#39;wk70&#39;, &#39;wk71&#39;, &#39;wk72&#39;, &#39;wk73&#39;, &#39;wk74&#39;, &#39;wk75&#39;, &#39;wk76&#39;], dtype=&#39;object&#39;) . idvars = billboard.columns[~bb.columns.str.startswith(&#39;wk&#39;)] tidy_billboard = billboard.melt( id_vars=idvars, var_name=&#39;week&#39;, value_name=&#39;rating&#39; ) tidy_billboard.head() . year artist track time date.entered week rating . 0 2000 | 2 Pac | Baby Don&#39;t Cry (Keep... | 4:22 | 2000-02-26 | wk1 | 87.0 | . 1 2000 | 2Ge+her | The Hardest Part Of ... | 3:15 | 2000-09-02 | wk1 | 91.0 | . 2 2000 | 3 Doors Down | Kryptonite | 3:53 | 2000-04-08 | wk1 | 81.0 | . 3 2000 | 3 Doors Down | Loser | 4:24 | 2000-10-21 | wk1 | 76.0 | . 4 2000 | 504 Boyz | Wobble Wobble | 3:35 | 2000-04-15 | wk1 | 57.0 | . Multiple variables stored in one column . ebola = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/country_timeseries.csv&#39;) ebola.head() . Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone Cases_Nigeria Cases_Senegal Cases_UnitedStates Cases_Spain Cases_Mali Deaths_Guinea Deaths_Liberia Deaths_SierraLeone Deaths_Nigeria Deaths_Senegal Deaths_UnitedStates Deaths_Spain Deaths_Mali . 0 1/5/2015 | 289 | 2776.0 | NaN | 10030.0 | NaN | NaN | NaN | NaN | NaN | 1786.0 | NaN | 2977.0 | NaN | NaN | NaN | NaN | NaN | . 1 1/4/2015 | 288 | 2775.0 | NaN | 9780.0 | NaN | NaN | NaN | NaN | NaN | 1781.0 | NaN | 2943.0 | NaN | NaN | NaN | NaN | NaN | . 2 1/3/2015 | 287 | 2769.0 | 8166.0 | 9722.0 | NaN | NaN | NaN | NaN | NaN | 1767.0 | 3496.0 | 2915.0 | NaN | NaN | NaN | NaN | NaN | . 3 1/2/2015 | 286 | NaN | 8157.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 3496.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 12/31/2014 | 284 | 2730.0 | 8115.0 | 9633.0 | NaN | NaN | NaN | NaN | NaN | 1739.0 | 3471.0 | 2827.0 | NaN | NaN | NaN | NaN | NaN | . tidy_ebola = ebola.melt(id_vars=[&#39;Date&#39;, &#39;Day&#39;], value_name=&#39;Cases&#39;) tidy_ebola[[&#39;Statistic&#39;, &#39;Country&#39;]] = (tidy_ebola.variable .str.split(&#39;_&#39;, expand=True)) tidy_ebola.drop(&#39;variable&#39;, axis=1, inplace=True) tidy_ebola.head() . Date Day Cases Statistic Country . 0 1/5/2015 | 289 | 2776.0 | Cases | Guinea | . 1 1/4/2015 | 288 | 2775.0 | Cases | Guinea | . 2 1/3/2015 | 287 | 2769.0 | Cases | Guinea | . 3 1/2/2015 | 286 | NaN | Cases | Guinea | . 4 12/31/2014 | 284 | 2730.0 | Cases | Guinea | . (ebola .melt(id_vars=[&#39;Date&#39;, &#39;Day&#39;], value_name=&#39;Cases&#39;) .assign(Statistic = lambda df: df.variable.str.split(&#39;_&#39;, expand=True)[0]) .assign(Country = lambda df: df.variable.str.split(&#39;_&#39;, expand=True)[1]) .drop(&#39;variable&#39;, axis=1) ).head() . Date Day Cases Statistic Country . 0 1/5/2015 | 289 | 2776.0 | Cases | Guinea | . 1 1/4/2015 | 288 | 2775.0 | Cases | Guinea | . 2 1/3/2015 | 287 | 2769.0 | Cases | Guinea | . 3 1/2/2015 | 286 | NaN | Cases | Guinea | . 4 12/31/2014 | 284 | 2730.0 | Cases | Guinea | . Variables are stored in both rows and columns . weather = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/weather.csv&#39;) weather.head() . id year month element d1 d2 d3 d4 d5 d6 d7 d8 d9 d10 d11 d12 d13 d14 d15 d16 d17 d18 d19 d20 d21 d22 d23 d24 d25 d26 d27 d28 d29 d30 d31 . 0 MX17004 | 2010 | 1 | tmax | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 27.8 | NaN | . 1 MX17004 | 2010 | 1 | tmin | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 14.5 | NaN | . 2 MX17004 | 2010 | 2 | tmax | NaN | 27.3 | 24.1 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 29.7 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 29.9 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 MX17004 | 2010 | 2 | tmin | NaN | 14.4 | 14.4 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 13.4 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 10.7 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 MX17004 | 2010 | 3 | tmax | NaN | NaN | NaN | NaN | 32.1 | NaN | NaN | NaN | NaN | 34.5 | NaN | NaN | NaN | NaN | NaN | 31.1 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . (weather .melt( id_vars=[&#39;id&#39;, &#39;year&#39;, &#39;month&#39;, &#39;element&#39;], var_name=&#39;day&#39;, value_name=&#39;temp&#39; ) .pivot_table( index=[&#39;id&#39;, &#39;year&#39;, &#39;month&#39;, &#39;day&#39;], columns=&#39;element&#39;, values=&#39;temp&#39; ) .reset_index() .assign(day = lambda df: df.day.str.extract(&#39;( d+)&#39;)) ).head() . element id year month day tmax tmin . 0 MX17004 | 2010 | 1 | 30 | 27.8 | 14.5 | . 1 MX17004 | 2010 | 2 | 11 | 29.7 | 13.4 | . 2 MX17004 | 2010 | 2 | 2 | 27.3 | 14.4 | . 3 MX17004 | 2010 | 2 | 23 | 29.9 | 10.7 | . 4 MX17004 | 2010 | 2 | 3 | 24.1 | 14.4 | . Multiple types of observational units are stored in a single table . tidy_billboard.head() tidy_bb = tidy_billboard tidy_bb.head() . year artist track time date.entered week rating . 0 2000 | 2 Pac | Baby Don&#39;t Cry (Keep... | 4:22 | 2000-02-26 | wk1 | 87.0 | . 1 2000 | 2Ge+her | The Hardest Part Of ... | 3:15 | 2000-09-02 | wk1 | 91.0 | . 2 2000 | 3 Doors Down | Kryptonite | 3:53 | 2000-04-08 | wk1 | 81.0 | . 3 2000 | 3 Doors Down | Loser | 4:24 | 2000-10-21 | wk1 | 76.0 | . 4 2000 | 504 Boyz | Wobble Wobble | 3:35 | 2000-04-15 | wk1 | 57.0 | . bb_songs = ( tidy_bb[[&#39;year&#39;, &#39;artist&#39;, &#39;track&#39;, &#39;time&#39;, &#39;date.entered&#39;]] .drop_duplicates() .assign(id = lambda df: range(len(df))) ) bb_songs.head() . year artist track time date.entered id . 0 2000 | 2 Pac | Baby Don&#39;t Cry (Keep... | 4:22 | 2000-02-26 | 0 | . 1 2000 | 2Ge+her | The Hardest Part Of ... | 3:15 | 2000-09-02 | 1 | . 2 2000 | 3 Doors Down | Kryptonite | 3:53 | 2000-04-08 | 2 | . 3 2000 | 3 Doors Down | Loser | 4:24 | 2000-10-21 | 3 | . 4 2000 | 504 Boyz | Wobble Wobble | 3:35 | 2000-04-15 | 4 | . bb_ratings = ( tidy_bb .merge(bb_songs) .loc[:, [&#39;week&#39;, &#39;rating&#39;, &#39;id&#39;]] ) bb_ratings.head() . week rating id . 0 wk1 | 87.0 | 0 | . 1 wk2 | 82.0 | 0 | . 2 wk3 | 72.0 | 0 | . 3 wk4 | 77.0 | 0 | . 4 wk5 | 87.0 | 0 | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/22/data-cleaning.html",
            "relUrl": "/python/pandas/2020/09/22/data-cleaning.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post74": {
            "title": "os",
            "content": "Full documentation is here. . from imports import * import os %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . os.walk(os.) . TypeError Traceback (most recent call last) &lt;ipython-input-16-7f906606989b&gt; in &lt;module&gt; -&gt; 1 os.walk() TypeError: walk() missing 1 required positional argument: &#39;top&#39; . Listing files in directory . List filenames: . path = &#39;/Users/fgu/tmp/testdir&#39; os.listdir(path) . [&#39;a.py&#39;, &#39;b.py&#39;] . List filenames with full path: . There are many ways to do this. I find using scandir() most useful. . paths = [f.path for f in os.scandir(path)] paths . [&#39;/Users/fgu/tmp/testdir/a.py&#39;, &#39;/Users/fgu/tmp/testdir/b.py&#39;] . This only works if path is an absolute path, else scandir() will return relative paths just like listdir(). . The os.DirEntry object returned by scandir() is quite useful for other things, too: . for f in os.scandir(path): print(f) print(f.name) print(f.path) print(f.is_file()) print(f.is_dir()) print() . &lt;DirEntry &#39;a.py&#39;&gt; a.py /Users/fgu/tmp/testdir/a.py True False &lt;DirEntry &#39;b.py&#39;&gt; b.py /Users/fgu/tmp/testdir/b.py True False .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/20/os.html",
            "relUrl": "/python/2020/09/20/os.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post75": {
            "title": "Functools",
            "content": "partial . from operator import mul from functools import partial print(mul(2, 3)) tripple = partial(mul, 3) tripple(2) . 6 . 6 . Sources . Fluent Python | Python Cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/17/functools.html",
            "relUrl": "/python/2020/09/17/functools.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post76": {
            "title": "Time series in Pandas",
            "content": "import pandas as pd from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Number of periods observed . Say I want to know the number of months for which each user in my dataset is observed (allowing for possible gaps in their data). . df = pd.read_parquet(SAMPLEDATA, columns=[&#39;user_id&#39;, &#39;transaction_date&#39;]) print(df.shape) df.head() . (438853, 2) . user_id transaction_date . 0 60777 | 2014-11-27 | . 1 60777 | 2014-11-27 | . 2 60777 | 2014-11-28 | . 3 60777 | 2014-12-08 | . 4 60777 | 2014-12-12 | . def num_periods(g, freq=&#39;M&#39;): &quot;&quot;&quot;Return number of periods observed.&quot;&quot;&quot; return (g.transaction_date.max().to_period(freq) - g.transaction_date.min().to_period(freq)).n + 1 df.groupby(&#39;user_id&#39;).apply(num_periods, freq=&#39;M&#39;).head(3) . user_id 777 103 1777 28 7777 90 dtype: int64 . df.groupby(&#39;user_id&#39;).apply(num_periods, freq=&#39;W&#39;).head(3) . user_id 777 447 1777 117 7777 387 dtype: int64 . Groupby vs resample . df = pd.DataFrame({&#39;data&#39;: [1, 2, 3, 4, 5]}, pd.date_range(&#39;2020-01-01&#39;, &#39;2020-01-10&#39;, freq=&#39;2d&#39;)) df . data . 2020-01-01 1 | . 2020-01-03 2 | . 2020-01-05 3 | . 2020-01-07 4 | . 2020-01-09 5 | . df.resample(&#39;d&#39;).sum() . data . 2020-01-01 1 | . 2020-01-02 0 | . 2020-01-03 2 | . 2020-01-04 0 | . 2020-01-05 3 | . 2020-01-06 0 | . 2020-01-07 4 | . 2020-01-08 0 | . 2020-01-09 5 | . dd.groupby(level=0).sum() . data . 2020-01-01 1 | . 2020-01-03 2 | . 2020-01-05 3 | . 2020-01-07 4 | . 2020-01-09 5 | . Main sources . Python for Data Analysis | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/15/time-series.html",
            "relUrl": "/python/pandas/2020/09/15/time-series.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post77": {
            "title": "Imports",
            "content": "Import basics . From reading Chapter 8 in Python Essential Reference: . Calling import module for the first time does three things: . Create a new namespace that acts as the global namespace for all objects defined in module. . | Execute the entire module. . | Create a name -- identical to the module name -- within the caller namespace that references to the module. This can be used to access module objects in the caller namespace as module.object. . | | Calling from module import symbol imports symbol into the current namespace. However, the global namespace for symbol (if it&#39;s a function) always remains the namespace in which it was defined, not the caller&#39;s namespace. . | One reason from module import * is generally discouraged is that it directly imports all the module&#39;s objects into the caller&#39;s namespace, which is often said to cluter it up. Especially when importing large modules this makes sense, as it&#39;s much cleaner to keep objects defined in imported modules in eponymous namespaces and accessing them via module.object, which immediately makes clear where object comes from and can help greatly with debugging. . | . Running files as scripts or modules . For relative imports to work as described, for instance, here and in Chapter 8 in Python Essential References and in recipees 10.1 and 10.3 in the Python Cookbook, the file into which you import has itself to be a module rather than a top-level script. If it&#39;s the latter, it&#39;s name will be main and it won&#39;t be considered part of a package, regardless of where on the file system it is saved. Generally, for a file to be considered part of a package, it needs to nave a dot (.) in its name, as in package.submodule.modulename. From this brilliant SO answer. | . Main sources . The Hitchhiker&#39;s Guide to Python | Fluent Python | Python Cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/15/import.html",
            "relUrl": "/python/2020/09/15/import.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post78": {
            "title": "Web frameworks",
            "content": "Every time I visit a website, an application called a web server who usually resides on a machine also called a web server sends HTML to my browser. HTML (Hypertext markup language) is used by browsers to describe the content and structure of a website. . | The essence of every web application is to send HTML to a browser. . | How does a web server know what data it needs to send? It sends what I request using the HTTP protocol. (HTTP means hypertext transfer protocol; a protocol is a universally agreed data format and sequence of steps to enable communication between two parties.) . | So, a web application receives HTTP requests (e.g. get or post) and responds with an HTTP request, usually in the form of the HTML for the requested page. . | . Main sources . Jeff Knupp post | Robert Chang post | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/06/web-frameworks.html",
            "relUrl": "/python/2020/09/06/web-frameworks.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post79": {
            "title": "Dictionaries",
            "content": "Constructing a dictionary . a = dict(x=1, y=2) b = {&#39;x&#39;: 1, &#39;y&#39;: 2} c = dict(zip([&#39;x&#39;, &#39;y&#39;], [1, 2])) d = dict([(&#39;x&#39;, 1), (&#39;y&#39;, 2)]) e = dict({&#39;y&#39;: 2, &#39;x&#39;: 1}) mylist = [(1, &#39;x&#39;), (2, &#39;y&#39;)] f = {key: value for value, key in mylist} a == b == c == d == e == f . True . a = {1, 2, 3} b = {2, 3, 4} a &amp;= b a . {2, 3} . Main sources . Fluent Python | Python Cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/06/dictionaries.html",
            "relUrl": "/python/2020/09/06/dictionaries.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post80": {
            "title": "Splitting dataframes based on column values",
            "content": "import pandas as pd import numpy as np . The problem . df = pd.DataFrame(data={&#39;case&#39;: [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;,&#39;A&#39;], &#39;data&#39;: np.random.randn(9)}) df . case data . 0 A | 0.684978 | . 1 A | 0.000269 | . 2 A | -1.040497 | . 3 B | 0.451358 | . 4 A | 0.448596 | . 5 A | 0.222168 | . 6 B | 1.031011 | . 7 A | -2.208787 | . 8 A | -0.440758 | . You want to split the dataframe every time case equals B and store the resulting dataframes in a list. . Understanding the cookbook solution . From the cookbook: . dfs = list(zip(*df.groupby((1 * (df[&#39;case&#39;] == &#39;B&#39;)).cumsum().rolling(window=3, min_periods=1).median())))[-1] dfs . ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 3 B 0.451358, case data 4 A 0.448596 5 A 0.222168 6 B 1.031011, case data 7 A -2.208787 8 A -0.440758) . This works. But because it&#39;s so heavily nested and uses methods like rolling() and median() not really designed for that purpose, the code is impossible to interpret at a glance. . Let&#39;s break this down into separate pieces. . First, the code creates a grouping variable that changes its value each time case equaled B on the previous row. The code below shows how it does this. . a = (df.case == &#39;B&#39;) b = 1 * (df.case == &#39;B&#39;) c = 1 * (df.case == &#39;B&#39;).cumsum() d = 1 * (df.case == &#39;B&#39;).cumsum().rolling(window=3, min_periods=1).median() a, b, c, d . (0 False 1 False 2 False 3 True 4 False 5 False 6 True 7 False 8 False Name: case, dtype: bool, 0 0 1 0 2 0 3 1 4 0 5 0 6 1 7 0 8 0 Name: case, dtype: int64, 0 0 1 0 2 0 3 1 4 1 5 1 6 2 7 2 8 2 Name: case, dtype: int64, 0 0.0 1 0.0 2 0.0 3 0.0 4 1.0 5 1.0 6 1.0 7 2.0 8 2.0 Name: case, dtype: float64) . Series d above is the argument passed to groupby() in the solution. This works, but is a very roundabout way to create such a series. I&#39;ll use a different approach below. . Next, the code uses list(), zip(), and argument expansion to pack the data for each group into a single list of dataframes. Let&#39;s look at these one by one. . First, a quick review of how argument expansion works: . def printer(*args, **kwargs): print(&#39;Printing args:&#39;) for arg in args: print(arg) print(&#39;Printing kwargs:&#39;) for kwarg in kwargs.items(): print(kwarg) mylist = [&#39;a&#39;, 2, &#39;k&#39;, 3] mydict = {&#39;first&#39;: 1, &#39;second&#39;: 2} printer(*mylist, **mydict) . Printing args: a 2 k 3 Printing kwargs: (&#39;first&#39;, 1) (&#39;second&#39;, 2) . Now, groupby() stores the grouped data as (label, dataframe) tuples, like so: . groups = df.groupby(&#39;case&#39;) for g in groups: print(g) print(type(g)) . (&#39;A&#39;, case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 4 A 0.448596 5 A 0.222168 7 A -2.208787 8 A -0.440758) &lt;class &#39;tuple&#39;&gt; (&#39;B&#39;, case data 3 B 0.451358 6 B 1.031011) &lt;class &#39;tuple&#39;&gt; . So zip() is used to separate the group label from the data, and list() consumes the iterator created by zip and displays its content. . list(zip(*groups)) . [(&#39;A&#39;, &#39;B&#39;), ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 4 A 0.448596 5 A 0.222168 7 A -2.208787 8 A -0.440758, case data 3 B 0.451358 6 B 1.031011)] . Because we only want the data, we select the last element from the list: . list(zip(*groups))[-1] . ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 4 A 0.448596 5 A 0.222168 7 A -2.208787 8 A -0.440758, case data 3 B 0.451358 6 B 1.031011) . Now we&#39;re basically done. What remains is to use the list(zip(*groups)) procedure on the more complicated grouping variable, to obtain the original result. . d = 1 * (df.case == &#39;B&#39;).cumsum().rolling(window=3, min_periods=1).median() groups = df.groupby(d) list(zip(*groups))[-1] . ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 3 B 0.451358, case data 4 A 0.448596 5 A 0.222168 6 B 1.031011, case data 7 A -2.208787 8 A -0.440758) . Simplifying the code . I think this can be made much more readable like so: . df . case data . 0 A | 0.684978 | . 1 A | 0.000269 | . 2 A | -1.040497 | . 3 B | 0.451358 | . 4 A | 0.448596 | . 5 A | 0.222168 | . 6 B | 1.031011 | . 7 A | -2.208787 | . 8 A | -0.440758 | . grouper = df.case.eq(&#39;B&#39;).cumsum().shift().fillna(0) grouper . 0 0.0 1 0.0 2 0.0 3 0.0 4 1.0 5 1.0 6 1.0 7 2.0 8 2.0 Name: case, dtype: float64 . dfs = [df for (g, df) in df.groupby(grouper)] dfs . [ case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 3 B 0.451358, case data 4 A 0.448596 5 A 0.222168 6 B 1.031011, case data 7 A -2.208787 8 A -0.440758] . In case the logic of this isn&#39;t immediately obvious, the below makes clear what&#39;s going on. . dd = df.set_index(&#39;case&#39;, drop=False) # Use case as index for clarity a = dd.case.eq(&#39;B&#39;) # Boolean logic b = a.cumsum() # Create groups c = b.shift() # Shift so B included in previous group d = c.fillna(0) # Replace 0th element emptied by shift a, b, c, d . (case A False A False A False B True A False A False B True A False A False Name: case, dtype: bool, case A 0 A 0 A 0 B 1 A 1 A 1 B 2 A 2 A 2 Name: case, dtype: int64, case A NaN A 0.0 A 0.0 B 0.0 A 1.0 A 1.0 B 1.0 A 2.0 A 2.0 Name: case, dtype: float64, case A 0.0 A 0.0 A 0.0 B 0.0 A 1.0 A 1.0 B 1.0 A 2.0 A 2.0 Name: case, dtype: float64) .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/04/splitting-dataframes.html",
            "relUrl": "/python/pandas/2020/09/04/splitting-dataframes.html",
            "date": " • Sep 4, 2020"
        }
        
    
  
    
        ,"post81": {
            "title": "Multiindexing in Pandas",
            "content": "Working with indices, expecially column indices, and especially with hierarchical ones, is an area of Pandas I keep finding perplexing. The point of this notebook is to help my future self. . from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . iris = sns.load_dataset(&#39;iris&#39;) iris.head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . Stacking and unstacking column labels . df = df.set_index(&#39;species&#39;) df.head() . sepal_length sepal_width petal_length petal_width . species . setosa 5.1 | 3.5 | 1.4 | 0.2 | . setosa 4.9 | 3.0 | 1.4 | 0.2 | . setosa 4.7 | 3.2 | 1.3 | 0.2 | . setosa 4.6 | 3.1 | 1.5 | 0.2 | . setosa 5.0 | 3.6 | 1.4 | 0.2 | . tups = [tuple(name.split(&#39;_&#39;)) for name in df.columns] colnames = pd.MultiIndex.from_tuples(tups) df.columns = colnames df.head() . sepal petal . length width length width . species . setosa 5.1 | 3.5 | 1.4 | 0.2 | . setosa 4.9 | 3.0 | 1.4 | 0.2 | . setosa 4.7 | 3.2 | 1.3 | 0.2 | . setosa 4.6 | 3.1 | 1.5 | 0.2 | . setosa 5.0 | 3.6 | 1.4 | 0.2 | . df.columns = [&#39;_&#39;.join(name) for name in df.columns] df = df.reset_index() df.head() . species sepal_length sepal_width petal_length petal_width . 0 setosa | 5.1 | 3.5 | 1.4 | 0.2 | . 1 setosa | 4.9 | 3.0 | 1.4 | 0.2 | . 2 setosa | 4.7 | 3.2 | 1.3 | 0.2 | . 3 setosa | 4.6 | 3.1 | 1.5 | 0.2 | . 4 setosa | 5.0 | 3.6 | 1.4 | 0.2 | . Use column values as column label . ... . Sources . Python for Data Analysis | Pandas cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/02/multiindexing-in-pandas.html",
            "relUrl": "/python/pandas/2020/09/02/multiindexing-in-pandas.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post82": {
            "title": "Grouping in Pandas",
            "content": "import pandas as pd import numpy as np . Using apply with groupby . From the cookbook: . df = pd.DataFrame({&#39;animal&#39;: &#39;cat dog cat fish dog cat cat&#39;.split(), &#39;size&#39;: list(&#39;SSMMMLL&#39;), &#39;weight&#39;: [8, 10, 11, 1, 20, 12, 12], &#39;adult&#39;: [False] * 5 + [True] * 2}) df . animal size weight adult . 0 cat | S | 8 | False | . 1 dog | S | 10 | False | . 2 cat | M | 11 | False | . 3 fish | M | 1 | False | . 4 dog | M | 20 | False | . 5 cat | L | 12 | True | . 6 cat | L | 12 | True | . df.groupby(&#39;animal&#39;).apply(lambda g: g.loc[g.weight.idxmax(), &#39;size&#39;]) . animal cat L dog M fish M dtype: object . Expanding apply . Assume you want to calculate the cumulative return from a series of one-period returns in an expanding fashion -- in each period, you want the cumulative return up to that period. . s = pd.Series([i / 100.0 for i in range(1, 4)]) s . 0 0.01 1 0.02 2 0.03 dtype: float64 . The solution is given here. . import functools def cum_return(x, y): return x * (1 + y) def red(x): res = functools.reduce(cum_return, x, 1) return res s.expanding().apply(red, raw=True) . 0 1.010000 1 1.030200 2 1.061106 dtype: float64 . I found that somewhere between bewildering and magical. To see what&#39;s going on, it helps to add a few print statements: . import functools def cum_return(x, y): print(&#39;x:&#39;, x) print(&#39;y:&#39;, y) return x * (1 + y) def red(x): print(&#39;Series:&#39;, x) res = functools.reduce(cum_return, x, 1) print(&#39;Result:&#39;, res) print() return res s.expanding().apply(red, raw=True) . Series: [0.01] x: 1 y: 0.01 Result: 1.01 Series: [0.01 0.02] x: 1 y: 0.01 x: 1.01 y: 0.02 Result: 1.0302 Series: [0.01 0.02 0.03] x: 1 y: 0.01 x: 1.01 y: 0.02 x: 1.0302 y: 0.03 Result: 1.061106 . 0 1.010000 1 1.030200 2 1.061106 dtype: float64 . This makes transparent how reduce works: it takes the starting value (1 here) as the initial x value and the first value of the series as y value, and then returns the result of cum_returns. Next, it uses that result as x, and the second element in the series as y, and calculates the new result of cum_returns. This is then repeated until it has run through the entire series. . What surprised me is to see that reduce always starts the calculation from the beginning, rather than re-using the last calculated result. This seems inefficient, but is probably necessary for some reason. . Sort by sum of group values . df = pd.DataFrame({&#39;code&#39;: [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;] * 2, &#39;data&#39;: [0.16, -0.21, 0.33, 0.45, -0.59, 0.62], &#39;flag&#39;: [False, True] * 3}) df . code data flag . 0 foo | 0.16 | False | . 1 bar | -0.21 | True | . 2 baz | 0.33 | False | . 3 foo | 0.45 | True | . 4 bar | -0.59 | False | . 5 baz | 0.62 | True | . g = df.groupby(&#39;code&#39;) sort_order = g[&#39;data&#39;].transform(sum).sort_values().index df.loc[sort_order] . code data flag . 1 bar | -0.21 | True | . 4 bar | -0.59 | False | . 0 foo | 0.16 | False | . 3 foo | 0.45 | True | . 2 baz | 0.33 | False | . 5 baz | 0.62 | True | . g = df.groupby(&#39;code&#39;) g.apply(lambda g: g.loc[g.data.idxmax()]) . code data flag . code . bar bar | -0.21 | True | . baz baz | 0.62 | True | . foo foo | 0.45 | True | . Expanding group operations . Based on this answer. . df = pd.DataFrame({&#39;code&#39;: [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;] * 4, &#39;data&#39;: [0.16, -0.21, 0.33, 0.45, -0.59, 0.62] * 2, &#39;flag&#39;: [False, True] * 6}) df . code data flag . 0 foo | 0.16 | False | . 1 bar | -0.21 | True | . 2 baz | 0.33 | False | . 3 foo | 0.45 | True | . 4 bar | -0.59 | False | . 5 baz | 0.62 | True | . 6 foo | 0.16 | False | . 7 bar | -0.21 | True | . 8 baz | 0.33 | False | . 9 foo | 0.45 | True | . 10 bar | -0.59 | False | . 11 baz | 0.62 | True | . g = df.groupby(&#39;code&#39;) def helper(g): s = g.data.expanding() g[&#39;exp_mean&#39;] = s.mean() g[&#39;exp_sum&#39;] = s.sum() g[&#39;exp_count&#39;] = s.count() return g g.apply(helper).sort_values(&#39;code&#39;) . code data flag exp_mean exp_sum exp_count . 1 bar | -0.21 | True | -0.210000 | -0.21 | 1.0 | . 4 bar | -0.59 | False | -0.400000 | -0.80 | 2.0 | . 7 bar | -0.21 | True | -0.336667 | -1.01 | 3.0 | . 10 bar | -0.59 | False | -0.400000 | -1.60 | 4.0 | . 2 baz | 0.33 | False | 0.330000 | 0.33 | 1.0 | . 5 baz | 0.62 | True | 0.475000 | 0.95 | 2.0 | . 8 baz | 0.33 | False | 0.426667 | 1.28 | 3.0 | . 11 baz | 0.62 | True | 0.475000 | 1.90 | 4.0 | . 0 foo | 0.16 | False | 0.160000 | 0.16 | 1.0 | . 3 foo | 0.45 | True | 0.305000 | 0.61 | 2.0 | . 6 foo | 0.16 | False | 0.256667 | 0.77 | 3.0 | . 9 foo | 0.45 | True | 0.305000 | 1.22 | 4.0 | . Pivoting . From here . df = pd.DataFrame(data={&#39;province&#39;: [&#39;ON&#39;, &#39;QC&#39;, &#39;BC&#39;, &#39;AL&#39;, &#39;AL&#39;, &#39;MN&#39;, &#39;ON&#39;], &#39;city&#39;: [&#39;Toronto&#39;, &#39;Montreal&#39;, &#39;Vancouver&#39;, &#39;Calgary&#39;, &#39;Edmonton&#39;, &#39;Winnipeg&#39;, &#39;Windsor&#39;], &#39;sales&#39;: [13, 6, 16, 8, 4, 3, 1]}) df . province city sales . 0 ON | Toronto | 13 | . 1 QC | Montreal | 6 | . 2 BC | Vancouver | 16 | . 3 AL | Calgary | 8 | . 4 AL | Edmonton | 4 | . 5 MN | Winnipeg | 3 | . 6 ON | Windsor | 1 | . You want to group sales by province and get subtotal for total state. . table = ( df .pivot_table(values=&#39;sales&#39;, index=&#39;province&#39;, columns=&#39;city&#39;, aggfunc=&#39;sum&#39;, margins=True) .stack() .drop(&#39;All&#39;) ) table . province city AL Calgary 8.0 Edmonton 4.0 All 12.0 BC Vancouver 16.0 All 16.0 MN Winnipeg 3.0 All 3.0 ON Toronto 13.0 Windsor 1.0 All 14.0 QC Montreal 6.0 All 6.0 dtype: float64 . Aggregating . From here . df = pd.DataFrame( {&#39;StudentID&#39;: [&quot;x1&quot;, &quot;x10&quot;, &quot;x2&quot;,&quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;], &#39;StudentGender&#39; : [&#39;F&#39;, &#39;M&#39;, &#39;F&#39;, &#39;M&#39;, &#39;F&#39;, &#39;M&#39;, &#39;F&#39;, &#39;M&#39;, &#39;M&#39;, &#39;M&#39;], &#39;ExamYear&#39;: [&#39;2007&#39;,&#39;2007&#39;,&#39;2007&#39;,&#39;2008&#39;,&#39;2008&#39;,&#39;2008&#39;,&#39;2008&#39;,&#39;2009&#39;,&#39;2009&#39;,&#39;2009&#39;], &#39;Exam&#39;: [&#39;algebra&#39;, &#39;stats&#39;, &#39;bio&#39;, &#39;algebra&#39;, &#39;algebra&#39;, &#39;stats&#39;, &#39;stats&#39;, &#39;algebra&#39;, &#39;bio&#39;, &#39;bio&#39;], &#39;Participated&#39;: [&#39;no&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;no&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;], &#39;Passed&#39;: [&#39;no&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;no&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;no&#39;,&#39;yes&#39;]}, columns = [&#39;StudentID&#39;, &#39;StudentGender&#39;, &#39;ExamYear&#39;, &#39;Exam&#39;, &#39;Participated&#39;, &#39;Passed&#39;]) df.columns = [str.lower(c) for c in df.columns] df . studentid studentgender examyear exam participated passed . 0 x1 | F | 2007 | algebra | no | no | . 1 x10 | M | 2007 | stats | yes | yes | . 2 x2 | F | 2007 | bio | yes | yes | . 3 x3 | M | 2008 | algebra | yes | yes | . 4 x4 | F | 2008 | algebra | no | no | . 5 x5 | M | 2008 | stats | yes | yes | . 6 x6 | F | 2008 | stats | yes | yes | . 7 x7 | M | 2009 | algebra | yes | yes | . 8 x8 | M | 2009 | bio | yes | no | . 9 x9 | M | 2009 | bio | yes | yes | . numyes = lambda x: sum(x == &#39;yes&#39;) df.groupby(&#39;examyear&#39;).agg({&#39;participated&#39;: numyes, &#39;passed&#39;: numyes}) . participated passed . examyear . 2007 2 | 2 | . 2008 3 | 3 | . 2009 3 | 2 | . Troubleshooting . idx = [1, 1, 1, 1, 1, 2, 3, 3] keys = [&#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;, &#39;C&#39;, &#39;A&#39;, &#39;B&#39;, &#39;C&#39;] vals = [ 1, 2, 3, 4, 5, 6, 7, 8] pd.DataFrame(zip(keys, vals), columns=[&#39;id&#39;, &#39;var&#39;], index=idx).groupby(&#39;id&#39;).apply(lambda x: x) . id var . 1 A | 1 | . 1 A | 2 | . 1 B | 3 | . 1 B | 4 | . 1 C | 5 | . 2 A | 6 | . 3 B | 7 | . 3 C | 8 | . Sources . Python for Data Analysis | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/02/grouping-in-pandas.html",
            "relUrl": "/python/pandas/2020/09/02/grouping-in-pandas.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post83": {
            "title": "Sequences",
            "content": "Disclaimer: most of the code here is directly taken from one of the sources mentioned at the bottom. The point of this post is not to produce new knowledge, but to understand and practice what I read, and to leave a reference for my future self. . Splitting and slicing . Slice from (and/or) to particular characters . a = &#39;abc[def]&#39; a[a.find(&#39;[&#39;):] . &#39;[def]&#39; . this works because . a.find(&#39;[&#39;) . 3 . Lists . Elegantly creating a list . Instead of this: . a = [&#39;apple&#39;, &#39;banana&#39;, &#39;cherry&#39;] . I can do this: . b = &#39;apple banana cherry&#39;.split() . a == b . True . List comprehensions . colors = [&#39;blue&#39;, &#39;yellow&#39;] sizes = &#39;SML&#39; for color in colors: for size in sizes: print(color, size) . blue S blue M blue L yellow S yellow M yellow L . shirts = [(color, size) for color in colors for size in sizes] shirts . [(&#39;blue&#39;, &#39;S&#39;), (&#39;blue&#39;, &#39;M&#39;), (&#39;blue&#39;, &#39;L&#39;), (&#39;yellow&#39;, &#39;S&#39;), (&#39;yellow&#39;, &#39;M&#39;), (&#39;yellow&#39;, &#39;L&#39;)] . shirts = [(color, size) for size in sizes for color in colors] shirts . [(&#39;blue&#39;, &#39;S&#39;), (&#39;yellow&#39;, &#39;S&#39;), (&#39;blue&#39;, &#39;M&#39;), (&#39;yellow&#39;, &#39;M&#39;), (&#39;blue&#39;, &#39;L&#39;), (&#39;yellow&#39;, &#39;L&#39;)] . Note that the ordering depends on the order of the for statements, just as it would in a nested for loop. . Tuples . Named tuples . Main sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/01/sequences.html",
            "relUrl": "/python/2020/09/01/sequences.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post84": {
            "title": "Pandas cookbook notes",
            "content": "The cookbook is here . import pandas as pd import numpy as np . Sort rows based on closeness to certain value . df = pd.DataFrame({&#39;AAA&#39;: [4, 5, 6, 7], &#39;BBB&#39;: [10, 20, 30, 40], &#39;CCC&#39;: [100, 50, -30, -50]}) df . AAA BBB CCC . 0 4 | 10 | 100 | . 1 5 | 20 | 50 | . 2 6 | 30 | -30 | . 3 7 | 40 | -50 | . To sort the rows in order of closeness to myval, do this: . myval = 35 df.loc[(df.CCC - myval).abs().argsort()] . AAA BBB CCC . 1 5 | 20 | 50 | . 0 4 | 10 | 100 | . 2 6 | 30 | -30 | . 3 7 | 40 | -50 | . A note of caution on argsort: . myval = 35 a = (df.CCC - myval).abs() b = a.argsort() a, b . (0 65 1 15 2 65 3 85 Name: CCC, dtype: int64, 0 1 1 0 2 2 3 3 Name: CCC, dtype: int64) . myval = 34 a = (df.CCC - myval).abs() b = a.argsort() a, b . (0 66 1 16 2 64 3 84 Name: CCC, dtype: int64, 0 1 1 2 2 0 3 3 Name: CCC, dtype: int64) . I tripped up expecting the result of argsort to look like the first case and got really confused by the result of the second case because I expected argsort to return a series containing the rank of each value in the original series. This post provided the solution: argsort doesn&#39;t return a series of ranks but a series such that a[b] returns a sorted version a. Hence, the first value in b tells us that in the sorted series a[b], the 0th element will be element 1 in the original series a. . Compound boolean selection . Careful when using compound boolean conditions; it took me a moment to figure out why the result below (from the cookbook) is correct. . df . AAA BBB CCC . 0 4 | 10 | 100 | . 1 5 | 20 | 50 | . 2 6 | 30 | -30 | . 3 7 | 40 | -50 | . df[~((df.AAA &lt;= 6) &amp; (df.index.isin([0, 2, 4])))] . AAA BBB CCC . 1 5 | 20 | 50 | . 3 7 | 40 | -50 | . What confused me was that 5 is smaller than 6. The key thing to remember is that not (a &amp; b) equals (not a) | (not b). . Creating new columns based on existing ones using mappings . The below is a straightforward adaptation from the cookbook: . df = pd.DataFrame({&#39;AAA&#39;: [1, 2, 1, 3], &#39;BBB&#39;: [1, 1, 4, 2], &#39;CCC&#39;: [2, 1, 3, 1]}) source_cols = [&#39;AAA&#39;, &#39;BBB&#39;] new_cols = [str(c) + &#39;_cat&#39; for c in source_cols] cats = {1: &#39;One&#39;, 2: &#39;Two&#39;, 3: &#39;Three&#39;} dd = df.copy() dd[new_cols] = df[source_cols].applymap(cats.get) dd . AAA BBB CCC AAA_cat BBB_cat . 0 1 | 1 | 2 | One | One | . 1 2 | 1 | 1 | Two | One | . 2 1 | 4 | 3 | One | None | . 3 3 | 2 | 1 | Three | Two | . But it made me wonder why applymap required the use of the get method while we can map values of a series like so: . s = pd.Series([1, 2, 3, 1]) s.map(cats) . 0 One 1 Two 2 Three 3 One dtype: object . or so . s.map(cats.get) . 0 One 1 Two 2 Three 3 One dtype: object . The answer is simple: applymap requires a function as argument, while map takes functions or mappings. . One limitation of the cookbook solution above is that is doesn&#39;t seem to allow for default values (notice that 4 gets substituted with &quot;None&quot;). . One way around this is the following: . df[new_cols] = df[source_cols].applymap(lambda x: cats.get(x, &#39;Hello&#39;)) df . AAA BBB CCC AAA_cat BBB_cat . 0 1 | 1 | 2 | One | One | . 1 2 | 1 | 1 | Two | One | . 2 1 | 4 | 3 | One | Hello | . 3 3 | 2 | 1 | Three | Two | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/01/pandas-cookbook-notes.html",
            "relUrl": "/python/pandas/2020/09/01/pandas-cookbook-notes.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post85": {
            "title": "Categorical variables in Pandas",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-4c1e2ea1ae5b&gt; in &lt;module&gt; -&gt; 1 from imports import * 2 3 get_ipython().run_line_magic(&#39;config&#39;, &#34;InlineBackend.figure_format = &#39;retina&#39;&#34;) 4 get_ipython().run_line_magic(&#39;load_ext&#39;, &#39;autoreload&#39;) 5 get_ipython().run_line_magic(&#39;autoreload&#39;, &#39;2&#39;) ~/Library/Mobile Documents/com~apple~CloudDocs/fab/projects/blog/_notebooks/imports.py in &lt;module&gt; 5 import sqlite3 6 -&gt; 7 import linearmodels 8 import numpy as np 9 import pandas as pd ModuleNotFoundError: No module named &#39;linearmodels&#39; . drinks = pd.read_csv(&#39;http://bit.ly/drinksbycountry&#39;) drinks.head() . country beer_servings spirit_servings wine_servings total_litres_of_pure_alcohol continent . 0 Afghanistan | 0 | 0 | 0 | 0.0 | Asia | . 1 Albania | 89 | 132 | 54 | 4.9 | Europe | . 2 Algeria | 25 | 0 | 14 | 0.7 | Africa | . 3 Andorra | 245 | 138 | 312 | 12.4 | Europe | . 4 Angola | 217 | 57 | 45 | 5.9 | Africa | . Inspect memory usage . drinks.info(memory_usage=&#39;deep&#39;) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 193 entries, 0 to 192 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 country 193 non-null object 1 beer_servings 193 non-null int64 2 spirit_servings 193 non-null int64 3 wine_servings 193 non-null int64 4 total_litres_of_pure_alcohol 193 non-null float64 5 continent 193 non-null object dtypes: float64(1), int64(3), object(2) memory usage: 30.5 KB . Inspect memory usage by column . drinks.memory_usage(deep=&#39;true&#39;) . Index 128 country 12588 beer_servings 1544 spirit_servings 1544 wine_servings 1544 total_litres_of_pure_alcohol 1544 continent 12332 dtype: int64 . Convert country to category type . drinks.continent = drinks.continent.astype(&#39;category&#39;) . drinks.memory_usage(deep=&#39;true&#39;) . Index 128 country 18094 beer_servings 1544 spirit_servings 1544 wine_servings 1544 total_litres_of_pure_alcohol 1544 continent 744 dtype: int64 . drinks.continent.cat.categories . Index([&#39;Africa&#39;, &#39;Asia&#39;, &#39;Europe&#39;, &#39;North America&#39;, &#39;Oceania&#39;, &#39;South America&#39;], dtype=&#39;object&#39;) . df = pd.DataFrame({&#39;id&#39;:[1, 2, 3, 4, 5], &#39;quality&#39;:[&#39;good&#39;, &#39;excellent&#39;, &#39;very good&#39;, &#39;excellent&#39;, &#39;good&#39;]}) df . id quality . 0 1 | good | . 1 2 | excellent | . 2 3 | very good | . 3 4 | excellent | . 4 5 | good | . df.sort_values(&#39;quality&#39;) . id quality . 1 2 | excellent | . 3 4 | excellent | . 0 1 | good | . 4 5 | good | . 2 3 | very good | . from pandas.api.types import CategoricalDtype quality_cat = CategoricalDtype([&#39;good&#39;, &#39;very good&#39;, &#39;excellent&#39;], ordered=True) df.quality = df.quality.astype(quality_cat) df . id quality . 0 1 | good | . 1 2 | excellent | . 2 3 | very good | . 3 4 | excellent | . 4 5 | good | . df.quality . 0 good 1 excellent 2 very good 3 excellent 4 good Name: quality, dtype: category Categories (3, object): [good &lt; very good &lt; excellent] . dummies = pd.get_dummies(df.quality) df = pd.concat([df, dummies], axis=1) df . id quality good very good excellent . 0 1 | good | 1 | 0 | 0 | . 1 2 | excellent | 0 | 0 | 1 | . 2 3 | very good | 0 | 1 | 0 | . 3 4 | excellent | 0 | 0 | 1 | . 4 5 | good | 1 | 0 | 0 | . Sources . Python for Data Analysis | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/01/categorical-variables-in-pandas.html",
            "relUrl": "/python/pandas/2020/09/01/categorical-variables-in-pandas.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post86": {
            "title": "Reset cumsum counter at missing values",
            "content": "Problem . I want to reset cumcount() at each missing value in an array or series. . Solution . From the Pandas cookbook, the below two approaches solve the problem using Numpy and Pandas. . Numpy solution, from here . import numpy as np v = np.array([1., 1., 1., np.nan, 1., 1., 1., 1., np.nan, 1.]) isna = np.isnan(v) notna = ~isna # replace missing values with cumsum up to this point cumsum = np.cumsum(notna) diffs = np.diff(np.concatenate(([0], cumsum[isna]))) v[isna] = -diffs np.cumsum(v) . array([1., 2., 3., 0., 1., 2., 3., 4., 0., 1.]) . Using Pandas . From here, has the additional benefit of also working with values other than 1. . import pandas as pd s = pd.Series([1., 3., 1., np.nan, 1., 1., 1., 1., np.nan, 1.]) cumsum = s.cumsum().ffill() reset = -cumsum[s.isna()].diff().fillna(cumsum) result = s.where(s.notna(), reset).cumsum() result . 0 1.0 1 4.0 2 5.0 3 0.0 4 1.0 5 2.0 6 3.0 7 4.0 8 0.0 9 1.0 dtype: float64 .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/08/09/reset-cumsum-at-missing.html",
            "relUrl": "/python/pandas/2020/08/09/reset-cumsum-at-missing.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post87": {
            "title": "Counting number of equal adjacent values",
            "content": "Problem . Count the number of adjacent equal values in a series. . import pandas as pd df = pd.DataFrame([0, 1, 5, 7, 7, 1, 1, 1, 0, 1, 1], columns=[&#39;a&#39;]) df . a . 0 0 | . 1 1 | . 2 5 | . 3 7 | . 4 7 | . 5 1 | . 6 1 | . 7 1 | . 8 0 | . 9 1 | . 10 1 | . Answer . Only slightly adapted from this Stack Overflow answer. . df[&#39;count&#39;] = df.groupby((df.a != df.a.shift()).cumsum()).cumcount().add(1) df . a count . 0 0 | 1 | . 1 1 | 1 | . 2 5 | 1 | . 3 7 | 1 | . 4 7 | 2 | . 5 1 | 1 | . 6 1 | 2 | . 7 1 | 3 | . 8 0 | 1 | . 9 1 | 1 | . 10 1 | 2 | . How it works . Demark group boundaries by checking for different preceeding value . df.a != df.a.shift() . 0 True 1 True 2 True 3 True 4 False 5 True 6 False 7 False 8 True 9 True 10 False Name: a, dtype: bool . Label groups using cumulative counting . (df.a != df.a.shift()).cumsum() . 0 1 1 2 2 3 3 4 4 4 5 5 6 5 7 5 8 6 9 7 10 7 Name: a, dtype: int64 . Group dataset by labelled groups . for idx, data in df.groupby((df.a != df.a.shift()).cumsum()): display(data) print() . a . 0 0 | . . a . 1 1 | . . a . 2 5 | . . a . 3 7 | . 4 7 | . . a . 5 1 | . 6 1 | . 7 1 | . . a . 8 0 | . . a . 9 1 | . 10 1 | . . Generate a cumulative count for each group . df.a.groupby((df.a != df.a.shift()).cumsum()).cumcount() . 0 0 1 0 2 0 3 0 4 1 5 0 6 1 7 2 8 0 9 0 10 1 dtype: int64 .",
            "url": "https://fabiangunzinger.github.io/blog/pandas/2020/08/09/count-equal-adjacent-values.html",
            "relUrl": "/pandas/2020/08/09/count-equal-adjacent-values.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post88": {
            "title": "Tidy Tuesdays with Python",
            "content": "Tidy Tuesday site | David Robinson&#39;s approach | . The project was to do what David Robinson does -- to give myself an hour to analyse a dataset -- and then to compare my results to his. Only did this once, and incompletely. But hope to get back to it at some point. It&#39;s a great and fun way to practice fluency in data munging and visualisation. . My code (after 1 hour) . import pandas as pd import numpy as np import matplotlib.pyplot as plt plt.style.use(&quot;seaborn&quot;) # %load_ext lab_black . cran = pd.read_csv( &quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-12/loc_cran_packages.csv&quot; ) . cran.describe() . file blank comment code . count 34477.000000 | 34477.000000 | 34477.000000 | 3.447700e+04 | . mean 11.165821 | 257.097775 | 432.708820 | 1.506400e+03 | . std 66.075754 | 2011.333988 | 2814.100058 | 1.255484e+04 | . min 1.000000 | 0.000000 | 0.000000 | 0.000000e+00 | . 25% 1.000000 | 17.000000 | 1.000000 | 8.300000e+01 | . 50% 3.000000 | 53.000000 | 33.000000 | 3.360000e+02 | . 75% 10.000000 | 174.000000 | 284.000000 | 1.043000e+03 | . max 10737.000000 | 310945.000000 | 304465.000000 | 1.580460e+06 | . fig, [ax0, ax1, ax2] = plt.subplots(1, 3, figsize=(15, 5), sharey=True) cran[&quot;code&quot;].hist(bins=np.arange(0, 5000, 50), ax=ax0) ax0.set_xlabel(&quot;Lines of code&quot;) ax0.set_ylabel(&quot;Count&quot;) cran[&quot;comment&quot;].hist(bins=np.arange(0, 2000, 20), ax=ax1) ax1.set_xlabel(&quot;Lines of comment&quot;) cran[&quot;blank&quot;].hist(bins=np.arange(0, 2000, 20), ax=ax2) ax2.set_xlabel(&quot;Number of blank lines&quot;) . Text(0.5, 0, &#39;Number of blank lines&#39;) . counts = cran[&quot;language&quot;].value_counts().head(20) counts.sort_values(ascending=True).plot(kind=&quot;barh&quot;) plt.xlabel(&quot;Number of packages&quot;) . Text(0.5, 0, &#39;Number of packages&#39;) . top = counts.head(4) colors = [&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;] for idx, lang in enumerate(top.index): data = df[df[&quot;language&quot;] == lang] data[&quot;code&quot;].hist( bins=np.arange(0, 2000, 20), alpha=0.3, label=lang, color=colors[idx] ) plt.legend() . &lt;matplotlib.legend.Legend at 0x120c3dc10&gt; . Stuff based on David Robinson&#39;s approach . print(cran.shape) cran.head() . (34477, 7) . file language blank comment code pkg_name version . 0 2 | R | 96 | 353 | 365 | A3 | 1.0.0 | . 1 1 | HTML | 347 | 5 | 2661 | aaSEA | 1.0.0 | . 2 23 | R | 63 | 325 | 676 | aaSEA | 1.0.0 | . 3 3 | HTML | 307 | 9 | 1275 | abbyyR | 0.5.5 | . 4 30 | R | 224 | 636 | 587 | abbyyR | 0.5.5 | . by_language = ( cran.groupby(&quot;language&quot;) .agg( packages=(&quot;code&quot;, &quot;count&quot;), files=(&quot;file&quot;, &quot;sum&quot;), code=(&quot;code&quot;, &quot;sum&quot;), comment=(&quot;comment&quot;, &quot;sum&quot;), ) .assign( lines_per_package=lambda df: df.code / df.packages, files_per_package=lambda df: df.files / df.packages, comment_code_ratio=lambda df: df.comment / df.code, ) .sort_values(&quot;packages&quot;, ascending=False) ) by_language . packages files code comment lines_per_package files_per_package comment_code_ratio . language . R 14689 | 267967 | 22822548 | 9414210 | 1553.716931 | 18.242699 | 0.412496 | . Markdown 5710 | 9036 | 636948 | 1 | 111.549562 | 1.582487 | 0.000002 | . HTML 3680 | 7893 | 4293856 | 32783 | 1166.808696 | 2.144837 | 0.007635 | . C 2162 | 13540 | 4764598 | 1171456 | 2203.791859 | 6.262720 | 0.245867 | . C++ 2041 | 16442 | 3957771 | 817848 | 1939.133268 | 8.055855 | 0.206644 | . ... ... | ... | ... | ... | ... | ... | ... | . SWIG 1 | 5 | 2666 | 507 | 2666.000000 | 5.000000 | 0.190173 | . ActionScript 1 | 2 | 3110 | 0 | 3110.000000 | 2.000000 | 0.000000 | . PowerShell 1 | 1 | 3 | 0 | 3.000000 | 1.000000 | 0.000000 | . Prolog 1 | 1 | 6 | 0 | 6.000000 | 1.000000 | 0.000000 | . Velocity Template Language 1 | 32 | 851 | 194 | 851.000000 | 32.000000 | 0.227967 | . 108 rows × 7 columns . by_language.head(20)[&quot;packages&quot;].sort_values(ascending=True).plot(kind=&quot;barh&quot;) plt.xlabel(&quot;Number of packages written in langauge&quot;) . Text(0.5, 0, &#39;Number of packages written in langauge&#39;) . data = by_language.sort_values(&quot;packages&quot;, ascending=False).query(&quot;packages &gt; 20&quot;) x = data[&quot;packages&quot;].transform(&quot;log10&quot;) y = data[&quot;comment_code_ratio&quot;] label = data.index plt.figure(figsize=(10, 7)) plt.scatter(x, y) for x, y, label in zip(x, y, label): plt.text(x + 0.02, y + 0.01, label) plt.xlabel(&quot;Number of packages (in log10)&quot;) plt.ylabel(&quot;Comment to code ratio&quot;) . Text(0, 0.5, &#39;Comment to code ratio&#39;) . data = by_language.sort_values(&quot;packages&quot;, ascending=False).query(&quot;packages &gt; 20&quot;) x = data[&quot;packages&quot;].transform(&quot;log10&quot;) y = data[&quot;lines_per_package&quot;] label = data.index plt.figure(figsize=(10, 7)) plt.scatter(x, y) for x, y, label in zip(x, y, label): plt.text(x + 0.02, y + 0.01, label) plt.xlabel(&quot;Number of packages (in log10)&quot;) plt.ylabel(&quot;Lines per package&quot;) . Text(0, 0.5, &#39;Lines per package&#39;) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) data = cran.query(&quot;language == &#39;R&#39; &amp; code &gt; 0&quot;)[&quot;code&quot;] data.transform(&quot;log10&quot;).hist(bins=40, ax=ax1) ax1.axvline( np.log10(data.median()), color=&quot;green&quot;, label=&quot;Median = {:2.0f}&quot;.format(data.median()), ) ax1.axvline( np.log10(data.mean()), color=&quot;orange&quot;, label=&quot;Mean = {:2.0f}&quot;.format(data.mean()) ) data.hist(bins=40, ax=ax2) ax2.axvline(data.median(), color=&quot;green&quot;) ax2.axvline(data.mean(), color=&quot;orange&quot;) fig.legend() . &lt;matplotlib.legend.Legend at 0x12835ac10&gt; . cran.sort_values(&quot;code&quot;, ascending=False).head(10) # cran.sort_values(&quot;code&quot;, ascending=False).tail(5) . file language blank comment code pkg_name version . 2096 10737 | C/C++ Header | 310945 | 304465 | 1580460 | BH | 1.69.0-1 | . 5647 27 | SVG | 0 | 0 | 1188625 | dabestr | 0.2.2 | . 4753 39 | JSON | 0 | 0 | 481022 | congressbr | 0.2.1 | . 30620 804 | C++ | 59385 | 102574 | 385839 | stringi | 1.4.3 | . 7649 20 | HTML | 17075 | 123 | 342681 | edgarWebR | 1.0.0 | . 23897 294 | JSON | 1019 | 0 | 295553 | rcorpora | 2.0.0 | . 5428 17 | C/C++ Header | 469 | 317 | 198162 | cubature | 2.0.3 | . 28181 48 | C | 23053 | 79977 | 184799 | seqminer | 7.1 | . 6836 913 | C/C++ Header | 46085 | 78546 | 183763 | dlib | 1.0.3 | . 1750 159 | C++ | 53470 | 26909 | 180621 | BayesXsrc | 3.0-1 | . plt.style.use(&#39;ggplot&#39;) (cran[cran.pkg_name.str.contains(&quot;^tidy&quot;)] .groupby([&quot;pkg_name&quot;, &quot;language&quot;]).code.sum() .unstack() .assign(tot=lambda x: x.sum(axis=1)) .sort_values(&#39;tot&#39;).drop(&#39;tot&#39;, axis=1) ).plot.barh(stacked=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x126467390&gt; . Replicate above again from memory and as fast as I can while takling out lound tomorrow . (cran.groupby(&#39;pkg_name&#39;).sum() .query(&#39;code &gt; 100 &amp; comment &gt; 0&#39;) .assign(code_comment_ratio = lambda df: (df.code / df.comment).transform(&#39;log10&#39;), tidy = lambda df: df.index.str.startswith(&#39;tidy&#39;) ) )[[&#39;tidy&#39;, &#39;code_comment_ratio&#39;]].groupby(&#39;tidy&#39;).hist(bins=10) . tidy False [[AxesSubplot(0.125,0.125;0.775x0.755)]] True [[AxesSubplot(0.125,0.125;0.775x0.755)]] dtype: object . Correlation between size and downloads . downloads = pd.read_csv(&#39;http://cran-logs.rstudio.com/2020/2020-03-02.csv.gz&#39;) . print(downloads.shape) downloads.head() . (5015440, 10) . date time size r_version r_arch r_os package version country ip_id . 0 2020-03-02 | 16:12:36 | 817352 | NaN | NaN | NaN | rlang | 0.4.5 | US | 1 | . 1 2020-03-02 | 16:15:07 | 65936 | NaN | NaN | NaN | generics | 0.0.2 | GB | 2 | . 2 2020-03-02 | 16:15:00 | 461452 | NaN | NaN | NaN | scatterplot3d | 0.3-41 | US | 3 | . 3 2020-03-02 | 16:13:46 | 202327 | 3.6.2 | x86_64 | mingw32 | modelr | 0.1.6 | US | 4 | . 4 2020-03-02 | 16:10:55 | 20721 | NaN | NaN | NaN | ColorPalette | 1.0-1 | US | 5 | . downloads_per_package = downloads.rename({&#39;package&#39;: &#39;pkg_name&#39;, &#39;data&#39;:&#39;downloads&#39;}, axis=1).groupby(&#39;pkg_name&#39;).downloads.count() . AttributeError Traceback (most recent call last) &lt;ipython-input-177-fae9a88496e2&gt; in &lt;module&gt; -&gt; 1 downloads_per_package = downloads.rename({&#39;package&#39;: &#39;pkg_name&#39;, &#39;data&#39;:&#39;downloads&#39;}, axis=1).groupby(&#39;pkg_name&#39;).downloads.count() ~/miniconda3/envs/basics/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in __getattr__(self, attr) 578 579 raise AttributeError( --&gt; 580 f&#34;&#39;{type(self).__name__}&#39; object has no attribute &#39;{attr}&#39;&#34; 581 ) 582 AttributeError: &#39;DataFrameGroupBy&#39; object has no attribute &#39;downloads&#39; . pd.merge(cran, downloads_per_package, on=&#39;pkg_name&#39;) . file language blank comment code pkg_name version date . 0 2 | R | 96 | 353 | 365 | A3 | 1.0.0 | 40 | . 1 1 | HTML | 347 | 5 | 2661 | aaSEA | 1.0.0 | 17 | . 2 23 | R | 63 | 325 | 676 | aaSEA | 1.0.0 | 17 | . 3 3 | HTML | 307 | 9 | 1275 | abbyyR | 0.5.5 | 26 | . 4 30 | R | 224 | 636 | 587 | abbyyR | 0.5.5 | 26 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 34146 1 | Markdown | 13 | 0 | 15 | ztype | 0.1.0 | 18 | . 34147 10 | R | 121 | 221 | 447 | ZVCV | 1.0.0 | 8 | . 34148 2 | C++ | 20 | 7 | 70 | ZVCV | 1.0.0 | 8 | . 34149 1 | Markdown | 7 | 0 | 11 | ZVCV | 1.0.0 | 8 | . 34150 1 | R | 51 | 17 | 169 | zyp | 0.10-1.1 | 27 | . 34151 rows × 8 columns .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/05/03/tidy-tuesday.html",
            "relUrl": "/python/pandas/2020/05/03/tidy-tuesday.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post89": {
            "title": "Data visualisation",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . Matplotlib . Subplots from columns in df . Adapted from this post . with plt.style.context(&#39;seaborn-deep&#39;): fig, axes = plt.subplots(2, 2, figsize=(8, 8)) for ax, col in zip(axes.flatten(), accounts.columns): # zip does the magic! s = accounts[col] ax.hist(s, alpha=0.7) med, p75 = s.quantile([.5, .75]) ax.axvline(med, color=&#39;g&#39;, label=&#39;50th pct: &#39; + format(med, &#39;.0f&#39;)) ax.axvline(p75, color=&#39;orange&#39;, label=&#39;75th pct: &#39; + format(p75, &#39;.0f&#39;)) ax.legend() . NameError Traceback (most recent call last) &lt;ipython-input-1-3a95d9d76c1b&gt; in &lt;module&gt; -&gt; 1 with plt.style.context(&#39;seaborn-deep&#39;): 2 fig, axes = plt.subplots(2, 2, figsize=(8, 8)) 3 4 for ax, col in zip(axes.flatten(), accounts.columns): # zip does the magic! 5 s = accounts[col] NameError: name &#39;plt&#39; is not defined . Effect of holidays on births . From Python Data Science Handbook . file = &#39;https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv&#39; df = pd.read_csv(file).dropna() print(df.shape) df.head() . (15067, 5) . year month day gender births . 0 1969 | 1 | 1.0 | F | 4046 | . 1 1969 | 1 | 1.0 | M | 4440 | . 2 1969 | 1 | 2.0 | F | 4454 | . 3 1969 | 1 | 2.0 | M | 4548 | . 4 1969 | 1 | 3.0 | F | 4548 | . # Eliminate outliers using sigma-clipping pcts = np.percentile(df.births, [25, 50, 75]) mu, sigma = pcts[1], 0.74 * (pcts[2] - pcts[0]) df = df.query(&#39;(births &gt; @mu - 5 * @sigma) &amp; (births &lt; @mu + 5 * @sigma)&#39;) # Create datetime index df.index = pd.to_datetime(df.year * 10_000 + df.month * 100 + df.day, format=&#39;%Y%m%d&#39;) # Make pivot table with month-day index and mean of births column day_means = df.pivot_table(&#39;births&#39;, [df.index.month, df.index.day]) # Turn pivot index into year-month-day index (for leap year) for plotting from datetime import date day_means.index = [date(2020, month, day) for month, day in day_means.index] . fig, ax = plt.subplots(figsize=(12, 4)) style = dict(color=&#39;cornflowerblue&#39;, linewidth=4, style=&#39;-&#39;) day_means.plot(ax=ax, legend=None, **style) # Format ax.set(ylim=(3600, None), ylabel=&#39;Mean number of births&#39;) ax.xaxis.set_major_locator(mpl.dates.MonthLocator()) ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(bymonthday=15)) ax.xaxis.set_major_formatter(plt.NullFormatter()) ax.xaxis.set_minor_formatter(mpl.dates.DateFormatter(&#39;%h&#39;)) # Add text ax.text(&#39;2020-01-01&#39;, 3900, &quot;New year&#39;s day&quot;, ha=&#39;left&#39;) ax.text(&#39;2020-07-04&#39;, 4200, &#39;Independence day&#39;, ha=&#39;center&#39;) ax.text(&#39;2020-12-25&#39;, 3700, &#39;Christmas&#39;, ha=&#39;right&#39;); . Seatle cycling data . From JVDP&#39;s blog . file = &#39;https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD&#39; data = pd.read_csv(file, skiprows=1, names=[&#39;date&#39;, &#39;total&#39;, &#39;east&#39;, &#39;west&#39;], parse_dates=True, index_col=&#39;date&#39;) . data.describe() . total east west . count 67118.000000 | 67118.000000 | 67118.000000 | . mean 112.912527 | 51.559835 | 61.352692 | . std 144.160880 | 66.522811 | 89.768937 | . min 0.000000 | 0.000000 | 0.000000 | . 25% 14.000000 | 6.000000 | 7.000000 | . 50% 60.000000 | 28.000000 | 30.000000 | . 75% 147.000000 | 69.000000 | 74.000000 | . max 1097.000000 | 698.000000 | 850.000000 | . data.plot(); . weekly = data.resample(&#39;W&#39;).sum() weekly.plot(style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . daily = data.resample(&#39;D&#39;).sum() daily.rolling(window=30, center=True).sum().plot(style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . by_time = data.groupby(data.index.time).sum() hourly_ticks = 3 * 60 * 60 * np.arange(8) by_time.plot(xticks=hourly_ticks, style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . by_weekday = data.groupby(data.index.dayofweek).sum() by_weekday.index = [&#39;Mo&#39;, &#39;Tu&#39;, &#39;We&#39;, &#39;Th&#39;, &#39;Fr&#39;, &#39;Sa&#39;, &#39;So&#39;] by_weekday.plot(style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . mpl.get_configdir() . &#39;/Users/fgu/.matplotlib&#39; . wknd = np.where(data.index.dayofweek &gt; 4, &#39;weekend&#39;, &#39;weekday&#39;) hourly = data.groupby([wknd, data.index.time]).sum() fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 5)) hourly.loc[&#39;weekday&#39;].plot(ax=ax1, title=&#39;Weekdays&#39;, xticks=hourly_ticks, style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]) hourly.loc[&#39;weekend&#39;].plot(ax=ax0, title=&#39;Weekends&#39;, xticks=hourly_ticks, style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . USA.gov data from Bitly . From Python for Data Analysis . import pandas as pd import numpy as np import seaborn as sns sns.set() %config InlineBackend.figure_format =&#39;retina&#39; . path = &#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/bitly_usagov/example.txt&#39; df = pd.read_json(path, lines=True) df.head() . a c nk tz gr g h l al hh r u t hc cy ll _heartbeat_ kw . 0 Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... | US | 1.0 | America/New_York | MA | A6qOVH | wfLQtf | orofrog | en-US,en;q=0.8 | 1.usa.gov | http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/... | http://www.ncbi.nlm.nih.gov/pubmed/22415991 | 1.331923e+09 | 1.331823e+09 | Danvers | [42.576698, -70.954903] | NaN | NaN | . 1 GoogleMaps/RochesterNY | US | 0.0 | America/Denver | UT | mwszkS | mwszkS | bitly | NaN | j.mp | http://www.AwareMap.com/ | http://www.monroecounty.gov/etc/911/rss.php | 1.331923e+09 | 1.308262e+09 | Provo | [40.218102, -111.613297] | NaN | NaN | . 2 Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ... | US | 1.0 | America/New_York | DC | xxr3Qb | xxr3Qb | bitly | en-US | 1.usa.gov | http://t.co/03elZC4Q | http://boxer.senate.gov/en/press/releases/0316... | 1.331923e+09 | 1.331920e+09 | Washington | [38.9007, -77.043098] | NaN | NaN | . 3 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8)... | BR | 0.0 | America/Sao_Paulo | 27 | zCaLwp | zUtuOu | alelex88 | pt-br | 1.usa.gov | direct | http://apod.nasa.gov/apod/ap120312.html | 1.331923e+09 | 1.331923e+09 | Braz | [-23.549999, -46.616699] | NaN | NaN | . 4 Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... | US | 0.0 | America/New_York | MA | 9b6kNl | 9b6kNl | bitly | en-US,en;q=0.8 | bit.ly | http://www.shrewsbury-ma.gov/selco/ | http://www.shrewsbury-ma.gov/egov/gallery/1341... | 1.331923e+09 | 1.273672e+09 | Shrewsbury | [42.286499, -71.714699] | NaN | NaN | . Let&#39;s plot the most occuring time-zones. . counts = df.tz.str.replace(&#39;^$&#39;, &#39;Unknown&#39;).fillna(&#39;Missing&#39;).value_counts()[:10] sns.barplot(counts.values, counts.index); . Now, let&#39;s split the bars by operating system. . pd.Series.reverse = lambda self: self[::-1] # Cool trick from here: https://stackoverflow.com/a/46624694 df[&#39;os&#39;] = np.where(df.a.str.contains(&#39;Mac&#39;), &#39;Mac&#39;, &#39;Not Mac&#39;) agg_counts = (df.replace(&#39;^$&#39;, &#39;Unknown&#39;, regex=True) .groupby([&#39;tz&#39;, &#39;os&#39;]) .size() .unstack() .fillna(0)) indexer = agg_counts.sum(1).argsort() data = agg_counts.take(indexer[-10:]).reverse().stack() data.name = &#39;totals&#39; data = data.reset_index() sns.barplot(x=&#39;totals&#39;, y=&#39;tz&#39;, hue=&#39;os&#39;, data=data); . MovieLens 1M dataset . From Python for Data Analysis . !ls data/ml-1m . README movies.dat ratings.dat users.dat . import pandas as pd . path = &#39;data/ml-1m/&#39; unames = [&#39;user_id&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;occupation&#39;, &#39;zip&#39;] users = pd.read_table(path + &#39;users.dat&#39;, sep=&#39;::&#39;, engine=&#39;python&#39;, names=unames) rnames = [&#39;user_id&#39;, &#39;movie_id&#39;, &#39;rating&#39;, &#39;timestamp&#39;] ratings = pd.read_table(path + &#39;ratings.dat&#39;, sep=&#39;::&#39;, engine=&#39;python&#39;, names=rnames) mnames = [&#39;movie_id&#39;, &#39;title&#39;, &#39;genres&#39;] movies = pd.read_table(path + &#39;movies.dat&#39;, sep=&#39;::&#39;, engine=&#39;python&#39;, names=mnames) data = pd.merge(pd.merge(users, ratings), movies) data.head() . user_id gender age occupation zip movie_id rating timestamp title genres . 0 1 | F | 1 | 10 | 48067 | 1193 | 5 | 978300760 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 1 2 | M | 56 | 16 | 70072 | 1193 | 5 | 978298413 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 2 12 | M | 25 | 12 | 32793 | 1193 | 4 | 978220179 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 3 15 | M | 25 | 7 | 22903 | 1193 | 4 | 978199279 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 4 17 | M | 50 | 1 | 95350 | 1193 | 5 | 978158471 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . Calculate average ratings by gender . mean_ratings = data.pivot_table(values=&#39;rating&#39;, index=&#39;title&#39;, columns=&#39;gender&#39;, aggfunc=&#39;mean&#39;) mean_ratings.head() . gender F M . title . $1,000,000 Duck (1971) 3.375000 | 2.761905 | . &#39;Night Mother (1986) 3.388889 | 3.352941 | . &#39;Til There Was You (1997) 2.675676 | 2.733333 | . &#39;burbs, The (1989) 2.793478 | 2.962085 | . ...And Justice for All (1979) 3.828571 | 3.689024 | . Keep only movies with at least 200 ratings . ratings_count = data.groupby(&#39;title&#39;).size() active_titles = ratings_count[ratings_count &gt; 200].index mean_ratings = mean_ratings.loc[active_titles] # mean_ratings = mean_ratings.reindex(active_titles) # alternative . Above was mainly to practice, what I actually want is to exclude movies with fewer than 200 ratings from the very start . rating_count = data.groupby(&#39;title&#39;).size() active_movies = rating_count[rating_count &gt; 200].index data = data[data.title.isin(active_movies)] data.head() . user_id gender age occupation zip movie_id rating timestamp title genres . 0 1 | F | 1 | 10 | 48067 | 1193 | 5 | 978300760 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 1 2 | M | 56 | 16 | 70072 | 1193 | 5 | 978298413 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 2 12 | M | 25 | 12 | 32793 | 1193 | 4 | 978220179 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 3 15 | M | 25 | 7 | 22903 | 1193 | 4 | 978199279 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 4 17 | M | 50 | 1 | 95350 | 1193 | 5 | 978158471 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . Calculate ratings difference by gender (again, just for fun, and to compare to above result) . mean_ratings2 = data.pivot_table(&#39;rating&#39;, &#39;title&#39;, &#39;gender&#39;, &#39;mean&#39;) all(mean_ratings2 == mean_ratings) . True . Look at top movis by gender . mean_ratings.sort_values(&#39;F&#39;, ascending=False).head() . gender F M . title . Close Shave, A (1995) 4.644444 | 4.473795 | . Wrong Trousers, The (1993) 4.588235 | 4.478261 | . General, The (1927) 4.575758 | 4.329480 | . Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) 4.572650 | 4.464589 | . Wallace &amp; Gromit: The Best of Aardman Animation (1996) 4.563107 | 4.385075 | . mean_ratings.sort_values(&#39;M&#39;, ascending=False).head() . gender F M . title . Godfather, The (1972) 4.314700 | 4.583333 | . Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954) 4.481132 | 4.576628 | . Shawshank Redemption, The (1994) 4.539075 | 4.560625 | . Raiders of the Lost Ark (1981) 4.332168 | 4.520597 | . Usual Suspects, The (1995) 4.513317 | 4.518248 | . Calculate rating differences . mean_ratings[&#39;diff&#39;] = np.abs(mean_ratings[&#39;F&#39;] - mean_ratings[&#39;M&#39;]) mean_ratings.sort_values(&#39;diff&#39;, ascending=False).head() . gender F M diff . title . Dirty Dancing (1987) 3.790378 | 2.959596 | 0.830782 | . Good, The Bad and The Ugly, The (1966) 3.494949 | 4.221300 | 0.726351 | . To Wong Foo, Thanks for Everything! Julie Newmar (1995) 3.486842 | 2.795276 | 0.691567 | . Kentucky Fried Movie, The (1977) 2.878788 | 3.555147 | 0.676359 | . Jumpin&#39; Jack Flash (1986) 3.254717 | 2.578358 | 0.676359 | . Find movies with the most rating disagreement among all viwers . data.groupby(&#39;title&#39;).rating.std().sort_values(ascending=False).head() . title Plan 9 from Outer Space (1958) 1.455998 Texas Chainsaw Massacre, The (1974) 1.332448 Dumb &amp; Dumber (1994) 1.321333 Blair Witch Project, The (1999) 1.316368 Natural Born Killers (1994) 1.307198 Name: rating, dtype: float64 . Baby names . From Python for Data Analysis . !head data/names/yob1880.txt . Mary,F,7065 Anna,F,2604 Emma,F,2003 Elizabeth,F,1939 Minnie,F,1746 Margaret,F,1578 Ida,F,1472 Alice,F,1414 Bertha,F,1320 Sarah,F,1288 . import re files = !ls data/names/yob* pieces = [] columns = [&#39;name&#39;, &#39;sex&#39;, &#39;births&#39;] for file in files: frame = pd.read_csv(file, names=columns) year = int(re.findall(&#39; d+&#39;, file)[0]) frame[&#39;year&#39;] = year pieces.append(frame) names = pd.concat(pieces, ignore_index=True) names.head() . name sex births year . 0 Mary | F | 7065 | 1880 | . 1 Anna | F | 2604 | 1880 | . 2 Emma | F | 2003 | 1880 | . 3 Elizabeth | F | 1939 | 1880 | . 4 Minnie | F | 1746 | 1880 | . years = range(1880, 2019) pieces = [] columns = [&#39;name&#39;, &#39;gender&#39;, &#39;births&#39;] for year in years: path = &#39;data/names/yob%d.txt&#39; % year frame = pd.read_csv(path, names=columns) frame[&#39;year&#39;] = year pieces.append(frame) names = pd.concat(pieces, ignore_index=True) names.head() . name gender births year . 0 Mary | F | 7065 | 1880 | . 1 Anna | F | 2604 | 1880 | . 2 Emma | F | 2003 | 1880 | . 3 Elizabeth | F | 1939 | 1880 | . 4 Minnie | F | 1746 | 1880 | . Plot number of girls and boys born over time . names.pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;gender&#39;, &#39;sum&#39;).plot(); . Add a proportion column . def add_prop(group): group[&#39;prop&#39;] = group.births / group.births.sum() return group names = names.groupby([&#39;gender&#39;, &#39;year&#39;]).apply(add_prop) names.head() . name gender births year prop . 0 Mary | F | 7065 | 1880 | 0.077642 | . 1 Anna | F | 2604 | 1880 | 0.028617 | . 2 Emma | F | 2003 | 1880 | 0.022012 | . 3 Elizabeth | F | 1939 | 1880 | 0.021309 | . 4 Minnie | F | 1746 | 1880 | 0.019188 | . Check that prop sums to 1 for each year-gender group . names.groupby([&#39;gender&#39;, &#39;year&#39;]).prop.sum() . gender year F 1880 1.0 1881 1.0 1882 1.0 1883 1.0 1884 1.0 ... M 2014 1.0 2015 1.0 2016 1.0 2017 1.0 2018 1.0 Name: prop, Length: 278, dtype: float64 . Keep only top 1000 names per gender and year . def top1000(group): return group.sort_values(by=&#39;births&#39;, ascending=False)[:1000] top1000 = names.groupby([&#39;gender&#39;, &#39;year&#39;]).apply(top1000).reset_index(drop=True) . Let&#39;s look at the number of births per year for common names . subset = [&#39;John&#39;, &#39;Harry&#39;, &#39;Mary&#39;, &#39;Marilyn&#39;] (names.pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;name&#39;, &#39;sum&#39;)[subset] .plot(subplots=True, figsize=(12,10), title=&#39;Number of births per year&#39;)); . Plot suggest that common names have become less popular. This could be either because people use other names instead, or becasue people just use more names overall. Let&#39;s look into this. First by looking at the proportion of birhts for the top 1000 names. . (top1000.pivot_table(&#39;prop&#39;, &#39;year&#39;, &#39;gender&#39;, &#39;sum&#39;) .plot(title=&#39;Proportion of top 1000 names of all births&#39;, figsize=(6, 5), yticks=np.linspace(0, 1.2, 13))); . It&#39;s clear from the above plot that the top 1000 names are becoming a smaller proportion of all names over time, indicating that naming diversity is increasing. To corroborate this, let&#39;s look at the number of names that account for 50 percent of all births in each year for each sex. . boys2018 = top1000[(top1000.year == 2018) &amp; (top1000.gender == &#39;M&#39;)] boys2018.sort_values(&#39;prop&#39;, ascending=False).prop.cumsum().searchsorted(.5) + 1 . 149 . def get_quantile_count(group, q=0.5): group = group.sort_values(&#39;prop&#39;, ascending=False) return group.prop.cumsum().searchsorted(.5) + 1 diversity = top1000.groupby([&#39;gender&#39;, &#39;year&#39;]).apply(get_quantile_count).unstack(level=0) diversity.plot(figsize=(8, 6)); . Explore the last-letter revolution . import matplotlib.pyplot as plt def get_last_letter(name): return name[-1] names[&#39;last_letter&#39;] = names.name.map(get_last_letter) table = names.pivot_table(&#39;births&#39;, &#39;last_letter&#39;, [&#39;sex&#39;, &#39;year&#39;], &#39;sum&#39;) subtable = table.reindex(columns=[1960, 1990, 2018], level=&#39;year&#39;) subtable = subtable / subtable.sum() subtable fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,10)) subtable[&#39;M&#39;].plot(kind=&#39;bar&#39;, ax=ax1) subtable[&#39;F&#39;].plot(kind=&#39;bar&#39;, ax=ax2); . For boys names, d, n, and y have changed markedly in popularity over the past six decads. Let&#39;s look at this more closely. . table[&#39;M&#39;].reindex([&#39;d&#39;, &#39;n&#39;, &#39;y&#39;]).T.plot(figsize=(8, 6), linewidth=5); . Leslie-like names have evolved from being boy to being girl names . def normalise(df): return df.div(df.sum(1), axis=&#39;rows&#39;) (names[names.name.str.lower().str.contains(&#39;^lesl&#39;)] .pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;sex&#39;, &#39;sum&#39;) .pipe(normalise) .plot(figsize=(8, 6), linewidth=5)); . Evolution of Molly and Fabian . names . name sex births year last_letter . 0 Mary | F | 7065 | 1880 | y | . 1 Anna | F | 2604 | 1880 | a | . 2 Emma | F | 2003 | 1880 | a | . 3 Elizabeth | F | 1939 | 1880 | h | . 4 Minnie | F | 1746 | 1880 | e | . ... ... | ... | ... | ... | ... | . 1957041 Zylas | M | 5 | 2018 | s | . 1957042 Zyran | M | 5 | 2018 | n | . 1957043 Zyrie | M | 5 | 2018 | e | . 1957044 Zyron | M | 5 | 2018 | n | . 1957045 Zzyzx | M | 5 | 2018 | x | . 1957046 rows × 5 columns . (names[names.name.isin([&#39;Molly&#39;, &#39;Fabian&#39;])] .pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;name&#39;, &#39;sum&#39;) .plot(figsize=(10, 8), linewidth=8)); . Baby names in Switzerland . ls data/ch-names/ . f.xlsx m.xlsx . genders = [&#39;f&#39;, &#39;m&#39;] def rename_cols(df, names): df.columns = names return df pieces = [] columns = [&#39;name&#39;, &#39;year&#39;, &#39;births&#39;, &#39;rank&#39;] for gender in genders: path = &#39;data/ch-names/%s.xlsx&#39; % gender data = (pd.read_excel(path, header=[2, 3], index_col=0, skipfooter=5) .stack(level=0) .reset_index() .pipe(rename_cols, columns) .assign(gender=gender)) pieces.append(data) names = pd.concat(pieces, ignore_index=True) names . name year births rank gender . 0 Emma | 1998 | 88 | 78 | f | . 1 Emma | 1999 | 80 | 85 | f | . 2 Emma | 2000 | 127 | 46 | f | . 3 Emma | 2001 | 116 | 46 | f | . 4 Emma | 2002 | 147 | 36 | f | . ... ... | ... | ... | ... | ... | . 41995 Rúben | 2014 | 5 | 1071 | m | . 41996 Rúben | 2015 | 4 | 1263 | m | . 41997 Rúben | 2016 | 6 | 972 | m | . 41998 Rúben | 2017 | 15 | 515 | m | . 41999 Rúben | 2018 | 7 | 924 | m | . 42000 rows × 5 columns . files = !ls data/ch-names/* pieces = [] columns = [&#39;name&#39;, &#39;year&#39;, &#39;births&#39;] for file in files: frame = (pd.read_excel(file, header=[2, 3], index_col=0, skipfooter=5) .stack(level=[0]).reset_index().drop(&#39;Rang&#39;, axis=1)) frame.columns = columns gender = file[-6] frame[&#39;gender&#39;] = gender pieces.append(frame) table = pd.concat(pieces, ignore_index=True) table.head() . name year births gender . 0 Emma | 1998 | 88 | f | . 1 Emma | 1999 | 80 | f | . 2 Emma | 2000 | 127 | f | . 3 Emma | 2001 | 116 | f | . 4 Emma | 2002 | 147 | f | . Add proportions and rank columns . def calc_prop(group): group[&#39;prop&#39;] = group.births / group.births.sum() * 100 return group table = table.groupby([&#39;year&#39;, &#39;gender&#39;]).apply(calc_prop) table[&#39;rank&#39;] = table.groupby([&#39;year&#39;, &#39;gender&#39;]).births.rank(method=&#39;min&#39;) . Most popular names by year . def top_n(group, n=5): return group.sort_values(&#39;births&#39;, ascending=False)[:n] num_names = 10 years = [1998] (table.groupby([&#39;year&#39;, &#39;gender&#39;]) .apply(top_n, num_names) .drop([&#39;year&#39;, &#39;gender&#39;], axis=1) .reset_index(level=2, drop=True) .loc[years]) . name births prop rank . year gender . 1998 m Luca | 648 | 2.214173 | 1000.0 | . m David | 528 | 1.804141 | 999.0 | . m Simon | 511 | 1.746053 | 998.0 | . m Marco | 435 | 1.486366 | 997.0 | . m Joel | 424 | 1.448780 | 996.0 | . m Michael | 407 | 1.390692 | 995.0 | . m Lukas | 392 | 1.339438 | 994.0 | . m Nicolas | 375 | 1.281350 | 993.0 | . m Fabian | 368 | 1.257432 | 992.0 | . m Kevin | 350 | 1.195927 | 991.0 | . w Laura | 607 | 2.405675 | 1000.0 | . w Celine | 414 | 1.640774 | 999.0 | . w Sarah | 407 | 1.613031 | 998.0 | . w Jessica | 391 | 1.549620 | 997.0 | . w Lea | 375 | 1.486208 | 996.0 | . w Michelle | 357 | 1.414870 | 995.0 | . w Sara | 344 | 1.363348 | 994.0 | . w Vanessa | 320 | 1.268231 | 993.0 | . w Lara | 308 | 1.220672 | 992.0 | . w Julia | 299 | 1.185003 | 991.0 | . What&#39;s going on with girl names ending in &#39;a&#39;? . table[table.name.str.endswith(&#39;a&#39;)] . 0 False 1 False 2 False 3 False 4 False ... 41995 False 41996 False 41997 False 41998 False 41999 False Name: name, Length: 42000, dtype: bool . girl_names = [&#39;Emma&#39;, &#39;Ivy&#39;, &#39;Audrey&#39;, &#39;Vivien&#39;] . def get_last_letter(name): return name[-1] table[&#39;last_letter&#39;] = table.name.map(get_last_letter) last_letters = table[table.gender == &#39;w&#39;].pivot_table(&#39;births&#39;, &#39;last_letter&#39;, &#39;year&#39;, &#39;sum&#39;) last_letters = last_letters / last_letters.sum() last_letters.T[&#39;a&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a32573f50&gt; . Popularity of names over time . import matplotlib.pyplot as plt def plot_births(names, axis, values=&#39;prop&#39;): (table[(table.name.isin(names))] .pivot_table(values, &#39;year&#39;, &#39;name&#39;, &#39;sum&#39;) .plot(ax=axis, xticks=range(1998, 2019), rot=45, linewidth=7)) fig, (left, right) = plt.subplots(1, 2, figsize=(20, 8)) girl_names = [&#39;Emma&#39;] boy_names = [&#39;Liam&#39;, &#39;Leo&#39;, &#39;Theo&#39;, &#39;Fabian&#39;] plot_births(girl_names, left, values=&#39;births&#39;) plot_births(boy_names, right, values=&#39;births&#39;) . USDA Food database . From Python for Data Analysis . ls data/usda_foods.json . chap14-examples.ipynb data/ . import json db = json.load(open(&#39;data/usda_foods.json&#39;)) len(db) . db[0].keys() . dict_keys([&#39;id&#39;, &#39;description&#39;, &#39;tags&#39;, &#39;manufacturer&#39;, &#39;group&#39;, &#39;portions&#39;, &#39;nutrients&#39;]) . pd.DataFrame(db[0][&#39;nutrients&#39;]) . value units description group . 0 25.180 | g | Protein | Composition | . 1 29.200 | g | Total lipid (fat) | Composition | . 2 3.060 | g | Carbohydrate, by difference | Composition | . 3 3.280 | g | Ash | Other | . 4 376.000 | kcal | Energy | Energy | . ... ... | ... | ... | ... | . 157 1.472 | g | Serine | Amino Acids | . 158 93.000 | mg | Cholesterol | Other | . 159 18.584 | g | Fatty acids, total saturated | Other | . 160 8.275 | g | Fatty acids, total monounsaturated | Other | . 161 0.830 | g | Fatty acids, total polyunsaturated | Other | . 162 rows × 4 columns . Produce df with info variables . info_keys = [&#39;description&#39;, &#39;id&#39;, &#39;manufacturer&#39;, &#39;group&#39;] new_col_names = {&#39;description&#39;: &#39;food&#39;, &#39;group&#39;: &#39;fgroup&#39;} info = pd.DataFrame(db, columns=info_keys) info = info.rename(columns=new_col_names) info.head() . food id manufacturer fgroup . 0 Cheese, caraway | 1008 | | Dairy and Egg Products | . 1 Cheese, cheddar | 1009 | | Dairy and Egg Products | . 2 Cheese, edam | 1018 | | Dairy and Egg Products | . 3 Cheese, feta | 1019 | | Dairy and Egg Products | . 4 Cheese, mozzarella, part skim milk | 1028 | | Dairy and Egg Products | . Create a df with all the nutrient info for each food . new_col_names = {&#39;description&#39;: &#39;nutrient&#39;, &#39;group&#39;: &#39;ngroup&#39;} pieces = [] for rec in db: nuts = pd.DataFrame(rec[&#39;nutrients&#39;]) nuts[&#39;id&#39;] = rec[&#39;id&#39;] pieces.append(nuts) nutrients = pd.concat(pieces, ignore_index=True) nutrients = nutrients.rename(columns=new_col_names) nutrients.head() . value units nutrient ngroup id . 0 25.18 | g | Protein | Composition | 1008 | . 1 29.20 | g | Total lipid (fat) | Composition | 1008 | . 2 3.06 | g | Carbohydrate, by difference | Composition | 1008 | . 3 3.28 | g | Ash | Other | 1008 | . 4 376.00 | kcal | Energy | Energy | 1008 | . Combine info and nutrient dfs . foods = pd.merge(info, nutrients).drop_duplicates() foods.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 375176 entries, 0 to 389354 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 food 375176 non-null object 1 id 375176 non-null int64 2 manufacturer 293054 non-null object 3 fgroup 375176 non-null object 4 value 375176 non-null float64 5 units 375176 non-null object 6 nutrient 375176 non-null object 7 ngroup 375176 non-null object dtypes: float64(1), int64(1), object(6) memory usage: 25.8+ MB . Plot nutrient content by food group . nutrient = &#39;Carbohydrate, by difference&#39; (foods[foods.nutrient.isin([nutrient])] .groupby(&#39;fgroup&#39;) .value .quantile(.5) .sort_values() .plot(kind=&#39;barh&#39;, figsize=(8, 6)) .set(xlabel=&#39;Median %s content&#39; % nutrient, ylabel=&#39;&#39;)); . Find the food with the maxium nutritional content for each nutrient . get_max = lambda x: x.loc[x.value.idxmax()] foods.groupby(&#39;nutrient&#39;).apply(get_max).head() . food id manufacturer fgroup value units nutrient ngroup . nutrient . Adjusted Protein Baking chocolate, unsweetened, squares | 19078 | | Sweets | 12.900 | g | Adjusted Protein | Composition | . Alanine Gelatins, dry powder, unsweetened | 19177 | | Sweets | 8.009 | g | Alanine | Amino Acids | . Alcohol, ethyl Alcoholic beverage, distilled, all (gin, rum, ... | 14533 | | Beverages | 42.500 | g | Alcohol, ethyl | Other | . Arginine Seeds, sesame flour, low-fat | 12033 | | Nut and Seed Products | 7.436 | g | Arginine | Amino Acids | . Ash Desserts, rennin, tablets, unsweetened | 19225 | | Sweets | 72.500 | g | Ash | Other | . FEC 2012 presidential election campaign contributions . From Python for Data Analysis . columns = {&#39;cand_nm&#39;:&#39;candidate&#39;, &#39;contbr_city&#39;:&#39;city&#39;, &#39;contbr_occupation&#39;:&#39;occupation&#39;, &#39;contb_receipt_amt&#39;:&#39;amount&#39;, &#39;contb_receipt_dt&#39;:&#39;date&#39;} parties = {&#39;Bachmann, Michelle&#39;: &#39;r&#39;, &#39;Romney, Mitt&#39;: &#39;r&#39;, &#39;Obama, Barack&#39;: &#39;d&#39;, &quot;Roemer, Charles E. &#39;Buddy&#39; III&quot;: &#39;r&#39;, &#39;Pawlenty, Timothy&#39;: &#39;r&#39;, &#39;Johnson, Gary Earl&#39;: &#39;r&#39;, &#39;Paul, Ron&#39;: &#39;r&#39;, &#39;Santorum, Rick&#39;: &#39;r&#39;, &#39;Cain, Herman&#39;: &#39;r&#39;, &#39;Gingrich, Newt&#39;: &#39;r&#39;, &#39;McCotter, Thaddeus G&#39;: &#39;r&#39;, &#39;Huntsman, Jon&#39;: &#39;r&#39;, &#39;Perry, Rick&#39;: &#39;r&#39;} path = &#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/fec/P00000001-ALL.csv&#39; fec = (pd.read_csv(path) [columns.keys()] .rename(columns=columns) .assign(party = lambda df: df.candidate.map(parties))) . /Users/fgu/miniconda3/envs/basics/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . fec.head() . candidate city occupation amount date party . 0 Bachmann, Michelle | MOBILE | RETIRED | 250.0 | 20-JUN-11 | r | . 1 Bachmann, Michelle | MOBILE | RETIRED | 50.0 | 23-JUN-11 | r | . 2 Bachmann, Michelle | LANETT | INFORMATION REQUESTED | 250.0 | 05-JUL-11 | r | . 3 Bachmann, Michelle | PIGGOTT | RETIRED | 250.0 | 01-AUG-11 | r | . 4 Bachmann, Michelle | HOT SPRINGS NATION | RETIRED | 300.0 | 20-JUN-11 | r | . Compare total donations . fec.groupby(&#39;party&#39;).amount.sum() . party d 1.335026e+08 r 1.652488e+08 Name: amount, dtype: float64 . Compare donations by occupation . occ_mapping = {&#39;INFORMATION REQUESTED&#39;:&#39;Not provided&#39;, &#39;INFORMATION REQUESTED PER BEST EFFORTS&#39;: &#39;Not provided&#39;} f = lambda x: occ_mapping.get(x, x) fec.occupation = fec.occupation.map(f) fec . candidate city occupation amount date party . 0 Bachmann, Michelle | MOBILE | RETIRED | 250.0 | 20-JUN-11 | r | . 1 Bachmann, Michelle | MOBILE | RETIRED | 50.0 | 23-JUN-11 | r | . 2 Bachmann, Michelle | LANETT | Not provided | 250.0 | 05-JUL-11 | r | . 3 Bachmann, Michelle | PIGGOTT | RETIRED | 250.0 | 01-AUG-11 | r | . 4 Bachmann, Michelle | HOT SPRINGS NATION | RETIRED | 300.0 | 20-JUN-11 | r | . ... ... | ... | ... | ... | ... | ... | . 1001726 Perry, Rick | INFO REQUESTED | Not provided | 5000.0 | 29-SEP-11 | r | . 1001727 Perry, Rick | INFO REQUESTED | BUSINESS OWNER | 2500.0 | 30-SEP-11 | r | . 1001728 Perry, Rick | INFO REQUESTED | Not provided | 500.0 | 29-SEP-11 | r | . 1001729 Perry, Rick | INFO REQUESTED | LONGWALL MAINTENANCE FOREMAN | 500.0 | 30-SEP-11 | r | . 1001730 Perry, Rick | INFO REQUESTED | Not provided | 2500.0 | 31-AUG-11 | r | . 1001731 rows × 6 columns . Discretise donations into buckets for contribution size . Seaborn . From Python Data Science Handbook . Create a simple random walk plot using default . rng = np.random.RandomState(2312) x = np.linspace(0, 10, 500) y = np.cumsum(rng.randn(500, 6), axis=0) plt.plot(x, y) plt.legend(&#39;abcdef&#39;, ncol=2, loc=&#39;lower left&#39;); . import seaborn as sns sns.set() plt.plot(x, y); plt.legend(&#39;abcdef&#39;, ncol=2, loc=&#39;lower left&#39;); . Explore marathon data . from datetime import timedelta # Read data def convert_time(s): h, m, s = map(int, s.split(&#39;:&#39;)) return timedelta(hours=h, minutes=m, seconds=s) file = &#39;https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv&#39; df = pd.read_csv(file, converters={&#39;split&#39;: convert_time, &#39;final&#39;: convert_time}) # Add times in seconds df[&#39;split_sec&#39;] = df.split.astype(int) / 1E9 df[&#39;final_sec&#39;] = df.final.astype(int) / 1E9 print(df.shape) df.head() . (37250, 6) . age gender split final split_sec final_sec . 0 33 | M | 01:05:38 | 02:08:51 | 3938.0 | 7731.0 | . 1 32 | M | 01:06:26 | 02:09:28 | 3986.0 | 7768.0 | . 2 31 | M | 01:06:49 | 02:10:42 | 4009.0 | 7842.0 | . 3 38 | M | 01:06:16 | 02:13:45 | 3976.0 | 8025.0 | . 4 31 | M | 01:06:32 | 02:13:59 | 3992.0 | 8039.0 | . with sns.axes_style(&#39;white&#39;): g = sns.jointplot(&#39;split_sec&#39;, &#39;final_sec&#39;, data=df, kind=&#39;hex&#39;) g.ax_joint.plot(np.linspace(4000, 17000), np.linspace(8000, 33000), &#39;:&#39;) . df[&#39;split_frac&#39;] = 1 - 2 * df.split_sec / df.final_sec sns.distplot(df.split_frac, kde=False) plt.axvline(0, linestyle=&#39;--&#39;); . g = sns.PairGrid(df, hue=&#39;gender&#39;, vars=[&#39;age&#39;, &#39;split_frac&#39;, &#39;final_sec&#39;, &#39;split_sec&#39;]) g.map(sns.scatterplot, alpha=0.5) g.add_legend(); . sns.kdeplot(df.split_frac[df.gender==&#39;M&#39;], label=&#39;Men&#39;, shade=True) sns.kdeplot(df.split_frac[df.gender==&#39;W&#39;], label=&#39;Women&#39;, shade=True); . sns.violinplot(&#39;gender&#39;, &#39;split_frac&#39;, data=df); . df[&#39;age_dec&#39;] = df.age.map(lambda age: age // 10 * 10) sns.violinplot(&#39;age_dec&#39;, &#39;split_frac&#39;, hue=&#39;gender&#39;, split=True, data=df); . g = sns.lmplot(&#39;final_sec&#39;, &#39;split_frac&#39;, col=&#39;gender&#39;, data=df, markers=&#39;.&#39;, scatter_kws=dict(color=&#39;cornflowerblue&#39;)) g.map(plt.axhline, y=0.1, color=&#39;k&#39;, ls=&#39;:&#39;); . Sources . Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/dataviz/2020/04/22/dataviz.html",
            "relUrl": "/python/pandas/dataviz/2020/04/22/dataviz.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post90": {
            "title": "TED talks",
            "content": "Analysing TED talks following [this] talk. . import pandas as pd . path = &#39;https://raw.githubusercontent.com/justmarkham/pycon-2019-tutorial/master/ted.csv&#39; ted = pd.read_csv(path) ted.head(2) . comments description duration event film_date languages main_speaker name num_speaker published_date ratings related_talks speaker_occupation tags title url views . 0 4553 | Sir Ken Robinson makes an entertaining and pro... | 1164 | TED2006 | 1140825600 | 60 | Ken Robinson | Ken Robinson: Do schools kill creativity? | 1 | 1151367060 | [{&#39;id&#39;: 7, &#39;name&#39;: &#39;Funny&#39;, &#39;count&#39;: 19645}, {... | [{&#39;id&#39;: 865, &#39;hero&#39;: &#39;https://pe.tedcdn.com/im... | Author/educator | [&#39;children&#39;, &#39;creativity&#39;, &#39;culture&#39;, &#39;dance&#39;,... | Do schools kill creativity? | https://www.ted.com/talks/ken_robinson_says_sc... | 47227110 | . 1 265 | With the same humor and humanity he exuded in ... | 977 | TED2006 | 1140825600 | 43 | Al Gore | Al Gore: Averting the climate crisis | 1 | 1151367060 | [{&#39;id&#39;: 7, &#39;name&#39;: &#39;Funny&#39;, &#39;count&#39;: 544}, {&#39;i... | [{&#39;id&#39;: 243, &#39;hero&#39;: &#39;https://pe.tedcdn.com/im... | Climate advocate | [&#39;alternative energy&#39;, &#39;cars&#39;, &#39;climate change... | Averting the climate crisis | https://www.ted.com/talks/al_gore_on_averting_... | 3200520 | . ted.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2550 entries, 0 to 2549 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 comments 2550 non-null int64 1 description 2550 non-null object 2 duration 2550 non-null int64 3 event 2550 non-null object 4 film_date 2550 non-null int64 5 languages 2550 non-null int64 6 main_speaker 2550 non-null object 7 name 2550 non-null object 8 num_speaker 2550 non-null int64 9 published_date 2550 non-null int64 10 ratings 2550 non-null object 11 related_talks 2550 non-null object 12 speaker_occupation 2544 non-null object 13 tags 2550 non-null object 14 title 2550 non-null object 15 url 2550 non-null object 16 views 2550 non-null int64 dtypes: int64(7), object(10) memory usage: 338.8+ KB . ted.isna().sum() . comments 0 description 0 duration 0 event 0 film_date 0 languages 0 main_speaker 0 name 0 num_speaker 0 published_date 0 ratings 0 related_talks 0 speaker_occupation 6 tags 0 title 0 url 0 views 0 dtype: int64 . Which talks provoke the most online discussion? . ted[&#39;views_per_comment&#39;] = ted.views / ted.comments vpc = ted.sort_values(&#39;views_per_comment&#39;)[[&#39;name&#39;, &#39;main_speaker&#39;, &#39;views_per_comment&#39;]] display(vpc.head()) display(vpc.tail()) . name main_speaker views_per_comment . 744 Diane J. Savino: The case for same-sex marriage | Diane J. Savino | 450.531587 | . 803 David Bismark: E-voting without fraud | David Bismark | 651.739808 | . 96 Richard Dawkins: Militant atheism | Richard Dawkins | 683.134291 | . 694 Sharmeen Obaid-Chinoy: Inside a school for sui... | Sharmeen Obaid-Chinoy | 703.886818 | . 954 Janet Echelman: Taking imagination seriously | Janet Echelman | 735.525682 | . name main_speaker views_per_comment . 2494 Jimmy Lin: A simple new blood test that can ca... | Jimmy Lin | 143643.714286 | . 2528 Chance Coughenour: How your pictures can help ... | Chance Coughenour | 179735.666667 | . 2542 Sethembile Msezane: Living sculptures that sta... | Sethembile Msezane | 180696.000000 | . 2501 Françoise Mouly: The stories behind The New Yo... | Françoise Mouly | 279680.000000 | . 2534 Benjamin Grant: What it feels like to see Eart... | Benjamin Grant | 323087.000000 | . Plot the distribution of comments . ted.comments.hist(); . ted[ted.comments &gt;= 1500].shape . (17, 19) . Most talks have fewer than 1500 comments (only 17 have more), so I drop the ones that have more. I initially tried to avoid this, but as Keving points out in the video: plotting entails decision making, as a plot is a summary of your data and not a representation of all your data. This is a good lesson to take away from this. . ted.loc[ted.comments &lt; 1000, &#39;comments&#39;].hist(bins=20, grid=False); . Plot the number of talks that took place each year . ted.published_date . 744 1282062180 803 1288685640 96 1176689220 694 1274865960 954 1307489760 ... 2494 1500994384 2528 1504209631 2542 1505488093 2501 1501770244 2534 1504814438 Name: published_date, Length: 2550, dtype: int64 . (pd.to_datetime(ted.film_date, unit=&#39;s&#39;).dt.year .value_counts() .sort_index() .plot(marker=&#39;o&#39;, ylabel=&#39;Number of events&#39;)); . What were the &quot;best&quot; events in TED history to attend . Defining &quot;best&quot; as event with most overall views, this is simple . ted.groupby(&#39;event&#39;).views.sum().sort_values(ascending=False)[:5] . event TED2013 177307937 TED2014 174121423 TEDGlobal 2013 170554736 TED2015 150826305 TED2006 147345533 Name: views, dtype: int64 . Unpack the ratings data . ted.ratings[0] . &#34;[{&#39;id&#39;: 7, &#39;name&#39;: &#39;Funny&#39;, &#39;count&#39;: 19645}, {&#39;id&#39;: 1, &#39;name&#39;: &#39;Beautiful&#39;, &#39;count&#39;: 4573}, {&#39;id&#39;: 9, &#39;name&#39;: &#39;Ingenious&#39;, &#39;count&#39;: 6073}, {&#39;id&#39;: 3, &#39;name&#39;: &#39;Courageous&#39;, &#39;count&#39;: 3253}, {&#39;id&#39;: 11, &#39;name&#39;: &#39;Longwinded&#39;, &#39;count&#39;: 387}, {&#39;id&#39;: 2, &#39;name&#39;: &#39;Confusing&#39;, &#39;count&#39;: 242}, {&#39;id&#39;: 8, &#39;name&#39;: &#39;Informative&#39;, &#39;count&#39;: 7346}, {&#39;id&#39;: 22, &#39;name&#39;: &#39;Fascinating&#39;, &#39;count&#39;: 10581}, {&#39;id&#39;: 21, &#39;name&#39;: &#39;Unconvincing&#39;, &#39;count&#39;: 300}, {&#39;id&#39;: 24, &#39;name&#39;: &#39;Persuasive&#39;, &#39;count&#39;: 10704}, {&#39;id&#39;: 23, &#39;name&#39;: &#39;Jaw-dropping&#39;, &#39;count&#39;: 4439}, {&#39;id&#39;: 25, &#39;name&#39;: &#39;OK&#39;, &#39;count&#39;: 1174}, {&#39;id&#39;: 26, &#39;name&#39;: &#39;Obnoxious&#39;, &#39;count&#39;: 209}, {&#39;id&#39;: 10, &#39;name&#39;: &#39;Inspiring&#39;, &#39;count&#39;: 24924}]&#34; . import ast ast.literal_eval(ted.ratings[0]) . [{&#39;id&#39;: 7, &#39;name&#39;: &#39;Funny&#39;, &#39;count&#39;: 19645}, {&#39;id&#39;: 1, &#39;name&#39;: &#39;Beautiful&#39;, &#39;count&#39;: 4573}, {&#39;id&#39;: 9, &#39;name&#39;: &#39;Ingenious&#39;, &#39;count&#39;: 6073}, {&#39;id&#39;: 3, &#39;name&#39;: &#39;Courageous&#39;, &#39;count&#39;: 3253}, {&#39;id&#39;: 11, &#39;name&#39;: &#39;Longwinded&#39;, &#39;count&#39;: 387}, {&#39;id&#39;: 2, &#39;name&#39;: &#39;Confusing&#39;, &#39;count&#39;: 242}, {&#39;id&#39;: 8, &#39;name&#39;: &#39;Informative&#39;, &#39;count&#39;: 7346}, {&#39;id&#39;: 22, &#39;name&#39;: &#39;Fascinating&#39;, &#39;count&#39;: 10581}, {&#39;id&#39;: 21, &#39;name&#39;: &#39;Unconvincing&#39;, &#39;count&#39;: 300}, {&#39;id&#39;: 24, &#39;name&#39;: &#39;Persuasive&#39;, &#39;count&#39;: 10704}, {&#39;id&#39;: 23, &#39;name&#39;: &#39;Jaw-dropping&#39;, &#39;count&#39;: 4439}, {&#39;id&#39;: 25, &#39;name&#39;: &#39;OK&#39;, &#39;count&#39;: 1174}, {&#39;id&#39;: 26, &#39;name&#39;: &#39;Obnoxious&#39;, &#39;count&#39;: 209}, {&#39;id&#39;: 10, &#39;name&#39;: &#39;Inspiring&#39;, &#39;count&#39;: 24924}] . ted[&#39;ratings&#39;] = ted.ratings.apply(ast.literal_eval) pieces = [] for idx, talk in ted.iterrows(): df = pd.DataFrame(talk[&#39;ratings&#39;]) df[&#39;talk_name&#39;] = talk[&#39;name&#39;] pieces.append(df) ratings = pd.concat(pieces) ratings.head() . id name count talk_name . 0 7 | Funny | 19645 | Ken Robinson: Do schools kill creativity? | . 1 1 | Beautiful | 4573 | Ken Robinson: Do schools kill creativity? | . 2 9 | Ingenious | 6073 | Ken Robinson: Do schools kill creativity? | . 3 3 | Courageous | 3253 | Ken Robinson: Do schools kill creativity? | . 4 11 | Longwinded | 387 | Ken Robinson: Do schools kill creativity? | . Count total number of ratings received by each talk . def counter(list_of_dics): return sum(a[&#39;count&#39;] for a in list_of_dics) ted.ratings.apply(counter) . 0 93850 1 2936 2 2824 3 3728 4 25620 ... 2545 192 2546 151 2547 136 2548 583 2549 142 Name: ratings, Length: 2550, dtype: int64 .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/04/17/ted-talks.html",
            "relUrl": "/python/pandas/2020/04/17/ted-talks.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post91": {
            "title": "What were the "best" events in TED history to attend",
            "content": "Kevin Markham best practices, following this talk. . import pandas as pd path = &#39;https://raw.githubusercontent.com/justmarkham/pycon-2019-tutorial/master/ted.csv&#39; ted = pd.read_csv(path) . ted.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2550 entries, 0 to 2549 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 comments 2550 non-null int64 1 description 2550 non-null object 2 duration 2550 non-null int64 3 event 2550 non-null object 4 film_date 2550 non-null int64 5 languages 2550 non-null int64 6 main_speaker 2550 non-null object 7 name 2550 non-null object 8 num_speaker 2550 non-null int64 9 published_date 2550 non-null int64 10 ratings 2550 non-null object 11 related_talks 2550 non-null object 12 speaker_occupation 2544 non-null object 13 tags 2550 non-null object 14 title 2550 non-null object 15 url 2550 non-null object 16 views 2550 non-null int64 17 film_year 2550 non-null int64 dtypes: int64(8), object(10) memory usage: 358.7+ KB . ted.isna().sum() . comments 0 description 0 duration 0 event 0 film_date 0 languages 0 main_speaker 0 name 0 num_speaker 0 published_date 0 ratings 0 related_talks 0 speaker_occupation 6 tags 0 title 0 url 0 views 0 film_year 0 dtype: int64 . Which talks provoke the most online discussion? . We can rank the talks by number of comments, normalised by the number of views. . ted[&#39;views_per_comment&#39;] = ted.views / ted.comments ted.sort_values(&#39;views_per_comment&#39;)[[&#39;name&#39;, &#39;views_per_comment&#39;]].head() . name views_per_comment . 744 Diane J. Savino: The case for same-sex marriage | 450.531587 | . 803 David Bismark: E-voting without fraud | 651.739808 | . 96 Richard Dawkins: Militant atheism | 683.134291 | . 694 Sharmeen Obaid-Chinoy: Inside a school for sui... | 703.886818 | . 954 Janet Echelman: Taking imagination seriously | 735.525682 | . Visualise the distribution of comments . ted.comments.hist(); . ted[ted.comments &gt;= 1500].shape . (17, 19) . Most talks have fewer than 1500 comments (only 17 have more), so I drop the ones that have more. I initially tried to avoid this, but as Keving points out in the video: plotting entails decision making, as a plot is a summary of your data and not a representation of all your data. This is a good lesson to take away from this. . Use query to filter data . ted.query(&#39;comments &lt; 1000&#39;).comments.hist(); . Use indexing to filter . ted[ted.comments &lt; 1000].comments.hist(); . Use loc to filter, which is considered best practice . ted.loc[ted.comments &lt; 1000, &#39;comments&#39;].hist(bins=20, grid=False); . Plot the number of talks that took place each year . film_year = pd.to_datetime(ted.film_date, unit=&#39;s&#39;).dt.year ted.groupby(film_year).size().plot(marker=&#39;o&#39;); . Alternative approach (no need to group data first, here) . film_year[film_year.between(1997, 2016)].value_counts().sort_index().plot(marker=&#39;o&#39;); . Let&#39;s define &quot;best&quot; by event that had the most discussed talks, defined as talks with low view-to-comments ratio (i.e. talks for which people were likely to leave comments), as used above. . df = (ted.groupby(&#39;event&#39;) .agg({&#39;views_per_comment&#39;:[(&#39;total&#39;, &#39;sum&#39;), (&#39;talks&#39;, &#39;count&#39;)]}) .droplevel(level=0, axis=1) .reset_index()) df = df[df.talks &gt;= 10] df[&#39;weighted_sum&#39;] = df.total / df.talks df.sort_values(&#39;weighted_sum&#39;)[:10] . event total talks weighted_sum . 97 TEDGlobal 2010 | 286404.503200 | 55 | 5207.354604 | . 135 TEDWomen 2010 | 190844.942208 | 34 | 5613.086536 | . 106 TEDIndia 2009 | 212406.277338 | 35 | 6068.750781 | . 24 Mission Blue Voyage | 109437.301134 | 18 | 6079.850063 | . 60 TED2010 | 416144.893485 | 68 | 6119.777845 | . 93 TEDCity2.0 | 81633.440301 | 11 | 7421.221846 | . 9 EG 2007 | 97900.518061 | 13 | 7530.809082 | . 98 TEDGlobal 2011 | 518812.975335 | 68 | 7629.602578 | . 95 TEDGlobal 2007 | 214516.379586 | 27 | 7945.051096 | . 61 TED2011 | 574975.371702 | 70 | 8213.933881 | . Above is somewhat embarassing, as I&#39;m manually, and unnecessarily, calculate the mean. Simpler approach, focusing on total views, rather than normalised comments. . (ted.groupby(&#39;event&#39;).views .agg([&#39;mean&#39;, &#39;count&#39;, &#39;sum&#39;]) .sort_values(&#39;sum&#39;, ascending=False)[:5]) . mean count sum . event . TED2013 2.302700e+06 | 77 | 177307937 | . TED2014 2.072874e+06 | 84 | 174121423 | . TEDGlobal 2013 2.584163e+06 | 66 | 170554736 | . TED2015 2.011017e+06 | 75 | 150826305 | . TED2006 3.274345e+06 | 45 | 147345533 | . Unpack the ratings data . import ast # abstract syntax tree . ast.literal_eval(&#39;[1, 2, 3]&#39;) . [1, 2, 3] . pieces = [] for talk in range(len(ted.index)): ratings = ted.loc[talk, &#39;ratings&#39;] talk_name = ted.loc[talk, &#39;name&#39;] frame = pd.DataFrame(ast.literal_eval(ratings)) frame[&#39;talk_name&#39;] = talk_name pieces.append(frame) ratings = pd.concat(pieces) ratings.head() . id name count talk_name . 0 7 | Funny | 19645 | Ken Robinson: Do schools kill creativity? | . 1 1 | Beautiful | 4573 | Ken Robinson: Do schools kill creativity? | . 2 9 | Ingenious | 6073 | Ken Robinson: Do schools kill creativity? | . 3 3 | Courageous | 3253 | Ken Robinson: Do schools kill creativity? | . 4 11 | Longwinded | 387 | Ken Robinson: Do schools kill creativity? | . Alternative . ted.ratings = ted.ratings.apply(ast.literal_eval) pieces = [] for talk in range(len(ted.index)): ratings = ted.loc[talk, &#39;ratings&#39;] talk_name = ted.loc[talk, &#39;name&#39;] frame = pd.DataFrame(ratings) frame[&#39;talk_name&#39;] = talk_name pieces.append(frame) ratings = pd.concat(pieces) ratings.head() . id name count talk_name . 0 7 | Funny | 19645 | Ken Robinson: Do schools kill creativity? | . 1 1 | Beautiful | 4573 | Ken Robinson: Do schools kill creativity? | . 2 9 | Ingenious | 6073 | Ken Robinson: Do schools kill creativity? | . 3 3 | Courageous | 3253 | Ken Robinson: Do schools kill creativity? | . 4 11 | Longwinded | 387 | Ken Robinson: Do schools kill creativity? | . ratings.head() . id name count talk_name . 0 7 | Funny | 19645 | Ken Robinson: Do schools kill creativity? | . 1 1 | Beautiful | 4573 | Ken Robinson: Do schools kill creativity? | . 2 9 | Ingenious | 6073 | Ken Robinson: Do schools kill creativity? | . 3 3 | Courageous | 3253 | Ken Robinson: Do schools kill creativity? | . 4 11 | Longwinded | 387 | Ken Robinson: Do schools kill creativity? | . Count total number of ratings received by each talk . %%timeit def count_ratings(ratings): return pd.DataFrame(ratings)[&#39;count&#39;].sum() ted[&#39;count_ratings&#39;] = ted.ratings.apply(count_ratings) ted.sort_values(&#39;count_ratings&#39;, ascending=False).name[:5] . 1.53 s ± 11.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . Alternative that&#39;s much faster! . %%timeit def get_num_ratings(list_of_dicts): count = 0 for d in list_of_dicts: count += d[&#39;count&#39;] return count ted[&#39;count_ratings&#39;] = ted.ratings.apply(get_num_ratings) ted.sort_values(&#39;count_ratings&#39;, ascending=False).name[:5] . 6.33 ms ± 173 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) .",
            "url": "https://fabiangunzinger.github.io/blog/2020/04/17/markham-best-practices.html",
            "relUrl": "/2020/04/17/markham-best-practices.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post92": {
            "title": "Python data science handbook tricks",
            "content": "Based on content from the Python Data Science Handbook. . Planets dataset from data science handbook . import pandas as pd import seaborn as sns df = sns.load_dataset(&#39;planets&#39;) print(df.shape) df.head() . (1035, 6) . method number orbital_period mass distance year . 0 Radial Velocity | 1 | 269.300 | 7.10 | 77.40 | 2006 | . 1 Radial Velocity | 1 | 874.774 | 2.21 | 56.95 | 2008 | . 2 Radial Velocity | 1 | 763.000 | 2.60 | 19.84 | 2011 | . 3 Radial Velocity | 1 | 326.030 | 19.40 | 110.62 | 2007 | . 4 Radial Velocity | 1 | 516.220 | 10.50 | 119.47 | 2009 | . Show number of planets discovered by each methods in each decade . decade = (df.year // 10) * 10 decade = decade.astype(&#39;str&#39;) + &#39;s&#39; df.groupby([&#39;method&#39;, decade]).number.sum().unstack().fillna(0) . year 1980s 1990s 2000s 2010s . method . Astrometry 0.0 | 0.0 | 0.0 | 2.0 | . Eclipse Timing Variations 0.0 | 0.0 | 5.0 | 10.0 | . Imaging 0.0 | 0.0 | 29.0 | 21.0 | . Microlensing 0.0 | 0.0 | 12.0 | 15.0 | . Orbital Brightness Modulation 0.0 | 0.0 | 0.0 | 5.0 | . Pulsar Timing 0.0 | 9.0 | 1.0 | 1.0 | . Pulsation Timing Variations 0.0 | 0.0 | 1.0 | 0.0 | . Radial Velocity 1.0 | 52.0 | 475.0 | 424.0 | . Transit 0.0 | 0.0 | 64.0 | 712.0 | . Transit Timing Variations 0.0 | 0.0 | 0.0 | 9.0 | . Loop over groupbs and use nice formatting . for (method, data) in df.groupby(&#39;method&#39;): print(f&#39;{method:35}{data.shape}&#39;) . Astrometry (2, 6) Eclipse Timing Variations (9, 6) Imaging (38, 6) Microlensing (23, 6) Orbital Brightness Modulation (3, 6) Pulsar Timing (5, 6) Pulsation Timing Variations (1, 6) Radial Velocity (553, 6) Transit (397, 6) Transit Timing Variations (4, 6) . Titanic survivors . tit = sns.load_dataset(&#39;titanic&#39;) print(tit.shape) tit.head() . (891, 15) . survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone . 0 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | Third | man | True | NaN | Southampton | no | False | . 1 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | First | woman | False | C | Cherbourg | yes | False | . 2 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | Third | woman | False | NaN | Southampton | yes | True | . 3 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | First | woman | False | C | Southampton | yes | False | . 4 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | Third | man | True | NaN | Southampton | no | True | . tit.pivot_table(&#39;survived&#39;, index=&#39;sex&#39;, columns=&#39;class&#39;) . class First Second Third . sex . female 0.968085 | 0.921053 | 0.500000 | . male 0.368852 | 0.157407 | 0.135447 | . age = pd.cut(tit.age, [0, 18, 100]) fare = pd.qcut(tit.fare, 2) tit.pivot_table(&#39;survived&#39;, index=[&#39;sex&#39;, age], columns=[&#39;class&#39;, fare]) . class First Second Third . fare (-0.001, 14.454] (14.454, 512.329] (-0.001, 14.454] (14.454, 512.329] (-0.001, 14.454] (14.454, 512.329] . sex age . female (0, 18] NaN | 0.909091 | 1.000000 | 1.000000 | 0.714286 | 0.318182 | . (18, 100] NaN | 0.972973 | 0.880000 | 0.914286 | 0.444444 | 0.391304 | . male (0, 18] NaN | 0.800000 | 0.000000 | 0.818182 | 0.260870 | 0.178571 | . (18, 100] 0.0 | 0.391304 | 0.098039 | 0.030303 | 0.125000 | 0.192308 | . US birth rate data . import seaborn as sns sns.set() sns.set_style(&#39;white&#39;) df = pd.read_csv(&#39;./data/births.csv&#39;) print(df.shape) df.head() . (15547, 5) . year month day gender births . 0 1969 | 1 | 1.0 | F | 4046 | . 1 1969 | 1 | 1.0 | M | 4440 | . 2 1969 | 1 | 2.0 | F | 4454 | . 3 1969 | 1 | 2.0 | M | 4548 | . 4 1969 | 1 | 3.0 | F | 4548 | . Number of birhts per year by gender . df.pivot_table(&#39;births&#39;, index=&#39;year&#39;, columns=&#39;gender&#39;, aggfunc=&#39;sum&#39;).plot(); .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/04/05/python-datascience-handbook-tricks.html",
            "relUrl": "/python/pandas/2020/04/05/python-datascience-handbook-tricks.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post93": {
            "title": "ML basics",
            "content": "Introduction and definitions . Why do we estimate f? . Purpose of ml is often to infer a function f that describes the relationship between target and features. . | Can estimate f for (1) prediction or (2) inference or both. . | . How do we estimate f? . 3 basic approaches: parametric (assume shape of f and estimate coefficients), non-parametric (also estimate shape of f), semi-parametric. . | Accuracy depends on (1) irreducible error (variance of error term) and (2) reducible error (appropriateness of our model and its assumptions) . | . Ingredients to statistical learning . Specify aim . | Gather and pre-process data . | Select a learning algorithm . | Apply learning algorithm to data to build (and train) a model . | Assess model performance (by testing) and tune model . | Make predictions . | . Types of learning . Supervised (labelled examples) . | Unsupervised (unlabelled examples) . | Semi-supervised (labelled and unlabelled examples) . | Reinforcement . | . The trade-off between prediction accuracy and model interpretability . Linear models vs non-linear models (hard to interpret models often predict more accurately). | . Supervised vs. unsupervised learning . Regression vs classification problems . Classification assigns categorical labels, regression real-valued labels to unlabelled examples. | . Hyperparameters vs parameters . Hyperparameters determine how the algorithm works and are set by the researcher. . | Parameters determine the shape of the model and are estimated. . | . Model-based vs instance-based learning . Model-based algorithms estimate and then use parameters to make predictions (i.e. can discard training data once you have estimate), instance-based algorithms (e.g. KNN) use the entire training dataset. | . Deep vs shallow learning . Shallow learning algorithms learn parameters directly from features, deep learning algorithms (deep neural network) learn them from the output of preceeding layers. | . Sources of prediction error . If we think about $Y = f(X) + epsilon$ . | Reducible error stems from our imperfect ability to estimate the model (the systematic relationship between X and y) i.e. the difference between $f$ and $ hat{f}$. . | Irreducible error stems from the fact that even if we could perfectly model the relationship between X and Y, Y would still be a function of the error term $ epsilon$, which we cannot reduce. This could be variables other than X that predict Y, or random variation in Y (e.g. purchasing behaviour on given day influenced by car breakdown). | . Practical tips . One way to get a sense of how non-linear the problem is, is to compare the MSE of a simple linear model and a more complex model. If the two are very close, then assuming linearity and using a simple model is preferrable. | . Sources . The hundred-page machine learning book | An introduction to statistical learning | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/stats/2020/04/01/ml-basics.html",
            "relUrl": "/ml/stats/2020/04/01/ml-basics.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post94": {
            "title": "Creating dummy variables in Pandas",
            "content": "A quick post to remind my future self of how to create dummy variables. . import pandas as pd . df = pd.DataFrame({ &#39;id&#39;: [1, 2, 3, 4, 5], &#39;quality&#39;: [&#39;good&#39;, &#39;excellent&#39;, &#39;very good&#39;, &#39;excellent&#39;, &#39;good&#39;] }) df.head() . id quality . 0 1 | good | . 1 2 | excellent | . 2 3 | very good | . 3 4 | excellent | . 4 5 | good | . Pandas makes creating dummies easy: . pd.get_dummies(df.quality) . excellent good very good . 0 0 | 1 | 0 | . 1 1 | 0 | 0 | . 2 0 | 0 | 1 | . 3 1 | 0 | 0 | . 4 0 | 1 | 0 | . If you want to label the source of the data, you can use the prefix argument: . pd.get_dummies(df.quality, prefix=&#39;quality&#39;) . quality_excellent quality_good quality_very good . 0 0 | 1 | 0 | . 1 1 | 0 | 0 | . 2 0 | 0 | 1 | . 3 1 | 0 | 0 | . 4 0 | 1 | 0 | . Often when we work with dummies from a variable with $n$ distinct values, we create $n-1$ dummies and treat the remaining group as the reference group. Pandas provides a convenient way to do this: . pd.get_dummies(df.quality, prefix=&#39;quality&#39;, drop_first=True) . quality_good quality_very good . 0 1 | 0 | . 1 0 | 0 | . 2 0 | 1 | . 3 0 | 0 | . 4 1 | 0 | . Usually, we&#39;ll want to use the dummies with the rest of the data, so it&#39;s conveninet to have them in the original dataframe. One way to do this is to use concat like so: . dummies = pd.get_dummies(df.quality, prefix=&#39;quality&#39;, drop_first=True) df_with_dummies = pd.concat([df, dummies], axis=1) df_with_dummies.head() . id quality quality_good quality_very good . 0 1 | good | 1 | 0 | . 1 2 | excellent | 0 | 0 | . 2 3 | very good | 0 | 1 | . 3 4 | excellent | 0 | 0 | . 4 5 | good | 1 | 0 | . This works. But Pandas provides a much easier way: . df_with_dummies1 = pd.get_dummies(df, columns=[&#39;quality&#39;], drop_first=True) df_with_dummies1 . id quality_good quality_very good . 0 1 | 1 | 0 | . 1 2 | 0 | 0 | . 2 3 | 0 | 1 | . 3 4 | 0 | 0 | . 4 5 | 1 | 0 | . That&#39;s it. In one line we get a new dataframe that includes the dummies and excludes the original quality column. . Sources . Data School | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/04/01/creating-dummies.html",
            "relUrl": "/python/pandas/2020/04/01/creating-dummies.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post95": {
            "title": "Distributions",
            "content": "My notes from working through section 2, data and sampling distributions, of Practical statistics for data science, to revise consepts and get comfortable implementing them in Python. . Terminology . Stochastic is a synonym for random. A stochastic process is a random process. The distinction between stochastics and statistics is that a stochastic processes generate the data we analyse in statistics. | . Sampling . We rely on a sample to learn about a larger population. | We thus need to make sure that the sampling procedure is free of bias, so that units in the sample are representative of those in the population. | While representativeness cannot be achieved perfectly, it&#39;s important to ensure that non-representativeness is due to random error and not due to systematic bias. | Random errors produce deviations that vary over repeated samples, while systematic bias persists. Such selection bias can lead to misleading and ephemeral conclusions. | Two basic sampling procedures are simple random sampling (randomly select $n$ units from a population of $N$) and stratified random sampling (randomly select $n_s$ from each stratum $S$ of a population of $N$). | The mean outcome of the sample is denoted $ bar{x}$, that of the population $ mu$. | . Selection bias . Using the data to answer many questions will eventually reveal something interesting by mere chance (if 20,000 people flip a coin 10 times, some will have 10 straight heads). This is sometimes called the Vast Search Effect. | Common types of selection bias in data science: The vast search effect | Nonrandom sampling | Cherry-picking data | Selecting specific time-intervals | Stopping experiments prematurely | . | Ways to guard against selection bias: have one or many holdout datasets to confirm your results. | Regression to the mean results form a particular kind of selection bias in a setting where we measure outcomes repeatedly over time: when luck and skill combine to determine outcomes, winners of one period will be less lucky next period and perform closer to the mean performer. | . Sampling distributions . A sampling distribution is the distribution of a statistic (e.g. the mean) over many repeated samples. Classical statistics is much concerned with making inferences from samples about the population based on such statistics. | When we measure an attribute of the population based on a sample using a statistic, the result will vary over repeated samples. To capture by how much it varies, we are concerned with the sampling variability. . | Key distinctions: . The data distribution is the distribution of the data in the sample, the sampling distribution is the distribution of the sample statistic. | The standard deviation is a measure of spread of the data distribution, the standard error a measure of spread of the sampling distribution. | . | . import pandas as pd import numpy as np import seaborn as sns from scipy.stats import norm import matplotlib.pyplot as plt mean, sd, N = 0, 1, 1_000_000 full_data = norm.rvs(mean, sd, N) sample_data = pd.DataFrame({ &#39;income&#39;: np.random.choice(full_data, 1000), &#39;type&#39;: &#39;Data&#39; }) mof5 = pd.DataFrame({ &#39;income&#39;: [np.random.choice(full_data, 5).mean() for _ in range(1000)], &#39;type&#39;:&#39;Mean of 5&#39; }) mof20 = pd.DataFrame({ &#39;income&#39;: [np.random.choice(full_data, 20).mean() for _ in range(1000)], &#39;type&#39;:&#39;Mean of 20&#39; }) mof100 = pd.DataFrame({ &#39;income&#39;: [np.random.choice(full_data, 100).mean() for _ in range(1000)], &#39;type&#39;:&#39;Mean of 100&#39; }) results = pd.concat([sample_data, mof5, mof20, mof100]) g = sns.FacetGrid(results, col=&#39;type&#39;) g.map(plt.hist, &#39;income&#39;, bins=40) g.set_axis_labels(&#39;Income&#39;, &#39;Count&#39;) g.set_titles(&#39;{col_name}&#39;); . Plots show that: . Data distribution has larger spread than sampling distributions (each data point is a special case of a sample with n = 1) | The spread of sampling distributions decreases with increasing sample size | . Degrees of freedom . The number of parameters you had to estimate en route to calculate the desired statistic (source). If you calculate sample variance with an estimated mean rather than a known mean, you have to estimate the sample mean first and thus loose 1 degree of freedom. Hence, you&#39;d divide the sum of squared deviations from the (estimated) mean by n-1 rather than n. | . Central limit theorem . The second point above is an instance of the central limit theorem, which states that means from multiple samples are normally distributed even if the underlying distribution is not normal, provied that the sample size is large enough. . | More precisely: Suppose that we have a sequence of independent and identically distributed (iid) random variables $ {x_1, ..., x_n }$ drawn from a distribution with expected value $ mu$ and finite variance given by $ sigma^2$, and we are interested in the mean value $ bar{x} = frac{x_1 + ... + x_n}{n}$. By the law of large numbers, $ bar{x}$ converges to $ mu$. The central limite theorem describes the shape of the random variation of $ bar{x}$ around $ mu$ during this convergence. In particular, for large enough $n$, the distribution of $ bar{x}$ will be close to a normal distribution with mean $ mu$ and standard deviation $ sigma/n$. . | This is useful because it means that irrespective of the underlying distribution (i.e. the distribution of the values in our sequence above), we can use the normal distribution and approximations to it (such as the t-distribution) to calculate sample distributions when we do inference. Because of this, the CLT is at the heart of the theory of hypothesis testing and confidence intervals, and thus of much of classical statistics. . | For experiments, this means that our estiamted treatment effect is normally distributed, which is what allows us to draw inferences from our experimental setting ot the population as a whole. The CLT is thus at the heart of the experimental approach. . | . from scipy.stats import norm, gamma import matplotlib.pyplot as plt def means(n): return [np.mean(norm.rvs(0, 2, 10)) for _ in range(n)] plt.subplots(figsize=(10,10)) plt.subplot(441) plt.hist(means(100), bins=30) plt.subplot(442) plt.hist(means(1000), bins=30) plt.subplot(443) plt.hist(means(10000), bins=30); . Standard error . The standard error is a measure for the variability of the sampling distribution. | It is related to the standard deviation of the observations, $ sigma$ and the sample size $n$ in the following way: | . $$ se = frac{ sigma}{ sqrt{n}} $$ The relationship between sample size and se is sometimes called the &quot;Square-root of n rule&quot;, since reducing the $se$ by a factor of 2 requires an increase in the sample size by a factor of 4. | . Bootstrap . In practice, we often use the bootstrap to calculate standard errors of model parameters or statistics. | Conceptually, the bootstrap works as follows: 1) we draw an original sample and calculate our statistic, 2) we then create a blown-up version of that sample by duplicating it many times, 3) we then draw repeated samples from the large sample, recalculate our statistic, and calculate the standard deviation of these statistics to get the standard error. | To achieve this easily, we can skip step 2) by simply sampling with replacement from the original distribution in step 3). | The full procedure makes clear what the bootstrap results tell us, however: they tell us how lots of additional samples would behave if they were drawn from a population like our original sample. | Hence, if the original sample is not representative of the population of interest, then bootstrap results are not informative about that population either. | The bootstrap can also be used to improve the performance of classification or regression trees by fitting multiple trees on bootstrapped sample and then averaging their predictions. This is called &quot;bagging&quot;, short for &quot;bootstrap aggregating&quot;. | . from sklearn.utils import resample mean, sd, N = 0, 5, 1000 original_sample = norm.rvs(mean, sd, N) results = [] for nrepeat in range(1000): sample = resample(original_sample) results.append(np.median(sample)) print(&#39;Bootstrap Statistics:&#39;) print(f&#39;Original: {np.median(original_sample)}&#39;) print(f&#39;Bias: {np.median(results) - np.median(original_sample)}&#39;) print(f&#39;Std. error: {np.std(results)}&#39;) . Bootstrap Statistics: Original: 0.028785991557600685 Bias: -0.0017687396759709026 Std. error: 0.15409722327225703 . Confidence intervals . A CI is another way to learn about the variability of a test statistic. | It can be calculated using the (standard) normal distribution or the t-distribution (if sample sizes are small). | But for data science purposes we can compute a x percent CI from the bootstrap, following this algorithm: 1) Draw a large number of bootstrap samples and calculate the statistic of interest, 2) Trim [(100-x)/2] percent of the bootstrap results on either end of the distribution, 3) the trim points are the end point of the CI. | . The normal distribution . Useful not mainly because data is often normally distributed, but because sample distributions of statistics (as well as errors) often are. | But rely on normality assumption only as a last resort if using empirical distributions or bootstrap is not available. | . Q-Q plots . Q-Q plots (for quantile-quantile plot) help us compare the quantiles in our dataset to the quantiles of a theoretical distribution to see whether our data follows this distribution (I&#39;ll refer to the normal distribution below to fix ideas). | In general, the x percent quantile is a point in the data such that x percent of the data fall below it (this point is also the xth percentile). | To create a Q-Q plot, we proceed as follows: First, we split the data into quantiles such that each data point represents its own quantiles. Second, we split the normal distribution into an equal number of quantiles (for the normal distribution, quantiles are intervals of equal probability mass). Third, we mark the quantiles for the data on the y-axis and for the normal distribution on the x-axis. Finally, we use these points as coordinates for each quantile in the plot. (See this helpful video for more details on how to construct Q-Q plots, and this useful article for details on probability plots more generally.) | . import numpy as np import seaborn as sns import matplotlib.pyplot as plt from statsmodels.api import ProbPlot from scipy import stats %config InlineBackend.figure_format =&#39;retina&#39; sns.set_style(&#39;darkgrid&#39;) sns.mpl.rcParams[&#39;figure.figsize&#39;] = (10.0, 6.0) # Comparing skew normal and standard normal n = 10000 rv_std_normal = np.random.normal(size=n) rv_skew_normal = stats.skewnorm.rvs(a=5, size=n) fig, ax = plt.subplots(1, 3, figsize=(15,5)) ProbPlot(rv_std_normal).qqplot(line=&#39;s&#39;, ax=ax[0]) ax[0].set_title(&#39;Q-Q plot for std. normal - std. normal&#39;) ProbPlot(rv_skew_normal).qqplot(line=&#39;s&#39;, ax=ax[1]) ax[1].set_title(&#39;Q-Q plot for skew normal - std. normal&#39;) sns.distplot(rv_skew_normal, kde=False, norm_hist=True, label=&#39;Skew normal&#39;, ax=ax[2]) sns.distplot(rv_std_normal, kde=False, norm_hist=True, label=&#39;Std. normal&#39;, ax=ax[2]) ax[2].set_title(&#39;Histograms&#39;) ax[2].legend(); . /Users/fgu/miniconda3/envs/habits/lib/python3.7/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /Users/fgu/miniconda3/envs/habits/lib/python3.7/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . As expected, data from a standard normal distribution fits almost perfectly onto standard normal quantiles, while data from our positively skewed distribution does not -- it has more probability mass for lower values, as well as more extreme higher values. . import os import pandas_datareader as pdr from dotenv import load_dotenv from datetime import datetime load_dotenv() start = datetime(2019, 1, 1) end = datetime(2019, 12, 31) key = os.getenv(&#39;tiingo_api_key&#39;) goog = np.log(pdr.get_data_tiingo(&#39;GOOG&#39;, start, end, api_key=key)[&#39;close&#39;]).diff().dropna() fix, ax = plt.subplots(1, 2) ProbPlot(nflx).qqplot(line=&#39;s&#39;, ax=ax[0]) ax[0].set_title(&#39;Q-Q plot for Google returns - std. normal&#39;) sns.distplot(nflx, norm_hist=True, ax=ax[1]); . The above graph shows clearly that Google&#39;s daily stock returns are not normally distributed. While the inner part of the distribution fits a normal distribution relatively well, the returns distribution has (very) fat tails. . Chi-Squared distribution . To assess goodness of fit. | . F distribution . Can be used to measure whether means of different treatment groups differ from control condition. | F-statistic is calculated as the ratio of the variance between groups and the variance within groups (ANOVA). | F distribution gives all values that would be produced if between variance were zero (i.e. under the null model). | Df is given by the number of groups we compare. | . Poisson distribution . Useful to model processes that randomly generate outcomes at a constant rate (e.g. processes like arrivals that vary over time, or number of defects or typos that vary over space). | The parameter of the distribution is lambda, which is both the rate per unit of time and the variance. | The poisson and exponential distribution can be very useful when modelling, say, arrivals and waiting times. It&#39;s important, though, to remember the three key assumptions: 1) lambda remains constant across intervals, 2) events are independent, and 3) two events cannot occur at the same time. | To account for 1), defining the intervals such that they are sufficiently homogenous often helps. | . x = np.random.poisson(2, 1000000) y = np.random.poisson(6, 1000000) plt.hist(x, alpha=0.5, label=&#39;$ lambda = 2$&#39;, bins=np.arange(min(x), max(x))-0.5) plt.hist(y, alpha=0.5, label=&#39;$ lambda = 6$&#39;, bins=np.arange(min(y), max(y))-0.5) plt.legend(); . Exponential distribution . Takes the same parameter lambda as the Poisson distribution, but can be used to model the time between random events occuring at a frequent rate lambda (i.e. the time/space difference between Poisson events). | . n = 100000 x = np.random.exponential(2, n) y = np.random.exponential(6, n) plt.hist(x, alpha=0.5, label=&#39;$ lambda = 2$&#39;, bins=np.arange(min(x), max(x))-0.5) plt.hist(y, alpha=0.5, label=&#39;$ lambda = 6$&#39;, bins=np.arange(min(y), max(y))-0.5) plt.legend(); . Weibull distribution . Used to model events for which the event rate changes during the time of the interval, and thus violates the poisson and exponential assumption. | An example is mechanical failure, where the probability of failure increases as time goas by. | Parameters of the distribution are $ eta$, the scale parameter, and $ beta$, the shape parameter ($ beta &gt; 1$ indicates increasing probability of an event over time, $ beta &lt; 1$ decreasing probability). | . Linear regression and regression diagnostic . Follow this notebook. . Misc . Comparing speed of generating random samples with numpy and scipy . %%timeit np.random.normal(size=10) . 3.34 µs ± 38 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) . %%timeit norm.rvs(size=10) . 33.9 µs ± 1.27 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) . Turns out numpy is about ten times faster –- 3.3 microseconds (10e^-6s) compared to 33 microseconds. . Plotting distributions in Seaborn . n = 10000 rv_std_normal = np.random.normal(size=n) rv_normal = np.random.normal(1, 2.5, n) rv_skew_normal = stats.skewnorm.rvs(a=5, size=n) . x = np.linspace(min(rv_std_normal), max(rv_std_normal), 1000); pdf = stats.norm.pdf(x) cdf = stats.norm.cdf(x) ax = sns.distplot(rv_std_normal, kde=False, norm_hist=True, label=&#39;Data&#39;) ax.plot(x, pdf, lw=2, label=&#39;PDF&#39;) ax.plot(x, cdf, lw=2, label=&#39;CDF&#39;) ax.set_title(&#39;Standard normal distribution&#39;) ax.legend(); . ax = sns.distplot(rv_std_normal, kde=False, norm_hist=True, label=&#39;Standard normal&#39;) ax = sns.distplot(rv_normal, kde=False, norm_hist=True, label=&#39;N(1, 2.5)&#39;) ax = sns.distplot(rv_skew_normal, kde=False, norm_hist=True, label=&#39;Skew normal, $ alpha$=5&#39;) ax.set_title(&#39;Comparison of different distributions&#39;) ax.legend(); .",
            "url": "https://fabiangunzinger.github.io/blog/stats/2020/03/15/distributions.html",
            "relUrl": "/stats/2020/03/15/distributions.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post96": {
            "title": "Randomisation",
            "content": "An experiment is a proceedure to test whether one or multiple variations of the status quo deliver superior results. . We generally proceed in the following ways: . Define the problem (e.g. improve search experience) | Design solution (e.g. new landing page for dead searches) | Define outcome metric to be improved (e.g. number of dead searches) | Define target population and smapling approach | Specify hypothesis | Conduct power calculations | Randomise units into groups | Run experiment | Analyse results | Communicate results | Randomisation . Level of randomisation | Simple randomisation | Stratified randomisation | . Randomisation functions . import numpy as np import pandas as pd import random import matplotlib.pyplot as plt plt.style.use(&#39;seaborn&#39;) from scipy.stats import norm, bernoulli, uniform . np.random.seed(231286) n = 100 cities = [&#39;London&#39;, &#39;Manchester&#39;, &#39;Birmingham&#39;] data = { &#39;ids&#39;: np.arange(n), &#39;age&#39;: norm.rvs(45, 20, n).astype(&#39;int&#39;), &#39;female&#39;: np.random.randint(0, 2, n), &#39;city&#39;: np.random.choice(cities, n) } df = pd.DataFrame(data) df.head(10) . ids age female city . 0 0 | 55 | 0 | Manchester | . 1 1 | 50 | 0 | Manchester | . 2 2 | 53 | 1 | Manchester | . 3 3 | 61 | 1 | Birmingham | . 4 4 | 58 | 1 | Manchester | . 5 5 | 39 | 1 | Birmingham | . 6 6 | 31 | 1 | London | . 7 7 | 50 | 0 | London | . 8 8 | 40 | 0 | Manchester | . 9 9 | 28 | 1 | Manchester | . def simple_rand(df, n_groups): # Randomly sort df rows df = df.sample(frac=1).reset_index(drop=True) # Allocate to groups df[&#39;group&#39;] = np.ceil(n_groups * (df.index+1) / len(df)).astype(&#39;int&#39;) return df def blocked_rand(df, n_groups, block_vars): return ( df.groupby(block_vars) .apply(simple_rand, n_groups) .reset_index(drop=True) ) blocked_rand(df, 3, [&#39;city&#39;, &#39;female&#39;]).head() . ids age female city group . 0 33 | 22 | 0 | Birmingham | 1 | . 1 20 | 12 | 0 | Birmingham | 1 | . 2 46 | 47 | 0 | Birmingham | 1 | . 3 73 | 21 | 0 | Birmingham | 1 | . 4 34 | 38 | 0 | Birmingham | 2 | . np.random.random_sample? . Misc . n = int(1e6) bins = 100 plt.hist(norm.rvs(0, 8, n), bins=bins) plt.hist(norm.rvs(0, 4, n), bins=bins) plt.hist(norm.rvs(0, 2, n), bins=bins); .",
            "url": "https://fabiangunzinger.github.io/blog/python/stats/2020/03/09/randomisation.html",
            "relUrl": "/python/stats/2020/03/09/randomisation.html",
            "date": " • Mar 9, 2020"
        }
        
    
  
    
        ,"post97": {
            "title": "Power",
            "content": "Based on this post from Deliveroo engineering blog. . import numpy as np import pandas as pd import altair as alt from scipy.stats import norm, binom, mannwhitneyu from statsmodels.stats.weightstats import ttest_ind . t-test . # experiment. For illustration purposes here we have generated data from a normal # distribution. sample_mean = 21.50 sample_sd = 12.91 sample_data = norm.rvs(loc=sample_mean, scale=sample_sd, size=20000) sample_sizes = range(250, 20000 + 1, 250) # Sample sizes we will test over alpha = 0.05 # Our fixed alpha sims = 20 # The number of simulations we will run per sample size # The minimum relative effect we will test for (3%). We could try multiple relative # effect is we are not sure what our minimum relative effect should be relative_effect = 1.03 alternative = &quot;two-sided&quot; # Is the alternative one-sided or two-sided power_dist = np.empty((len(sample_sizes), 2)) for i in range(0, len(sample_sizes)): N = sample_sizes[i] control_data = sample_data[0:N] # Multiply the control data by the relative effect, this will shift the distribution # of the variant left or right depending on the direction of the relative effect variant_data = control_data * relative_effect significance_results = [] for j in range(0, sims): # Randomly allocate the sample data to the control and variant rv = binom.rvs(1, 0.5, size=N) control_sample = control_data[rv == True] variant_sample = variant_data[rv == False] # Use Welch&#39;s t-test, make no assumptions on tests for equal variances test_result = ttest_ind(control_sample, variant_sample, alternative=alternative, usevar=&#39;unequal&#39;) # Test for significance significance_results.append(test_result[1] &lt;= alpha) # The power is the number of times we have a significant result # as we are assuming the alternative hypothesis is true power_dist[i,] = [N, np.mean(significance_results)] . power_dist = pd.DataFrame(power_dist, columns=[&#39;sample_size&#39;, &#39;power&#39;]) source = power_dist tbase = alt.Chart().encode( x=alt.X(&#39;sample_size&#39;, axis=alt.Axis(title=&#39;Sample size&#39;)), y=alt.Y(&#39;power&#39;, axis=alt.Axis(title=&#39;Power&#39;)) ) hline = alt.Chart().mark_rule().encode( y=alt.Y(&#39;a:Q&#39;, axis=alt.Axis(title=&#39;&#39;)), ) alt.layer( tbase.mark_point(), tbase.transform_loess(&#39;sample_size&#39;, &#39;power&#39;).mark_line(), hline, data=source ).transform_calculate(a=&quot;0.8&quot;) . t and U test . # experiment. For illustration purposes here we have generated data from a normal # distribution. sample_mean = 21.50 sample_sd = 12.91 sample_data = norm.rvs(loc=sample_mean, scale=sample_sd, size=20000) sample_sizes = range(250, 20000 + 1, 250) # Sample sizes we will test over alpha = 0.05 # Our fixed alpha sims = 20 # The number of simulations we will run per sample size # The minimum relative effect we will test for (3%). We could try multiple relative # effect is we are not sure what our minimum relative effect should be relative_effect = 1.03 alternative = &quot;two-sided&quot; # Is the alternative one-sided or two-sided power_dist = np.empty((len(sample_sizes), 3)) for i in range(0, len(sample_sizes)): N = sample_sizes[i] control_data = sample_data[0:N] # Multiply the control data by the relative effect, this will shift the distribution # of the variant left or right depending on the direction of the relative effect variant_data = control_data * relative_effect significance_tresults = [] significance_uresults = [] for j in range(0, sims): # Randomly allocate the sample data to the control and variant rv = binom.rvs(1, 0.5, size=N) control_sample = control_data[rv == True] variant_sample = variant_data[rv == False] # Use Welch&#39;s t-test, make no assumptions on tests for equal variances ttest_result = ttest_ind(control_sample, variant_sample, alternative=alternative, usevar=&#39;unequal&#39;) # Use Mann-Whitney U-test utest_result = mannwhitneyu(control_sample, variant_sample, alternative=alternative) # Test for significance significance_tresults.append(ttest_result[1] &lt;= alpha) significance_uresults.append(utest_result[1] &lt;= alpha) # The power is the number of times we have a significant result # as we are assuming the alternative hypothesis is true power_dist[i,] = [N, np.mean(significance_tresults), np.mean(significance_uresults)] . power_dist = pd.DataFrame(power_dist, columns=[&#39;sample_size&#39;, &#39;tpower&#39;, &#39;upower&#39;]) power_dist = power_dist.melt(id_vars=&#39;sample_size&#39;, value_vars=[&#39;tpower&#39;, &#39;upower&#39;], var_name=&#39;test&#39;, value_name=&#39;power&#39;) labels = {&#39;tpower&#39;:&#39;MC Simulation t-test&#39;, &#39;upower&#39;: &#39;MC Simulation MWW U-test&#39;} power_dist[&#39;test&#39;].replace(labels, inplace=True) . power_dist.head() . sample_size test power . 0 250.0 | MC Simulation t-test | 0.00 | . 1 500.0 | MC Simulation t-test | 0.05 | . 2 750.0 | MC Simulation t-test | 0.05 | . 3 1000.0 | MC Simulation t-test | 0.10 | . 4 1250.0 | MC Simulation t-test | 0.20 | . dots = alt.Chart(power_dist).mark_point().encode( x=alt.X(&#39;sample_size&#39;, axis=alt.Axis(title=&#39;Sample size&#39;)), y=alt.Y(&#39;power&#39;, axis=alt.Axis(title=&#39;Power&#39;)), color=alt.Color(&#39;test&#39;, legend=alt.Legend(title=&quot;&quot;)) ) hline = alt.Chart(power_dist).mark_rule().encode( y=alt.Y(&#39;a:Q&#39;, axis=alt.Axis(title=&#39;&#39;)), ).transform_calculate(a=&#39;0.8&#39;) dots + dots.transform_loess(&#39;sample_size&#39;, &#39;power&#39;, groupby=[&#39;test&#39;]).mark_line() + hline . Program . import numpy as np import pandas as pd import altair as alt from scipy.stats import norm, binom, mannwhitneyu from statsmodels.stats.weightstats import ttest_ind . def get_power_dist(data, sample_min, sample_max, stepsize, relative_effect=1.03, sims=20, alpha=0.05, alternative_t=&#39;two-sided&#39;, alternative_u=&#39;two-sided&#39;, tests=[&#39;t&#39;, &#39;u&#39;]): &quot;&quot;&quot; Calculates power for each sample size and each specified test. Code is adapted version of https://deliveroo.engineering/2018/12/07/monte-carlo-power-analysis.html Parameters -- sample_min : integer (required) minimum sample size for which power is calculated sample_max : integer (required) maximum sample size for which power is calculated stepsize : integer (required) number of sample sized skipped between each calculation relative_effect : float (optional) relative effect size of variant, default is 3 percent sims : integer (optional) number of simulations per sample size, default is 20 alpha : float (optional) probability of Type I error, default is 5 percent alternative_t : string (optional) type of t-test performed, either &#39;two-sided&#39; (default), &#39;larger&#39;, or &#39;smaller&#39; alternative_u : string (optional) type of Mann-Whitney-Wilkinson U-test performed, either &#39;two-sided&#39; (default), &#39;more&#39;, or &#39;less&#39; tests : list of strings (optional) test results plotted, either [&#39;t&#39;, &#39;u&#39;] (default), [&#39;t&#39;], or [&#39;u&#39;] Returns -- dataframe : a dataframe containing sample size and associated power levels for each test &quot;&quot;&quot; sample_sizes = range(sample_min, sample_max + 1, stepsize) power_dist = np.empty((len(sample_sizes), 3)) for i in range(0, len(sample_sizes)): N = sample_sizes[i] control_data = data[0:N] variant_data = control_data * relative_effect significance_tresults = [] significance_uresults = [] for j in range(0, sims): # Randomly allocate the sample data to the control and variant rv = binom.rvs(1, 0.5, size=N) control_sample = control_data[rv == True] variant_sample = variant_data[rv == False] # Calculate Welch&#39;s t-test and Mann-Whitney U-test ttest_result = ttest_ind(control_sample, variant_sample, alternative=alternative_t, usevar=&#39;unequal&#39;) utest_result = mannwhitneyu(control_sample, variant_sample, alternative=alternative_u) # Test for significance and calculate power significance_tresults.append(ttest_result[1] &lt;= alpha) tpower = np.mean(significance_tresults) significance_uresults.append(utest_result[1] &lt;= alpha) upower = np.mean(significance_uresults) # Store results for sample size i power_dist[i,] = [N, tpower, upower] # Convert to dataframe and keep results for selected tests power_dist = pd.DataFrame(power_dist, columns=[&#39;sample_size&#39;, &#39;t&#39;, &#39;u&#39;]) power_dist = power_dist.melt(id_vars=&#39;sample_size&#39;, value_vars=[&#39;t&#39;, &#39;u&#39;], var_name=&#39;test&#39;, value_name=&#39;power&#39; ) power_dist = power_dist[power_dist[&#39;test&#39;].isin(tests)] labels = {&#39;t&#39;:&#39;MC Simulation t-test&#39;, &#39;u&#39;: &#39;MC Simulation MWW U-test&#39;} power_dist[&#39;test&#39;].replace(labels, inplace=True) return power_dist def plot_power_graph(data, x=&#39;sample_size&#39;, y=&#39;power&#39;, color=&#39;test&#39;, hline_pos=&#39;0.8&#39;): &quot;&quot;&quot; Creates a standardised power graph Parameters - data : dataframe (required) name of dataframe x : string (optional) name of x coordinate (sample size), default is &#39;sample_size&#39; y : string (optional) name of y coordinate (power), default is &#39;power&#39; color : string (optional) name of variable for color encoding, default is &#39;test&#39; hline_pos : str of float (optional) position of horizontal line to indicate target power level, default is &#39;0.8&#39; Returns - Altair plot: A plot showing level of power for each sample size for each test. &quot;&quot;&quot; dots = alt.Chart(data).mark_point().encode( x=alt.X(x, axis=alt.Axis(title=&#39;Sample size&#39;)), y=alt.Y(y, axis=alt.Axis(title=&#39;Power&#39;)), color=alt.Color(color, legend=alt.Legend(title=&quot;&quot;)) ) loess = dots.transform_loess(x, y, groupby=[color]).mark_line() hline = alt.Chart(data).mark_rule(color=&#39;red&#39;).encode( y=alt.Y(&#39;a:Q&#39;, axis=alt.Axis(title=&#39;&#39;)), ).transform_calculate(a=hline_pos) return dots + loess + hline . newdata = norm.rvs(loc=20, scale=10, size=12000) data = get_power_dist(newdata, 250, 12000, 250) plot_power_graph(data) .",
            "url": "https://fabiangunzinger.github.io/blog/python/stats/2020/03/08/power.html",
            "relUrl": "/python/stats/2020/03/08/power.html",
            "date": " • Mar 8, 2020"
        }
        
    
  
    
        ,"post98": {
            "title": "Naive Bayes",
            "content": "We&#39;re interested in estimating $P(L|features)$, the probability of a label given a collection of features. . Bayes&#39;s formula helps us express this in terms of things we can measure in the data as: . $$P(L|features) = frac{P(features|L) * P(L)}{P(features)}$$ . To decide which label is more probable given a set of features, we can compute the posterior ratio of likelihoods: . $$ frac{P(L_1|features)}{P(L_2|features)} = frac{P(features|L_1) * P(L_1)}{P(features|L_2) * P(L_2)}$$. . To be able to do this, we need to specify a generative model (a hypothetical random process that generates the data) for each label in the data. Generating these models is the main task of training a Bayesian classifier. . A naive Bayesian classifier comes about by making naive assumptions about the nature of these models, which can, in principle, be quite complex. . Gaussian Naive Bayes . A simple approach is to assume that data from each label is drawn from a simple Gaussian distribution (with no covariance between them). We can then estimate the mean and standard deviation for each set of labelled points, which is all we need to estimate the distribution. Once we have the distribution, we can calculate the likelihood of being drawn from the distribution for each new data-point, and thus compute the posterior ratio above. . import numpy as np import matplotlib.pyplot as plt import seaborn as sns; sns.set() from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split X, y = make_blobs(500, 2, centers=2, random_state=1223, cluster_std=1.5) Xtrain, Xtest, ytrain, ytest = train_test_split(X, y) plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain, cmap=&quot;Paired&quot;); . from sklearn.naive_bayes import GaussianNB model = GaussianNB().fit(Xtrain, ytrain) # New data to show decision boundary rng = np.random.RandomState(1223) Xnew = [-5, -14] + [20, 20] * rng.rand(2000, 2) ynew = model.predict(Xnew) plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain, cmap=&quot;Paired&quot;, label=&#39;Training&#39;); plt.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest, marker=&#39;D&#39;, label=&#39;Testing&#39;); lim = plt.axis() plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=50, cmap=&quot;Paired&quot;, alpha=0.1) plt.axis(lim) plt.legend() plt.title(&#39;Training and testing data with decision boundary&#39;); . When to use? . - . Sources . Python data science handbook, Jake Vanderplas | .",
            "url": "https://fabiangunzinger.github.io/blog/python/ml/stats/2020/03/07/naive-bayes.html",
            "relUrl": "/python/ml/stats/2020/03/07/naive-bayes.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post99": {
            "title": "Model selection and assessment",
            "content": "Notes . The ultimate trade-off for model performance is between bias and variance. Simple models will have higher bias and lower variance, more complex models lower bias but higher variance (becuse they tend to overfit the training data). . Two things mainly determine model performance: . Model complexity (validation curves in PDSH) | Training data size (learning curves in PDSH) | . What to do when model is underperforming? . Use a more complicated/flexible model | Use a less complicated/flexible model | Gather more training samples (make data longer) | Gather more data for additional features for each sample (make data wider) | Train and test vs cross-validation . These approaches are complementary, as nicely explained in the Scikit-learn docs: the generall process is to split the data into a training and a testing dataset, and then to use cross-validation on the training data to find the optimal hyperparameters. | . Assessing model accuracy . Measuring the quality of fit . MSE for regressions. . | Error rate for classification. . | Beware of overfitting! Maximise mse or error rate for testing data, not for training data (overfitting). . | Overfitting definition: situation where a simpler model with a worse training score would have achieved a better testing score. . | . The bias-variance trade-off . MSE comprises (1) squared bias of estimate, (2) variance of estimate, and (3) variance of error. We want to minimise 1 and 2 (3 is fixed). . | Relationship between MSE and model complexity is U-shaped, because variance increases and bias decreases with complexity. We want to find optimal balance. . | . Classification setting . Bayesian classifier as unattainable benchmark (we don&#39;t know P(y|x)). . | KNN one approach to estimate P(y|x), then use bayesian classifier. . | Intuition as for MSE error: testing error rate is U-shaped in K (higher K means more flexibel model). . | . Cross validation . Use cases: . Model assessment (&quot;how good is model fit?&quot;) | Model selection (hypterparameter tuning) | . Holdout set approach . How it works: . Split data into training and test data, fit model on the training data, assess on the test data. | . Advantages: . Conceptually simple and computationally cheap. | . Disadvantages: . Very sensitive to train-test split, especially for small samples. | Because we only use a fraction of the available data to train the model, model fit is poorer comparated to using entire dataset, and thus we tend to overestimate test error. | . Leave one out cross-validation . How it works: . If we have n observations, fit model using n-1 observations and calculate MSE for nth observation. Repeat n times and then average MSEs to get test error rate. | . Advantage: . Deals with both problems of holdout set approach. | . Disadvantage: . Computationally expensive or prohibitive for large ns | . k-fold cross-validation . How it works: . Similar to LOOCV (which is a special case where k = n), but we split data into k groups (folds), and then use each of them in turn as the test training set while training the model on the remaining k-1 folds. We again average the k MSEs to get the overall test error rate. | . Advantage: . Computationally cheaper than LOOCV | Beats LOOCV in terms of MSE because it trades-off bias and variance in a more balanced way: LOOCV has virtually no bias (as we use almost the entire data to fit the model each time) but has higher variance because all k MSE estimates are highly correlated (because each model is fit with almost identical data). In contrast, training data for k-fold CV is less similar, meaning MSEs are less correlated, meaning reduction in variance from averaging is greater. | . Bootstrap . Use cases . Model assessment (&quot;how good is parameter fit?&quot;) | . How it works . To get an estimate for our model fit, we would ideally want to fit the model on many random population samples, so that we could then calculate means and standard deviations of our model assessment metric of interest. | We can&#39;t draw multiple samples from the population, but the bootstrap mimics this approach by repeatedly sampling (with replacement) from our original sample. | . Confusion matrix . import seaborn as sns; sns.set() import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix from sklearn.naive_bayes import GaussianNB data = load_breast_cancer() Xtrain, Xtest, ytrain, ytest = train_test_split(data.data, data.target, random_state=23124) model = GaussianNB() model.fit(Xtrain, ytrain) ymodel = model.predict(Xtest) mat = confusion_matrix(ytest, ymodel) sns.heatmap(mat, square=True, annot=True, cbar=False, cmap=&#39;Blues&#39;) plt.xlabel(&#39;Predicted value&#39;) plt.ylabel(&#39;True value&#39;); . Terminology . Can think of &quot;+&quot; as &quot;diseases&quot; or &quot;alternative hypothesis&quot;, and of &quot;-&quot; as &quot;no disease&quot; or &quot;null hypothesis&quot;. . Predicted - + . True | - | True negative (TN) | False positive (FP) | N | . | + | False negative (FN) | True positive (TP) | P | . | | N* | P* | . True positive rate: $ frac{TP}{TP + FN} = frac{TP}{P}$, is the proportion of positives that we correctly predict as such. Also called sensitivity, of hit rate, or recall. . | False positive rate: $ frac{FP}{FP + TN} = frac{FP}{N}$, is the proportion of negatives events we incorrectly predict as positives. Also called the alarm rate or inverted specificity, where specificity is $ frac{TN}{FP + TN} = frac{TN}{N}$. . | Precision: $ frac{TP}{TP + FP} = frac{TP}{P*}$. . | Precision and recall originate in the field of information retrieval (e.g. getting documents from a query). In its original context, precision is useful documents as a proportion of all retrieved documents, recall the retrieved useful documents as a proportion of all available useful documents. In the context of machine learning, we can think of precision as positive predictive power (how good is the model at predicting the positive class), while recall is the same as sensitivity -- the proportion of all events that were successfully predicted. . | . ROC curves and AUC . Plots the trade-off between the false positive rate (x-axis) and the true positive rate (y-axis) - the trade-off between the false alarm rate and the hit rate. . | The ROC is useful because it directly shows false/true negatives (on the x-axis) and false/true positives (on the y-axis) for different thresholds and thus helps choose the best threshold, and because the area under the curve (AUC) can be interpreted as an overall model summary, and thus allows us to compare different models. . | . import seaborn as sns; sns.set() import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import plot_roc_curve data = load_breast_cancer() Xtrain, Xtest, ytrain, ytest = train_test_split(data.data, data.target, random_state=23124) model = GaussianNB() model.fit(Xtrain, ytrain) ymodel = model.predict(Xtest) roc = plot_roc_curve(model, Xtest, ytest) rfc = RandomForestClassifier(n_estimators=10, random_state=42) rfc.fit(Xtrain, ytrain) ax = plt.gca() _ = plot_roc_curve(rfc, Xtest, ytest, ax=ax, alpha=0.8) roc.plot(ax=ax, alpha=0.8); . Precision-recall curves . The precision-recall curve is particularly useful when we have much more no-event than event cases. In this case, we&#39;re often not interested much in correctly predicting no-events but focused on correctly predicting events. Because neither precision nor recall use true negatives in their calculations, they are well suited to this context (paper). . | The precision recall curve plots precision (y-axis) and recall (x-axis). An unskilled model is a horizontal line at hight equal to the proportion of events in the sample. We can use ROC to compare models as different thresholds, or the F-score (the harmonic mean between precision and recall) at a specific threshold. . | Use precision-recall curves if classes are imbalanced, in which case ROC can be misleading (see example in last section here). . | Shape of curve: recall increases monotonically as we lower the threshold (move from left to right in the graph) because we&#39;ll find more and more true positives (&quot;putting ones from the false negative to the true positive bucket&quot;, which increases the numerator but leaves the denominator unchanged), but precision needn&#39;t fall monotonically, because we also increase false positives (both the numerator and the denominator increase as we lower the threshold, and the movement of recall depends on which increases faster, which depends on the sequence of ordered predicted events). See here for a useful example. . | . from sklearn.metrics import plot_precision_recall_curve data = load_breast_cancer() Xtrain, Xtest, ytrain, ytest = train_test_split(data.data, data.target, random_state=23124) model = GaussianNB() model.fit(Xtrain, ytrain) ymodel = model.predict(Xtest) plot_precision_recall_curve(model, Xtest, ytest); . Learning curves . Sources . An introduction to statistical learning | Hands on machine learning with scikit-learn, keras, and tenserflow | Python Data Science Handbook | The hundred-page machine learning book | .",
            "url": "https://fabiangunzinger.github.io/blog/python/ml/stats/2020/03/06/model-selection.html",
            "relUrl": "/python/ml/stats/2020/03/06/model-selection.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post100": {
            "title": "Feature engineering",
            "content": "General points . Having many uninformative features in your model might lower performance as it makes is more likely that the model overfits (e.g. including country names when predicting life satisfaction and using OECD sample finds that &quot;w&quot; in country name predicts high life satisfaction because of Norway, Switzerland, New Zealand, and Sweden. But this doesn&#39;t generalize to Rwanda and Zimbabwe). Hence, the more uninformative features, the bigger the chance that the model will find a pattern by chance in one of them in your training set. | . Process in practice . Brainstorm features | Decide what features to create | Create the features | Test impact on model performance | Interacting on features is useful | Iterate (go to 3 or 1) | Stuff to try: . Ratios | Counts | Cutoff points | Iterate on features (change cutoff and see whether it makes a difference) | Rate of Change in Values | Range of Values | Density within Intervals of Time | Proportion of Instances of Commonly Occurring Values | Time Between Occurrences of Commonly Occurring Values | . Data processing . Data transformations for individual predictors . | Data transformations for multiple predictors . | Dealing with missing values . | Removing predictors . | Adding predictors . | Binning predictors . | . Measuring predictor importance . Numeric outcomes . | Categorical outcomes . | Other approaches . | . Feature selection . Consequences of using non-informative predictors . | Approaches for reducing the number of predictors . | Wrapper methods . | Filter methods . | Selection bias . | . Factors that can affect model performance . Type III errors . | Measurement errors in the outcome . | Measurement errors in the predictors . | Distretising continuous outcomes . | When should you trust your model&#39;s prediction? . | The impact of a large sample . | . Resources . Applied predictive modeling | video | mlmastery article | .",
            "url": "https://fabiangunzinger.github.io/blog/ml/2020/03/05/feature-engineering.html",
            "relUrl": "/ml/2020/03/05/feature-engineering.html",
            "date": " • Mar 5, 2020"
        }
        
    
  
    
        ,"post101": {
            "title": "Experiments",
            "content": "My notes from working through section 3, experiments and significance testing, of Practical statistics for data science, to revise consepts and get comfortable implementing them in Python. . The basic experimental process: . Formulate a hypothesis | Design experiment | Collect data | Draw inferences and conclusions | . Hypothesis testing . The book makes a nice point about why we need hypothesis testing in the first place: to guard against our tendency to underestimate chance, and, as a result, to interprete observed differences as being driven by something &quot;real&quot; (e.g. our treatment). | A nice example is our intuition of what a random series of coin flips looks like: most people will switch between heads and tail too frequently, believing that five heads in a row are very unlikely in a series of 50 flips (which it is not). | . Resampling . Means to repeatedly sample values from observed data to assess variability in a statistic. | There are two main approaches: the bootstrap, usually used to assessed to reliability of an estimate (see distributions notebook), and permutation tests, used to test hypotheses. | . Permutation tests . Is a way to perform hypothesis testing while being agnostic about the outcome distribution. In a traditional hypothesis test, we assume that the outcome variable (e.g. the difference between treatment and control outcome) has a certain distribution (usually t or standard normal) and then see where in that distribution the observed outcome falls: if it&#39;s far out on either tail of the distribution, beyond some critical value (usually the 2.5th percentile on either end) we conclude that the result is unlikely to be produced by chance and reject the null hypothesis. | A permutation test works the same except that we don&#39;t compare our outcome to a theoretical distribution, but to a distribution we create. We create the distribution by pooling all outcome data, resampling (without replacement) to construct groups of the size of each treatment group, and then comparing the outcomes. This outcome is one observation. We then repeat this procedure R times to build a distribution of outcomes, and finally compare our original outcome to the distribution to see where it falls and how likely it is to be a product of chance. | Permutation tests come in three variants: random permutation tests, exhaustive permuation tests, and bootstrap permutation tests. The first is as described above, in the second we construct our permutatin samples such that they cover all possible cases we can construct, and in a bootstrap test we sample with replacement. The exhaustive test is sometimes clled &quot;exact test&quot; because it guarantees that we will not find a significant result more often than the alpha level of the test. Bootstrap resampling models not only the randomness in the treatment allocation to subjects (as random variation does) but also the randomness in the sampling of subjects from a population. | . import random import numpy as np import pandas as pd import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-darkgrid&#39;) %config InlineBackend.figure_format =&#39;retina&#39; # Create sample dataset np.random.seed(2312) sessions_a = np.random.normal(3.5, 1, 25) sessions_b = np.random.normal(4.5, 1, 15) session_times = np.concatenate([sessions_a, sessions_b ]) groups = [&#39;a&#39;] * 25 + [&#39;b&#39;] * 15 df = pd.DataFrame({&#39;group&#39;: groups, &#39;session_time&#39;: session_times}) df.sample(3) . group session_time . 4 a | 4.514161 | . 31 b | 4.621471 | . 2 a | 4.124164 | . mean_a = df[df[&#39;group&#39;] == &#39;a&#39;].session_time.mean() mean_b = df[df[&#39;group&#39;] == &#39;b&#39;].session_time.mean() mean_diff = mean_b - mean_a df.groupby(&#39;group&#39;).mean().plot(kind=&#39;bar&#39;, legend=None) print(f&#39;Mean difference in session times is {mean_diff:.2} seconds.&#39;) . Mean difference in session times is 0.51 seconds. . def random_perm(x, n_a, n_b): &quot;&quot;&quot; Return difference in x for random groups of size n_a and n_b (groups are sampled without replacement). &quot;&quot;&quot; n = n_a + n_b idx_a = set(random.sample(range(n), n_a)) idx_b = set(range(n)) - idx_a return x.loc[idx_a].mean() - x.loc[idx_b].mean() def bootstrap_perm(x, n_a, n_b): &quot;&quot;&quot; Return difference in x for random groups of size n_a and n_b (groups are sampled with replacement). &quot;&quot;&quot; n = n_a + n_b idx_a = random.choices(range(n), k=n_a) idx_b = random.choices(range(n), k=n_b) return x.loc[idx_a].mean() - x.loc[idx_b].mean() random_diffs = [random_perm(df[&#39;session_time&#39;], 25, 15) for _ in range(1000)] bootstrap_diffs = [bootstrap_perm(df[&#39;session_time&#39;], 25, 15) for _ in range(1000)] . fig, (ax0, ax1) = plt.subplots(1, 2, sharey=True, figsize=(10, 4)) ax0.hist(random_diffs, bins=15) ax0.axvline(mean_diff, color=&#39;orange&#39;, lw=2, ls=&#39;--&#39;) ax0.set_xlabel(&#39;Session time differences (in seconds)&#39;) ax0.set_ylabel(&#39;Frequency&#39;) ax1.hist(bootstrap_diffs, bins=15) ax1.axvline(mean_diff, color=&#39;orange&#39;, lw=2, ls=&#39;--&#39;) ax1.set_xlabel(&#39;Session time differences (in seconds)&#39;) p_random = sum(random_diffs &gt; mean_diff) / len(random_diffs) p_bootstrap = sum(bootstrap_diffs &gt; mean_diff) / len(bootstrap_diffs) print(f&#39;Random: P-value of observed difference is {p_random}&#39;) print(f&#39;Bootstrap: P-value of observed difference is {p_bootstrap}&#39;) . Random: P-value of observed difference is 0.051 Bootstrap: P-value of observed difference is 0.066 . Comparing resampling and formal testing . We have different conversion rates for two prices A and B as shown below. Test whether the difference in conversion rates is statistically significant at the 5 percent level. . Price A: conversion: 200; non-conversion 23539 . Price B: conversion: 182; non-conversion 22406 . a = [200, 23539]; b = [182, 22406] # [conversions, non-conversions] groups = [&#39;a&#39;] * sum(a) + [&#39;b&#39;] * sum(b) outcomes = [1] * a[0] + [0] * a[1] + [1] * b[0] + [0] * b[1] df = pd.DataFrame({&#39;group&#39;: groups, &#39;conv&#39;: outcomes}) df.head() . group conv . 0 a | 1 | . 1 a | 1 | . 2 a | 1 | . 3 a | 1 | . 4 a | 1 | . conv_a = df[df[&#39;group&#39;] == &#39;a&#39;].conv.mean() conv_b = df[df[&#39;group&#39;] == &#39;b&#39;].conv.mean() conv_diff = conv_a - conv_b print(f&#39;Difference in conversion rate is {conv_diff * 100:.3} percentage points.&#39;) . Difference in conversion rate is 0.0368 percentage points. . random_diffs = [random_perm(df[&#39;conv&#39;], sum(a), sum(b)) for _ in range(1000)] p_random = sum(random_diffs &gt; conv_diff) / len(random_diffs) print(f&#39;P-value is {p_random}&#39;) . P-value is 0.349 . from statsmodels.stats.proportion import proportions_ztest convs = np.array([a[0], b[0]]) obs = np.array([sum(a), sum(b)]) z, p = proportions_ztest(convs, obs, alternative=&#39;larger&#39;) print(f&#39;P-value is {p:.3}&#39;) . P-value is 0.331 . import scipy.stats chi2, p, df, E = scipy.stats.chi2_contingency([[a[0], a[1]], [b[0], b[1]]]) print(f&#39;P-value is {p / 2:.4}&#39;) . P-value is 0.3498 . outcomes_a = outcomes[:sum(a)] outcomes_b = outcomes[sum(a):] t, p = scipy.stats.ttest_ind(outcomes_a, outcomes_b, equal_var=False) print(f&#39;P-value is {p / 2:.4}&#39;) . P-value is 0.3309 . Multiple testing . If we use a significance level of 5 percent, then if we run 20 hypothesis tests the probability that we will at least make one Type I error (find a significant result purely due to chance) is $1 - 0.95^{20} = 0.64$. | This, in essence, captures the problem of multiple comparisons, which applies similarly to trying out different model specifications and models -- the more you try, the larger the probability that significant results appear due to chance. | Ways to guard against this: explicit corrections (like Bonferroni) in the case of multiple hypothesis tests, holdout sets in classification tasks, awareness of the problem in unsupervised learning tasks | . ANOVA . ANOVA decomposes the overall variance between groups into between- and within-group variance, and then constructs an F-test using the ratio of the two sources of variance to assess the likelihood that the observed between-group variance is a result of chance. | One use of this is to test whether means of multiple groups differ statistically (i.e. whether the variance between them is unlikely to result from chance). | The test is an omnibus test and tells us whether there is significant variation in the data overall (whether some group means are significantly different from others), but not which ones differ and by how much. | For this, we can conduct so-called post-hoc tests, which are simple pair-wise t-tests, usually with adjusted significance levels to account for the multiple testing problem. Two common approaches are Tukey&#39;s HSD test and Bonferroni&#39;s correction approach. | One way to straightforwardly perform an ANOVA analysis is to encode group indicators as dummies and then regress the outcome variable on the set of dummies (either dropping one and including an intercept, or including all dummies but omitting the intrecept). The F-test of the regression, which tests for whether the model as a whole (i.e. all estimators together) has explanatory power is exactly the ANOVA F-test. | As Peter Kennedy explains on p. 240 of his textbook, if you know multiple regression and understand the use of dummy variables, then there is no need to worry about ANOVA, because regression gives you ANOVA plus additional useful output (like parameter estimates for each group). But it&#39;s obviously worthwhile to know the link between the two. . | Helpful references: here . | . np.random.seed(2312) n = 10 groups = np.sort([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] * n) times_a = np.random.normal(5, 1, n) times_b = np.random.normal(4.5, 1, n) times_c = np.random.normal(4.25, 1, n) times_d = np.random.normal(4, 1, n) times = np.concatenate([times_a, times_b, times_c, times_d]) df = pd.DataFrame({&#39;group&#39;: groups, &#39;time&#39;: times}) df.head() observed_var = df.groupby(&#39;group&#39;).mean().var()[0] df.groupby(&#39;group&#39;).mean().plot(kind=&#39;bar&#39;, legend=None); print(f&#39;Observed variance of means is {observed_var:.2}.&#39;) . Observed variance of means is 0.36. . np.random.seed(231286) def perm_var(df): df = df.copy() df[&#39;time&#39;] = np.random.permutation(df[&#39;time&#39;].values) return df.groupby(&#39;group&#39;).mean().var()[0] p = np.mean([perm_var(df) &gt; observed_var for _ in range(1000)]) print(f&#39;P-value is {p:.3}&#39;) . P-value is 0.028 . from scipy import stats dataA = df[df[&#39;group&#39;] == &#39;a&#39;].time dataB = df[df[&#39;group&#39;] == &#39;b&#39;].time dataC = df[df[&#39;group&#39;] == &#39;c&#39;].time dataD = df[df[&#39;group&#39;] == &#39;d&#39;].time F, p = stats.f_oneway(dataA, dataB, dataC, dataD) print(f&#39;F-statistic is {F:.4}, p-value is {p:.3}&#39;) . F-statistic is 3.317, p-value is 0.0306 . x = pd.get_dummies(df[&#39;group&#39;]).values y = df[&#39;time&#39;].values sm.OLS(y, x).fit().summary() . OLS Regression Results Dep. Variable: y | R-squared: 0.217 | . Model: OLS | Adj. R-squared: 0.151 | . Method: Least Squares | F-statistic: 3.317 | . Date: Fri, 24 Jan 2020 | Prob (F-statistic): 0.0306 | . Time: 07:53:21 | Log-Likelihood: -56.023 | . No. Observations: 40 | AIC: 120.0 | . Df Residuals: 36 | BIC: 126.8 | . Df Model: 3 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . x1 5.4287 | 0.327 | 16.588 | 0.000 | 4.765 | 6.092 | . x2 4.5132 | 0.327 | 13.790 | 0.000 | 3.849 | 5.177 | . x3 4.4185 | 0.327 | 13.501 | 0.000 | 3.755 | 5.082 | . x4 4.0181 | 0.327 | 12.277 | 0.000 | 3.354 | 4.682 | . Omnibus: 0.738 | Durbin-Watson: 1.894 | . Prob(Omnibus): 0.691 | Jarque-Bera (JB): 0.746 | . Skew: 0.295 | Prob(JB): 0.689 | . Kurtosis: 2.686 | Cond. No. 1.00 | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Interpretation of output: . F(3, 36)-test is test statistics for ANOVA. | Durbin-Watson tests for autocorrelation in the residuals and takes values between 0 and 4, with 2 meaning no autocorrelation and values between [1.5, 2.5] considered to be relatively normal. Values below 2 indicate positive autocorrelation, values above 2 negative autocorrelation. Above, we have slight positive autocorrelation. | Omnibus tests for homogeneity of variance, which is not rejected above. | J-B tests for normality of the errors, which, too, is not rejected. | . Finally, we want to do pair-wise comparisons (post-hoc test in ANOVA lingo) to test for differerent effects among the different groups. . from statsmodels.stats.multicomp import pairwise_tukeyhsd from statsmodels.stats.multicomp import MultiComparison mc = MultiComparison(df[&#39;time&#39;], df[&#39;group&#39;]) print(mc.tukeyhsd()) . Multiple Comparison of Means - Tukey HSD, FWER=0.05 ==================================================== group1 group2 meandiff p-adj lower upper reject - a b -0.9155 0.2152 -2.162 0.331 False a c -1.0102 0.1475 -2.2567 0.2363 False a d -1.4107 0.0214 -2.6572 -0.1641 True b c -0.0947 0.9 -1.3412 1.1518 False b d -0.4952 0.6891 -1.7417 0.7514 False c d -0.4005 0.8021 -1.647 0.8461 False - . from itertools import combinations groups = df[&#39;group&#39;].unique() n_comparisons = len([c for c in combinations(groups, 2)]) alpha_adjusted = 0.05 / n_comparisons print(f&#39;Adjusted alpha is {alpha_adjusted:.3}&#39;) for a, b in combinations(groups, 2): t, p = stats.ttest_ind(df[&#39;time&#39;][df[&#39;group&#39;] == a], df[&#39;time&#39;][df[&#39;group&#39;] == b]) print(f&#39;{a} vs {b}: p-value: {p:.3}, reject H0: {p &lt; alpha_adjusted}&#39;) . Adjusted alpha is 0.00833 a vs b: p-value: 0.0621, reject H0: False a vs c: p-value: 0.0342, reject H0: False a vs d: p-value: 0.00297, reject H0: True b vs c: p-value: 0.855, reject H0: False b vs d: p-value: 0.32, reject H0: False c vs d: p-value: 0.401, reject H0: False . Chi-Square tests . Uses in data-science: . Determining appropriate sample size for web experiments where, despite large exposure, actual click rates are low (Fisher&#39;s exact test is recommended, here, look at &quot;Lady tasting tea&quot; example). | As a filter to check whether something is worth pursuing further: is crime concentrated in one area to a degree that is unlikely to result from chance? | . Resources: . Stat trek | . data = { &#39;counts&#39;: [14, 8, 12, 986, 992, 988], &#39;groups&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] * 2, &#39;col&#39;: np.sort([&#39;Clicks&#39;, &#39;NoClicks&#39;] * 3) } df = pd.DataFrame(data).pivot_table(index=&#39;col&#39;, columns=&#39;groups&#39;) df.columns = df.columns.droplevel() df . groups a b c . col . Clicks 14 | 8 | 12 | . NoClicks 986 | 992 | 988 | . def chi_squared(observed, expected): &quot;&quot;&quot; Returns Chi-Squared test statistic. &quot;&quot;&quot; pearson_residuals = (observed - expected) / np.sqrt(expected) chi_squared = np.sum(pearson_residuals) ** 2 return chi_squared observed = df.values expected = [observed[0].mean()] * len(observed[0]) + [observed[1].mean()] * len(observed[1]) observed = observed.flatten() chi_squared_observed = chi_squared(observed, expected) def chi_square_perm(box): clicks = [sum(random.sample(box, 1000)), sum(random.sample(box, 1000)), sum(random.sample(box, 1000))] noclicks = [1000 - n for n in clicks] observed = np.array([clicks, noclicks]).flatten() return chi_squared(observed, expected) box = [1] * df.loc[&#39;Clicks&#39;].sum() + [0] * df.loc[&#39;NoClicks&#39;].sum() perms = [chi_square_perm(box) for _ in range(100)] p = sum([perm &lt; chi_squared_observed for perm in perms]) / len(perms) print(f&#39;P-value is {p}.&#39;) . P-value is 0.04. . from scipy import stats chisq, p, dfs, exp = stats.chi2_contingency(df.values.T) print(f&#39;p-value: {pvalue:.4f}&#39;) . p-value: 0.4348 . Seems odd that the difference would be this large. But can&#39;t figure out what&#39;s going on. The test statistics differ widely, and I can&#39;t see why. Might look into this at some point. . Multi-arm bandit algorithms . A class of algorithms to decide to what extent and when to shift towards the dominant seeming treatment in an A/B test. | . Power basics . Based on this super helpful paper by Duflo, Glennerster, and Kremer. . . Power basics . In the simplest possible, we randomly draw a sample of size $N$ from an identical population, so that our observations can be assumed to be i.i.d, and we allocate a fraction $P$ of our sample to treatment. We can then estiamte the treatment effect using the OLS regression | . $$ y = alpha + beta T + epsilon$$ . where the standard error of $ beta$ is given by $ sqrt{ frac{1}{P(1-P} frac{ sigma^2}{N}}$. . | The distribution on the left hand side below shows the distribution of our effect size estimator $ hat{ beta}$ if the null hypothesis is true. . | We reject the null hypothesis if the estimated effect size is larger than the critical value $t_{ alpha}$, determined by the significance level $ alpha$. Hence, for this to happen we need $ hat{ beta} &gt; t_{ alpha} * SE( hat{ beta})$ | On the right is the distribution of $ hat{ beta}$ if the true effect size is $ beta$. | The power of the null hypothesis is the area under this curve to the left of $t_{ alpha}$ -- the probability that we reject the null if its false given the true effect size $ beta$. | Hence, to attain a power of $ kappa$ we need $ beta &gt; (t_a + t_{1- kappa}) * SE( hat{ beta})$. | This means that the minimum detectable effect ($ delta$) is given by: | . $$ delta = (t_a + t_{1- kappa}) * sqrt{ frac{1}{P(1-P)} frac{ sigma^2}{N}} $$ . Rearranding for the minimum required sample size we get: | . $$ N = frac{(t_a + t_{1- kappa})^2}{P(1-P)} left( frac{ sigma}{ delta} right)^2 $$ . So that the required sample size is inversele proportional to the minimal effect size we wish to detect. This makes sense, it means that the smaller an effect we want to detect, the larger the samle size we need. In particular, given that $N propto delta^{-2}$, to detect an effect of half the size we need a sample four times the size. | . Sequential experiments . Based on this blog post from deliveroo engineering and this paper referenced therein. . In practice, the above has two main consequences: detecting small effects requires large samples and thus long experiments (if we can increase sample size by running the experiment for longer), and the minimal detectable effect we decide to callibrate the experiment for is crucially important, even if we might not have a good sense of what the effect of the treatment is. | Sequential testing allows us to check for results along the way and stop the experiment early in case the effect is much larger (or smaller) than expected while accounting for multiple comparisons. . | The main idea is to use the idea of a family-wise error-rate (the error rate of concucting a family rather than a single hyopthesis test) and applying it to conducting hypothesis tests at different points in time. . | .",
            "url": "https://fabiangunzinger.github.io/blog/python/stats/2020/03/04/experiments.html",
            "relUrl": "/python/stats/2020/03/04/experiments.html",
            "date": " • Mar 4, 2020"
        }
        
    
  
    
        ,"post102": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://fabiangunzinger.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post103": {
            "title": "New post",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . plt.plot([0, 1]) . [&lt;matplotlib.lines.Line2D at 0x1a196b0a10&gt;] . Sources . Fluent Python | Python Cookbook | Learning Python | The Hitchhiker&#39;s Guide to Python | Effective Python | Python for Data Analysis | Python Data Science Handbook | Pandas cookbook | Numpy docs . | An introduction to statistical learning . | Applied predictive modeling | Hands on machine learning with scikit-learn, keras, and tenserflow | The hundred-page machine learning book | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2000/01/01/new.html",
            "relUrl": "/python/2000/01/01/new.html",
            "date": " • Jan 1, 2000"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi. I’m a PhD student at Warwick Business School, where I use insights from behavioural economics and tools from econometrics and machine learning to understand consumer financial behaviour from mass transaction data. .",
          "url": "https://fabiangunzinger.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fabiangunzinger.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}