{
  
    
        "post0": {
            "title": "Splitting a large file in Python",
            "content": "https://www.wefearchange.org/2013/05/resource-management-in-python-33-or.html . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/12/10/splitting-file.html",
            "relUrl": "/python/2020/12/10/splitting-file.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Regular expressions",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . path = &#39;/Users/fgu/tmp/data_777.csv&#39; df = pd.read_csv(path, sep=&#39;|&#39;) . a = &#39;b&#39; f&#39;he {a}&#39; . &#39;he b&#39; . [str(n) for n in range(1, 10)] . [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] . with open(path) as f: next(f) line = f.readline() pattern = &#39;&quot; d+&quot; |&quot;(?P&lt;user_id&gt; d+)&quot;&#39; match = re.match(pattern, line) print(line) print(match) print(match.group(&#39;user_id&#39;)) . &#34;688293&#34;|&#34;777&#34;|&#34;2011-07-20&#34;|&#34;1969&#34;|&#34;20K to 30K&#34;|&#34;WA1 4&#34;|&#34;E01012553&#34;|&#34;E02002603&#34;|&#34;M&#34;|&#34;2012-01-25&#34;|&#34;262916&#34;|&#34;NatWest Bank&#34;|&#34;Current&#34;|&#34;364.22&#34;|&#34;9572 24jan12 , tcs bowdon , bowdon gb - pos&#34;|&#34;Debit&#34;|&#34;25.03&#34;|&#34;No Tag&#34;|&#34;No Tag&#34;|&#34;No Tag&#34;|&#34;No Merchant&#34;|&#34;Unknown Merchant&#34;|&#34;2011-07-20&#34;|&#34;2020-07-21 20:32:00&#34;|&#34;2014-07-18&#34;|&#34;2017-10-24&#34;|&#34;U&#34; &lt;re.Match object; span=(0, 14), match=&#39;&#34;688293&#34;|&#34;777&#34;&#39;&gt; 777 . re.Match.group . def colname_cleaner(df): &quot;&quot;&quot;Convert column names to stripped lowercase with underscores.&quot;&quot;&quot; df.columns = df.columns.str.lower().str.strip() return df def str_cleaner(df): &quot;&quot;&quot;Convert string values to stripped lowercase.&quot;&quot;&quot; str_cols = df.select_dtypes(&#39;object&#39;) for col in str_cols: df[col] = df[col].str.lower().str.strip() return df movies = (data.movies() .pipe(colname_cleaner) .pipe(str_cleaner)) movies.head(2) . title us gross worldwide gross us dvd sales production budget release date mpaa rating running time min distributor source major genre creative type director rotten tomatoes rating imdb rating imdb votes . 0 the land girls | 146083.0 | 146083.0 | NaN | 8000000.0 | jun 12 1998 | r | NaN | gramercy | None | None | None | None | NaN | 6.1 | 1071.0 | . 1 first love, last rites | 10876.0 | 10876.0 | NaN | 300000.0 | aug 07 1998 | r | NaN | strand | None | drama | None | None | NaN | 6.9 | 207.0 | . import re . Finding a single pattern in text . pattern = &#39;hello&#39; text = &#39;hello world it is a beautiful day.&#39; match = re.search(pattern, text) match.start(), match.end(), match.group() . (0, 5, &#39;hello&#39;) . In Pandas . movies.title.str.extract(&#39;(love)&#39;) . 0 . 0 NaN | . 1 love | . 2 NaN | . 3 NaN | . 4 NaN | . ... ... | . 3196 NaN | . 3197 NaN | . 3198 NaN | . 3199 NaN | . 3200 NaN | . 3201 rows × 1 columns . contains(): Test if pattern or regex is contained within a string of a Series or Index. | match(): Determine if each string starts with a match of a regular expression. | fullmatch(): | extract(): Extract capture groups in the regex pat as columns in a DataFrame. | extractall(): Returns all matches (not just the first match). | find(): | findall(): | replace(): | . movies.title.replace(&#39;girls&#39;, &#39;hello&#39;) . 0 the land girls 1 first love, last rites 2 i married a strange person 3 let&#39;s talk about sex 4 slam ... 3196 zack and miri make a porno 3197 zodiac 3198 zoom 3199 the legend of zorro 3200 the mask of zorro Name: title, Length: 3201, dtype: object . Let&#39;s drop all movies by distributors with &quot;Pictures&quot; and &quot;Universal&quot; in their title. . names = [&#39;Universal&#39;, &#39;Pictures&#39;] pattern = &#39;|&#39;.join(names) mask = movies.distributor.str.contains(pattern, na=True) result = movies[~mask] result.head(2) . title us_gross worldwide_gross us_dvd_sales production_budget release_date mpaa_rating running_time_min distributor source major_genre creative_type director rotten_tomatoes_rating imdb_rating imdb_votes . 0 The Land Girls | 146083.0 | 146083.0 | NaN | 8000000.0 | Jun 12 1998 | R | NaN | Gramercy | None | None | None | None | NaN | 6.1 | 1071.0 | . 1 First Love, Last Rites | 10876.0 | 10876.0 | NaN | 300000.0 | Aug 07 1998 | R | NaN | Strand | None | Drama | None | None | NaN | 6.9 | 207.0 | . names = [&#39;Universal&#39;, &#39;Pictures&#39;] pattern = &#39; |&#39;.join(names) neg_pattern = f&#39;[^{pattern}]&#39; neg_pattern mask = movies.distributor.str.contains(neg_pattern, na=False) result2 =movies[mask] . neg_pattern . &#39;[^Universal |Pictures]&#39; . result == result2 . ValueError Traceback (most recent call last) &lt;ipython-input-114-1dac580d3c6f&gt; in &lt;module&gt; -&gt; 1 result == result2 ~/miniconda3/envs/habits/lib/python3.7/site-packages/pandas/core/ops/__init__.py in f(self, other) 837 if not self._indexed_same(other): 838 raise ValueError( --&gt; 839 &#34;Can only compare identically-labeled DataFrame objects&#34; 840 ) 841 new_data = dispatch_to_series(self, other, op, str_rep) ValueError: Can only compare identically-labeled DataFrame objects . def drop_card_repayments(df): &quot;&quot;&quot;Drop card repayment transactions from current accounts.&quot;&quot;&quot; tags = [&#39;credit card repayment&#39;, &#39;credit card payment&#39;, &#39;credit card&#39;] pattern = &#39;|&#39;.join(tags) mask = df.auto_tag.str.contains(pattern) &amp; df.account_type.eq(&#39;current&#39;) return df[~mask] . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/12/10/regex.html",
            "relUrl": "/python/2020/12/10/regex.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Profiling",
            "content": "We should forget about small efficiencies, say about 97% of the time:premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. -- Donald Knuth Takeaway: optimise where it matters. And to know where it matters, you need to profile your code. . from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Intro . What&#39;s efficient code? . Fast (minimal completion time) | Has no unnecessary memory overhead (minimal resource consumption) | . Profiling runtime . Optimise slow lines inside slow function. | . Finding slow functions (IPython implementation): time.time() decorator (%time, or %timeit for more precision) | cProfile (%prun) | Snakeviz is helpful for cProfile results visualisation | . | . Finding slow lines: line_profiler (%lprun) | . | . Best practices: . Check overall CPU usage during profiling (e.g. use activity monitor on Mac) to make sure that no other processes are influencing my results (e.g. Dropbox update). . | Form a hypothesis about what parts of the code are slow and then compare to the profiling results to improve your intuition over time. . | Start with quick and dirty profiling to zoom into the relevant area (e.g. use a timer decorator, use %time instead of %timeit in Jupyter) before doing more costly profiling. . | | . | . Useful: . https://www.machinelearningplus.com/python/cprofile-how-to-profile-your-python-code/ | . Things to know . cProfile() doesn&#39;t seem to work with multiprocessing. | . Sources . High Performance Python | Effective Python | Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/12/10/profiling.html",
            "relUrl": "/python/pandas/2020/12/10/profiling.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Numpy essentials",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Questions . What&#39;s the main difference between a Python list and a NumPy (or C) array? What is the main trade-off involved? | Do array slices return a view or a copy? What can this be useful for? | What are four methods to access and modify data in an array? | What two features make NumPy arrays more efficient storage containers than lists? | . Answers . The overhead: Python lists are references to objects, each of which has information like type, size, memory location, etc. NumPy arrays can only store a single data type and thus don&#39;t need this extra overhead for each element. The tradeoff is flexibility vs speed: Python lists are very flexible (can store different types) but slow, NumPy arrays can only store homogenous data but are fast. | A view, which allows you to operate on a subset of a large dataset without copying it. | Indexing, slicing, boolean indexing, fancy indexing. | They have less overhead (see above) and elements are stored in memory contiguously, which means they can be read faster. | . True of false? . Indexing and reshaping . a = np.arange(1, 10) b = a.reshape(3, 3) b[0, 0] = 99 a[0] == 1 . False . a = np.arange(3) (a.reshape(1, 3) == a[:, np.newaxis]).all() . False . Indexing and fancy indexing . X = np.arange(12).reshape(3, 4) X . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . row = np.array([1, 2]) col = np.array([2, 3]) (X[row, col] == [6, 10]).all() . False . mask = np.array([1, 0, 1]) rows = np.array([1, 2]) (X[rows[:, np.newaxis], mask] == [[5, 4, 5], [9, 8, 9]]).all() . True . x = np.zeros(3) idx = [0, 1, 1, 1] x[idx] += 1 y = np.zeros_like(x) np.add.at(y, idx, 1) (x == y).all() . False . x = np.array([4, 3, 1, 5, 2]) (x[np.argsort(x)] == np.sort(x)).all() . True . x = np.arange(1, 5) all(x.reshape(-1, 1) == x.reshape(len(x), 1)) . True . np.arange(2).repeat([1, 2]) . array([0, 1, 1]) . x = np.arange(2) all(x.repeat(2) == np.tile(x, 2)) . False . all(np.arange(2).repeat([1, 2]) == np.array([0, 1, 1])) . True . a = np.arange(5) i = [0, 4] a.put(i, [99, 88]) all(a.take(i) == [99, 88]) . True . a = np.array([[10, 30], [70, 90]]) imax = np.expand_dims(np.argmax(a, axis=1), axis=1) np.put_along_axis(a, imax, 99, axis=1) (a == [[10, 30], [70, 97]]).all() . False . Summing values . Lessons: . NumPy methods perform best on NumPy arrays; builtins, on lists. | np.add.reduce() is twice as fast as np.sum() | . mylist, myrange = range(1000), np.arange(1000) . %timeit np.add.reduce(myrange) . 1.47 µs ± 21.8 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) . %timeit np.sum(myrange) . 3.6 µs ± 87.6 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) . %timeit sum(mylist) . 12.4 µs ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each) . %timeit functools.reduce(operator.add, mylist) . 45 µs ± 1.33 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) . Broadcasting . First, it&#39;s helpful to know how NumPy labels dimensions. The image below makes this clear. . . Image from Elegant SciPy . Next, let&#39;s look at how broadcasting works: . . Image from Python Data Science Handbook . Determining broadcasting compatibility: . Comparing dimensions from right to left, ignoring different number of dimensions, arrays are compatible if the dimension of either array is 1 or if they match. | . 1D . x = np.arange(1, 5).reshape(-1, 1) y = np.arange(2, 6).reshape(1, -1) x * y . array([[ 2, 3, 4, 5], [ 4, 6, 8, 10], [ 6, 9, 12, 15], [ 8, 12, 16, 20]]) . 2D . a = np.arange(12).reshape(4, -1) # demeaning columns a - a.mean(0) # demeaning rows row_means = a.mean(1) a - row_means[:, np.newaxis] . array([[-1., 0., 1.], [-1., 0., 1.], [-1., 0., 1.], [-1., 0., 1.]]) . 3D . rng = np.random.default_rng(2312) a = rng.integers(1, 10, size=24).reshape(4, 3, 2) a . array([[[1, 5], [9, 8], [4, 3]], [[6, 5], [8, 8], [8, 3]], [[6, 8], [6, 5], [7, 6]], [[1, 9], [6, 7], [6, 5]]]) . Taking means along a dimension . What does it mean to take the mean of the zeroth dimension? It means to &quot;collapse&quot; that dimension down to size 0 or to &quot;flatten&quot; the array in that dimension, and to take the mean of each &quot;stack&quot; of values that was flattened. In the image above, imagine pressing down vertically from the top of the array, flattening the zeroth dimension and ending up with a shape of (3, 2). As we do this, we take the mean of each of the 3 x 2 = 6 stacks of values that we compressed on our way down. These are the means of dimension zero. In our array a, the stack in the top right corner, [0, 0], is [1, 6, 6, 1], with a mean of 3.5, so the [0, 0] element of our new flattened shape is 9, as we can see below. . mean0 = a.mean(0) mean0 . array([[3.5 , 6.75], [7.25, 7. ], [6.25, 4.25]]) . Demeaning an axis . To demean axis 0 of a we can now simply subtract our means from the original shape. (If we were to take the mean again, each element in the resulting (3, 2) array would be 0, so demeaning worked.) . (a - mean0) . array([[[-2.5 , -1.75], [ 1.75, 1. ], [-2.25, -1.25]], [[ 2.5 , -1.75], [ 0.75, 1. ], [ 1.75, -1.25]], [[ 2.5 , 1.25], [-1.25, -2. ], [ 0.75, 1.75]], [[-2.5 , 2.25], [-1.25, 0. ], [-0.25, 0.75]]]) . Broadcasting along an axis . We just did this above. The reason demeaning worked is because by broadcasting rules, our smaller (3, 2) array of means got padded in position 0 to (1, 3, 2), and then broadcasted (stretched) along the first dimension to match the (4, 3, 2) shape of a. Thus, to broadcast along axis 0, we need an array of shape (3, 2) or (1, 3, 2). Similarly, to broadcast along axis 1, we need an array of shape (4, 1, 2); to broadcast along axis 2, an array of shape (4, 3, 1). To practice, let&#39;s demean axis 1. . mean1 = a.mean(1) mean1 . array([[4.66666667, 5.33333333], [7.33333333, 5.33333333], [6.33333333, 6.33333333], [4.33333333, 7. ]]) . This has shape (4, 2), which means we can&#39;t demean directly. . a - mean1 . ValueError Traceback (most recent call last) &lt;ipython-input-92-5a9fe4bf9cf9&gt; in &lt;module&gt; -&gt; 1 a - mean1 ValueError: operands could not be broadcast together with shapes (4,3,2) (4,2) . Why didn&#39;t this work? The smaller (4, 2) array got padded in position 0 to (1, 4, 2), which can&#39;t be expanded to match the (4, 3, 2) shape of a. What we need -- and we already knew this -- is an array of shape (4, 1, 2). We can produce one by simply adding a dimension to our mean array. . mean1 = mean1[:, np.newaxis, :] mean1 . array([[[4.66666667, 5.33333333]], [[7.33333333, 5.33333333]], [[6.33333333, 6.33333333]], [[4.33333333, 7. ]]]) . Visually, in the image above, we have now separated out the two means of each of the four layers of values, and can now broadcast these along dimension 1. Another way of thinking about this is to think of each of the our layers as its own 2D shape. In that case, all we did was calculate column means by calculating means along the first axis (just as we would in a single 2D shape, except that the first axis there would be axis 0 instead of 1), and now we are broadcasting these mean values along the rows to demean the columns. . a - mean1 . array([[[-3.66666667, -0.33333333], [ 4.33333333, 2.66666667], [-0.66666667, -2.33333333]], [[-1.33333333, -0.33333333], [ 0.66666667, 2.66666667], [ 0.66666667, -2.33333333]], [[-0.33333333, 1.66666667], [-0.33333333, -1.33333333], [ 0.66666667, -0.33333333]], [[-3.33333333, 2. ], [ 1.66666667, 0. ], [ 1.66666667, -2. ]]]) . Again, we can see that the mean of each of the 4 x 2 = 8 blocks of size 3 is 0, just as we&#39;d expect. . Just as an exercise, let&#39;s demean axis 2, too: . a - a.mean(2)[:, :, np.newaxis] . array([[[-2. , 2. ], [ 0.5, -0.5], [ 0.5, -0.5]], [[ 0.5, -0.5], [ 0. , 0. ], [ 2.5, -2.5]], [[-1. , 1. ], [ 0.5, -0.5], [ 0.5, -0.5]], [[-4. , 4. ], [-0.5, 0.5], [ 0.5, -0.5]]]) . Setting array values . arr = np.zeros((4, 3)) arr[:] = 5 arr . array([[5., 5., 5.], [5., 5., 5.], [5., 5., 5.], [5., 5., 5.]]) . columns = np.array([1, 2, 3, 4]) arr[:] = columns[:, np.newaxis] arr . array([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.], [4., 4., 4.]]) . arr[:2] = [[8], [9]] arr . array([[8., 8., 8.], [9., 9., 9.], [3., 3., 3.], [4., 4., 4.]]) . Understanding ravel() . Basics . Ravel (meaning to untangle) flattens arrays. | ravel() returns a one dimensional array (i.e. a list) of the values of the input array in the specified order. | Basically: row major order (also C order) proceeds row-wise, column major order (also Fortran order) proceeds column-wise. | NumPy usually stores arrays in row-order, so that ravel() can produce a view without the need to produce a copy. If an array is stored differently (maybe because it was created from a slice), ravel() might have to produce a copy first. | Especially when working with higher dimensional data, the following is a helpful reminder of how order determines the result: row major order traverses data from high to low dimensions; column major, from low to high dimensions. | . a = np.array([[1, 2, 3], [4, 5, 6]]) a.ravel() . array([1, 2, 3, 4, 5, 6]) . Different order types . a.ravel(order=&#39;C&#39;) . array([1, 2, 3, 4, 5, 6]) . a.ravel(order=&#39;F&#39;) . array([1, 4, 2, 5, 3, 6]) . a.ravel(order=&#39;A&#39;) . array([1, 2, 3, 4, 5, 6]) . a.ravel(order=&#39;K&#39;) . array([1, 2, 3, 4, 5, 6]) . (a.ravel() == a.flatten()).all() and (a.flatten() == a.reshape(-1)).all() . True . Then on to https://stackoverflow.com/questions/38143717/groupby-in-python-pandas-fast-way . x = np.arange(8).reshape(2, 2, 2) . array([[[0, 1], [2, 3]], [[4, 5], [6, 7]]]) . x.ravel() . array([0, 1, 2, 3, 4, 5, 6, 7]) . x.ravel(order=&#39;F&#39;) . array([0, 4, 2, 6, 1, 5, 3, 7]) . ravel_multi_index() . Fluency exercises . Draw and plot a 100 points from a multivariate normal distribution and highlight a random sample of 20 . rng = np.random.default_rng(2312) mean = [0, 0] cov = [[1, 2], [2, 5]] data = rng.multivariate_normal(mean, cov, size=100) sample = rng.choice(data, 20) fmt = dict(s=150, facecolor=&#39;none&#39;, edgecolor=&#39;green&#39;) plt.scatter(data[:,0], data[:,1], alpha=0.5) plt.scatter(sample[:,0], sample[:,1], alpha=0.5, **fmt); . Manually draw a histogram of 1000 random values using only numpy functions and plt.plot(). . rng = np.random.default_rng(2312) x = rng.normal(size=1000) bins = np.linspace(-5, 5, 20) counts = np.zeros_like(bins) i = np.searchsorted(bins, x) np.add.at(counts, i, 1) plt.plot(bins, counts, drawstyle=&#39;steps&#39;); . Write a simple selection sort algorithm . def selection_sort(x): for i in range(len(x)): swap = i + np.argmin(x[i:]) x[i], x[swap] = x[swap], x[i] return x x = np.array([1, 4, 3, 2, 5]) selection_sort(x) . array([1, 2, 3, 4, 5]) . Use np and plt functions to calculate the k nearest neighbours of 10 random points and visualise the result . rng = np.random.default_rng(2312) x = rng.random(size=(10, 2)) # calculate squared distances squared_dist = ((x[np.newaxis, :, :] - x[:, np.newaxis, :]) ** 2).sum(-1) # identify k nearest neighbhours k = 2 nearest_partition = np.argpartition(squared_dist, k + 1) # plot points and connections to nearest neighbours plt.scatter(x[:, 0], x[:, 1]) for i in range(len(x)): for j in nearest_partition[i, :k + 1]: plt.plot(*zip(x[i], x[j]), color=&#39;green&#39;) . Understanding the solution . The most mind-bending bit for me is definitely the calculation of the point-wise differences. To wrap my head around this, I start with a simple case: . a = np.arange(5) a . array([0, 1, 2, 3, 4]) . To get the element-wise differences for a one-dimensional array, it&#39;s easier to visualise what to do: turn the array into a column and row vector and then use broadcasting to calculate each of the differences (broadcasting will expand each of the vectors into a 5x5 matrix, and then perform element-wise matrix subtraction). . diffs = a[:, np.newaxis] - a[np.newaxis, :] diffs . array([[ 0, -1, -2, -3, -4], [ 1, 0, -1, -2, -3], [ 2, 1, 0, -1, -2], [ 3, 2, 1, 0, -1], [ 4, 3, 2, 1, 0]]) . From here, we can then square the values and retrieve the nearest neighbour for each element of the original array: . k = 2 squared_diffs = diffs ** 2 nearest_partition = np.argpartition(squared_diffs, k + 1, axis=1) nearest_partition . array([[1, 0, 2, 3, 4], [1, 2, 0, 3, 4], [3, 2, 1, 0, 4], [3, 2, 4, 1, 0], [3, 4, 2, 1, 0]]) . I still occasionally get tripped up when interpreting the above: focusing on the first row, the first element, 1, says that in the partitioned array, element 1 from the squared_diffs array will be in position zero. The next element, 0, says that element zero from squared_diffs will be in position one, and so on. With this, we can now extract the k nearest neighbours for each element in our original array. . for i in range(len(a)): neighbours = [] for j in nearest_partition[i, :k + 1]: if a[j] != a[i]: neighbours.append(a[j]) print(a[i], neighbours) . 0 [1, 2] 1 [2, 0] 2 [3, 1] 3 [2, 4] 4 [3, 2] . To understand how to find nearest neighbouts in three dimensions, let&#39;s start with a toy example. . a = np.arange(4).reshape(2, 2) a . array([[0, 1], [2, 3]]) . Intuitively, what we want to do is build two cubes -- one built from the original array broadcasted vertically and one from the original array flipped on one of its edges and then broadcasted horizontally -- and then perform element-wise subtraction. Or, more acurately, we want to create arrays such that broadcasting builds appropriate cubes for us before it performs the element-wise subtraction. But for intuitions sake, let&#39;s build the cubes ourselves, so we can see what happens. . vertical_cube = np.broadcast_to(a[np.newaxis, :, :], (2, 2, 2)) vertical_cube . array([[[0, 1], [2, 3]], [[0, 1], [2, 3]]]) . horizontal_cube = np.broadcast_to(a[:, np.newaxis, :], (2, 2, 2)) horizontal_cube . array([[[0, 1], [0, 1]], [[2, 3], [2, 3]]]) . a[:, None, :] - a[:, :, None] . array([[[ 0, 1], [-1, 0]], [[ 0, 1], [-1, 0]]]) . Find index of values in array . From here . X = np.array([[4, 2], [9, 3], [8, 5], [3, 3], [5, 6]]) tofind = np.array([[4, 2], [3, 3], [5, 6]]) # desired result: [0, 3, 4] . result = [] for x in tofind: idx = np.argwhere([np.all((X - x) == 0, axis=1)])[0][1] result.append(idx) result . [0, 3, 4] . ... . Ellipsis . Sources . Python Data Science Handbook | Python for Data Analysis | Elegant SciPy | Fluent Python | Python Cookbook | Learning Python | The Hitchhiker&#39;s Guide to Python | Effective Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/numpy/2020/12/10/numpy-essentials.html",
            "relUrl": "/python/numpy/2020/12/10/numpy-essentials.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Misc learning",
            "content": "import numpy as np import scipy as sp import statsmodels.api as sm from statsmodels.formula.api import ols import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import seaborn as sns # sns.set_context(&quot;poster&quot;) # sns.set(rc={&#39;figure.figsize&#39;: (16, 9.)}) sns.set_style(&quot;whitegrid&quot;) import pandas as pd pd.set_option(&#39;display.max_rows&#39;, 50) pd.set_option(&#39;display.max_columns&#39;, 120) pd.set_option(&#39;max_colwidth&#39;, None) from sklearn.model_selection import train_test_split from sklearn.linear_model import RidgeClassifier from sklearn.linear_model import LogisticRegression from xgboost import XGBClassifier from sklearn.metrics import plot_roc_curve from sklearn.metrics import plot_precision_recall_curve from stargazer.stargazer import Stargazer from IPython.core.display import HTML from habits.cleaning import list_s3_files from habits.processing import read_o2_data, classify_users, make_yX, print_info from habits.modelling import make_stargazer, make_roc %load_ext autoreload %autoreload 2 . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-2df7d6552618&gt; in &lt;module&gt; 29 from IPython.core.display import HTML 30 &gt; 31 from habits.cleaning import list_s3_files 32 from habits.processing import read_o2_data, classify_users, make_yX, print_info 33 from habits.modelling import make_stargazer, make_roc ModuleNotFoundError: No module named &#39;habits&#39; . Creating datetime indices . from dateutil.parser import parse parse(&#39;3 Apr 2020&#39;).month parse(&#39;3.4.2020&#39;).month parse(&#39;3.4.2020&#39;, dayfirst=True).month . 4 . 3 . 4 . dates = pd.date_range(start=&#39;1/1/2000&#39;, freq=&#39;A-DEC&#39;, periods = 100) values = np.random.randn(100) ts = pd.Series(values, index=dates) ts.head(10) . 2000-12-31 0.843433 2001-12-31 -0.391251 2002-12-31 0.149087 2003-12-31 0.086131 2004-12-31 -2.308920 2005-12-31 -0.569420 2006-12-31 0.033575 2007-12-31 0.449340 2008-12-31 0.846790 2009-12-31 0.633025 Freq: A-DEC, dtype: float64 . idx = pd.period_range(&#39;2018-1&#39;, &#39;2019-12&#39;, freq=&#39;Q-DEC&#39;) s = pd.Series(np.random.randn(len(idx)), index=idx) s.asfreq(&#39;d&#39;, how=&#39;start&#39;).asfreq(&#39;Q&#39;) . 2018Q1 -0.205351 2018Q2 -0.673207 2018Q3 -0.872625 2018Q4 2.045383 2019Q1 -0.696708 2019Q2 -0.798782 2019Q3 -1.904917 2019Q4 -0.436799 Freq: Q-DEC, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;M&#39;, kind=&#39;period&#39;).mean() . 2000-01 0.014726 2000-02 0.056242 2000-03 -0.016878 2000-04 -0.690987 Freq: M, dtype: float64 . idx = pd.date_range(&#39;2000&#39;, freq=&#39;H&#39;, periods=100) s = pd.Series(np.random.randn(len(idx)), index=idx) s.resample(&#39;d&#39;).ohlc() . open high low close . 2000-01-01 -1.945039 | 2.231004 | -2.751270 | 0.272018 | . 2000-01-02 0.335163 | 2.173964 | -0.895301 | 0.995886 | . 2000-01-03 0.771501 | 1.399791 | -2.465960 | -0.030100 | . 2000-01-04 1.765987 | 1.765987 | -1.698415 | -0.056522 | . 2000-01-05 0.215849 | 1.192958 | 0.215849 | 0.786069 | . s.resample(&#39;min&#39;).asfreq().ffill() . 2000-01-01 00:00:00 -1.945039 2000-01-01 00:01:00 -1.945039 2000-01-01 00:02:00 -1.945039 2000-01-01 00:03:00 -1.945039 2000-01-01 00:04:00 -1.945039 ... 2000-01-05 02:56:00 1.192958 2000-01-05 02:57:00 1.192958 2000-01-05 02:58:00 1.192958 2000-01-05 02:59:00 1.192958 2000-01-05 03:00:00 0.786069 Freq: T, Length: 5941, dtype: float64 . data = df.reset_index(level=0).head(100).sort_index()[[&#39;amount&#39;]] data . amount . transaction_date . 2012-08-29 12.00 | . 2012-08-30 13.50 | . 2012-08-30 7.44 | . 2012-08-30 7.44 | . 2012-08-30 13.50 | . ... ... | . 2012-12-27 135.00 | . 2012-12-27 3.60 | . 2012-12-27 8.50 | . 2012-12-27 135.00 | . 2012-12-27 8.50 | . 100 rows × 1 columns . IPython / notebook shortcuts . clean_nb = !ls *2* clean_nb . [&#39;2.0-fgu-clean-and-split-data.ipynb&#39;] . %%timeit a = range(1000) . 174 ns ± 4.23 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) . %%writefile pythoncode.py import numpy def append_if_not_exists(arr, x): if x not in arr: arr.append(x) def some_useless_slow_function(): arr = list() for i in range(10000): x = numpy.random.randint(0, 10000) append_if_not_exists(arr, x) . Writing pythoncode.py . %pycat pythoncode.py . import numpy def append_if_not_exists(arr, x): if x not in arr: arr.append(x) def some_useless_slow_function(): arr = list() for i in range(10000): x = numpy.random.randint(0, 10000) append_if_not_exists(arr, x) . . I can transfer variables with any content from one notebook to the next (useful if you run a number of notebooks in sequence, for instance, and the ouput of one serves as the input of another). . var_to_pass_on = dfu.head() %store var_to_pass_on . Stored &#39;var_to_pass_on&#39; (DataFrame) . %store -r var_to_pass_on var_to_pass_on . user_id year_of_birth user_registration_date salary_range postcode gender soa_lower soa_middle user_uid . 0 3706 | 1967-01-01 | 2012-09-30 | NaN | XXXX 0 | M | NaN | NaN | 3706-0 | . 1 1078 | 1964-01-01 | 2011-11-29 | 20K to 30K | M25 9 | M | E01005038 | E02001043 | 1078-0 | . 2 232 | 1965-01-01 | 2010-09-09 | 30K to 40K | CM4 0 | M | E01021551 | E02004495 | 232-0 | . 3 6133 | 1968-01-01 | 2012-10-21 | 10K to 20K | SK12 1 | M | E01018665 | E02003854 | 6133-0 | . 4 7993 | 1961-01-01 | 2012-10-29 | 20K to 30K | LS17 8 | M | E01011556 | E02002344 | 7993-0 | . !conda list | grep pandas . pandas 1.0.1 py37h6c726b0_0 pandas-flavor 0.2.0 py_0 conda-forge . Using R and Python together . # %conda install tzlocal # %conda install simplegeneric . import rpy2 . # pandas2ri.activate() . %reload_ext rpy2.ipython . %R require(ggplot2) . R[write to console]: Loading required package: ggplot2 . array([0], dtype=int32) . import pandas as pd df = pd.DataFrame({ &#39;Letter&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;c&#39;], &#39;X&#39;: [4, 3, 5, 2, 1, 7, 7, 5, 9], &#39;Y&#39;: [0, 4, 3, 6, 7, 10, 11, 9, 13], &#39;Z&#39;: [1, 2, 3, 1, 2, 3, 1, 2, 3] }) . %%R -i df ggplot(data = df) + geom_point(aes(x = X, y= Y, color = Letter, size = Z)) . R[write to console]: Error in ggplot(data = df) : could not find function &#34;ggplot&#34; Calls: &lt;Anonymous&gt; -&gt; &lt;Anonymous&gt; -&gt; withVisible R[write to console]: In addition: R[write to console]: Warning messages: R[write to console]: 1: R[write to console]: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : R[write to console]: R[write to console]: there is no package called ‘ggplot’ R[write to console]: 2: R[write to console]: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : R[write to console]: R[write to console]: there is no package called ‘ggplot2’ . Error in ggplot(data = df) : could not find function &#34;ggplot&#34; Calls: &lt;Anonymous&gt; -&gt; &lt;Anonymous&gt; -&gt; withVisible . Printing virtually anything . happy_squirrels = !ls /Users/fgu/Library/Mobile Documents/com~apple~CloudDocs/fab/photos/squirrels/ . from IPython.display import display, Image for s in happy_squirrels: display(Image(&#39;/Users/fgu/Library/Mobile Documents/com~apple~CloudDocs/fab/photos/squirrels/&#39; + s, width=100)) . Write code to and load code from file . %%writefile test.py print(&#39;Hello&#39;) . Overwriting test.py . %pycat test.py . print(&#39;Hello&#39;) . Data from wide to long and back . data = pd.read_csv(&#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/examples/macrodata.csv&#39;) data.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name = &#39;date&#39;) columns = pd.Index([&#39;realgdp&#39;, &#39;cpi&#39;, &#39;unemp&#39;, &#39;infl&#39;], name=&#39;item&#39;) df = data.reindex(columns=columns) df.index = periods.to_timestamp(&#39;D&#39;, &#39;End&#39;) dfl = df.stack().reset_index().rename(columns={0:&#39;value&#39;}) dfl . date item value . 0 1959-03-31 23:59:59.999999999 | realgdp | 2710.349 | . 1 1959-03-31 23:59:59.999999999 | cpi | 28.980 | . 2 1959-03-31 23:59:59.999999999 | unemp | 5.800 | . 3 1959-03-31 23:59:59.999999999 | infl | 0.000 | . 4 1959-06-30 23:59:59.999999999 | realgdp | 2778.801 | . ... ... | ... | ... | . 807 2009-06-30 23:59:59.999999999 | infl | 3.370 | . 808 2009-09-30 23:59:59.999999999 | realgdp | 12990.341 | . 809 2009-09-30 23:59:59.999999999 | cpi | 216.385 | . 810 2009-09-30 23:59:59.999999999 | unemp | 9.600 | . 811 2009-09-30 23:59:59.999999999 | infl | 3.560 | . 812 rows × 3 columns . dfl.set_index([&#39;date&#39;, &#39;item&#39;]).unstack() . value . item cpi infl realgdp unemp . date . 1959-03-31 23:59:59.999999999 28.980 | 0.00 | 2710.349 | 5.8 | . 1959-06-30 23:59:59.999999999 29.150 | 2.34 | 2778.801 | 5.1 | . 1959-09-30 23:59:59.999999999 29.350 | 2.74 | 2775.488 | 5.3 | . 1959-12-31 23:59:59.999999999 29.370 | 0.27 | 2785.204 | 5.6 | . 1960-03-31 23:59:59.999999999 29.540 | 2.31 | 2847.699 | 5.2 | . ... ... | ... | ... | ... | . 2008-09-30 23:59:59.999999999 216.889 | -3.16 | 13324.600 | 6.0 | . 2008-12-31 23:59:59.999999999 212.174 | -8.79 | 13141.920 | 6.9 | . 2009-03-31 23:59:59.999999999 212.671 | 0.94 | 12925.410 | 8.1 | . 2009-06-30 23:59:59.999999999 214.469 | 3.37 | 12901.504 | 9.2 | . 2009-09-30 23:59:59.999999999 216.385 | 3.56 | 12990.341 | 9.6 | . 203 rows × 4 columns . dfl.pivot(&#39;date&#39;, &#39;item&#39;, &#39;value&#39;) . item cpi infl realgdp unemp . date . 1959-03-31 23:59:59.999999999 28.980 | 0.00 | 2710.349 | 5.8 | . 1959-06-30 23:59:59.999999999 29.150 | 2.34 | 2778.801 | 5.1 | . 1959-09-30 23:59:59.999999999 29.350 | 2.74 | 2775.488 | 5.3 | . 1959-12-31 23:59:59.999999999 29.370 | 0.27 | 2785.204 | 5.6 | . 1960-03-31 23:59:59.999999999 29.540 | 2.31 | 2847.699 | 5.2 | . ... ... | ... | ... | ... | . 2008-09-30 23:59:59.999999999 216.889 | -3.16 | 13324.600 | 6.0 | . 2008-12-31 23:59:59.999999999 212.174 | -8.79 | 13141.920 | 6.9 | . 2009-03-31 23:59:59.999999999 212.671 | 0.94 | 12925.410 | 8.1 | . 2009-06-30 23:59:59.999999999 214.469 | 3.37 | 12901.504 | 9.2 | . 2009-09-30 23:59:59.999999999 216.385 | 3.56 | 12990.341 | 9.6 | . 203 rows × 4 columns . HDF5 . Following this video. . import h5py import numpy as np . Create a HDF5 file object, which works kind of like a Python dictionary. . f = h5py.File(&#39;demo.hdf5&#39;) . data = np.arange(10) data . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . f[&#39;array&#39;] = data . dset = f[&#39;array&#39;] . dset . &lt;HDF5 dataset &#34;array&#34;: shape (10,), type &#34;&lt;i8&#34;&gt; . dset[:] . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . dset[[1, 2, 5]] . array([1, 2, 5]) . dset.attrs . &lt;Attributes of HDF5 object at 4425393432&gt; . Atributes again have dictionarry structure, so can add attribute like so: . dset.attrs[&#39;sampling frequency&#39;] = &#39;Every other week between 1 Jan 2001 and 7 Feb 2010&#39; dset.attrs[&#39;PI&#39;] = &#39;Fabian&#39; . list(dset.attrs.items()) for i in dset.attrs.items(): print(i) . (&#39;sampling frequency&#39;, &#39;Every other week between 1 Jan 2001 and 7 Feb 2010&#39;) (&#39;PI&#39;, &#39;Fabian&#39;) . f.close() . f = h5py.File(&#39;demo.hdf5&#39;) . list(f.keys()) . [&#39;array&#39;] . dset = f[&#39;array&#39;] . hdf5 files are organised in a hierarchy - that&#39;s what &quot;h&quot; stands for. . dset.name . &#39;/array&#39; . root = f[&#39;/&#39;] . list(root.keys()) . [&#39;array&#39;] . f[&#39;dataset&#39;] = data . f[&#39;full/dataset&#39;] = data . grp = f[&#39;full&#39;] . &#39;dataset&#39; in grp . True . list(grp.keys()) . [&#39;dataset&#39;] . dset2 = f.create_dataset(&#39;/full/bigger&#39;, (10000, 1000, 1000, 1000), dtype=&#39;f&#39;, compression=&#39;gzip&#39;) . list(f[&#39;full&#39;].keys()) . [&#39;bigger&#39;, &#39;dataset&#39;] . Misc. . idx = pd.IndexSlice df.loc[idx[:, &quot;2014&quot;], :] . from scipy import stats s = np.arange(5) std = np.std(s) mean = np.mean(s) manual_z = (s - mean) / std scipy_z = stats.zscore(s) manual_z == scipy_z . array([ True, True, True, True, True]) . np.arange(32).reshape((4, 8)) . array([[ 0, 1, 2, 3, 4, 5, 6, 7], [ 8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]) . import random print(&quot;one&quot;) if random.randint(0, 1) else print(&quot;zero&quot;) . zero . ndraws = 1000 draws = np.random.randint(0, 2, ndraws) steps = np.where(draws &gt; 0, 1, -1) walk = steps.cumsum() # Find how long it took to make 10 steps in either direction idx = (np.abs(walk) &gt;= 10).argmax() print(&quot;It took {} steps to get to {}&quot;.format(idx, walk[idx])) . It took 57 steps to get to -10 . nwalks = 5000 ndraws = 1000 draws = np.random.randint(0, 2, size=(nwalks, ndraws)) steps = np.where(draws &gt; 0, 1, -1) walk = steps.cumsum(axis=1) # Find number of walks that cross 30 and the average crossing time hit30 = (np.abs(walk) &gt;= 30).any(1) crossing_times = (np.abs(walk[hit30]) &gt;= 30).argmax(1) print( &quot;{} walks cross 30, taking {} steps on average.&quot;.format( hit30.sum(), crossing_times.mean() ) ) . 3385 walks cross 30, taking 498.33353028064994 steps on average. . Grouping . df = pd.DataFrame({&#39;key1&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], &#39;key2&#39;: [&#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;], &#39;data1&#39;: np.random.randn(5), &#39;data2&#39;: np.random.randn(5)}) df . key1 key2 data1 data2 . 0 a | one | -0.722346 | 1.350070 | . 1 b | two | -0.864278 | 0.652911 | . 2 c | one | 0.875035 | 0.838726 | . 3 d | two | 1.420677 | -0.464896 | . 4 e | one | -0.789309 | 0.148121 | . for group, data in df.groupby(&#39;key1&#39;): print(group) print(data) . a key1 key2 data1 data2 0 a one -0.722346 1.35007 b key1 key2 data1 data2 1 b two -0.864278 0.652911 c key1 key2 data1 data2 2 c one 0.875035 0.838726 d key1 key2 data1 data2 3 d two 1.420677 -0.464896 e key1 key2 data1 data2 4 e one -0.789309 0.148121 . pieces = dict(list(df.groupby(&#39;key1&#39;))) pieces[&#39;a&#39;] . key1 key2 data1 data2 . 0 a | one | -0.722346 | 1.35007 | . grouped = df.groupby(df.dtypes, axis=1) for dtype, data in grouped: print(dtype) print(data) . float64 data1 data2 0 -0.722346 1.350070 1 -0.864278 0.652911 2 0.875035 0.838726 3 1.420677 -0.464896 4 -0.789309 0.148121 object key1 key2 0 a one 1 b two 2 c one 3 d two 4 e one . Solution to my common indexing problem (index with shorter boolean series) . Problem: I want to index my df based on a boolean series that is shorter than the length of the df. E.g. I have a subset of users that fulfill a condition and want to keep these only. . data = o2.sample(frac=0.05) . high_spender = data.groupby(&quot;user_id&quot;).amount.sum() &gt; 350 . . todrop = high_spender[~high_spender].index.values data.drop(todrop, level=0).sort_index() . transaction_id transaction_description amount auto_tag merchant_name account_type . user_id transaction_date . 20912 2013-12-18 12152457 | card purchase o2 uk ref xxx xxxxxx0000 bcc | 11.990000 | mobile | o2 | current | . 2014-04-17 17466459 | direct debit o2 gedxxxxx154 ddr | 44.160000 | mobile | o2 | current | . 2015-01-19 54324542 | direct debit o2 gedxxxx9154 ddr | 38.380001 | mobile | o2 | current | . 2015-03-24 80422755 | direct debit o2 gedxxxx9097 ddr | 34.799999 | mobile | o2 | current | . 2015-09-23 96952964 | counter credit &lt;mdbremoved&gt; o2 bgc | 54.500000 | mobile | o2 | current | . 2016-04-25 134730196 | direct debit o2 gedxxxx9097 ddr | 40.990002 | mobile | o2 | current | . 2016-05-17 138708140 | direct debit o2 gedxxxx1717 ddr | 30.000000 | mobile | o2 | current | . 2016-08-17 154735556 | direct debit o2 gedxxxx9154 ddr | 34.270000 | mobile | o2 | current | . 2016-10-07 163699913 | counter credit &lt;mdbremoved&gt; o2/ &lt;mdbremoved&gt; bgc | 68.059998 | mobile | o2 | current | . 160992 2014-09-29 38730708 | o2 uk cd 9531 | 25.000000 | mobile | o2 | current | . 2015-01-19 54406685 | o2 uk cd 9531 | 175.000000 | mobile | o2 | current | . 2015-01-19 54406686 | o2 uk cd 9531 | 202.539993 | mobile | o2 | current | . high_spender . user_id 8 False 659 False 1078 False 1146 False 2324 False ... 420102 False 421678 False 423912 False 424865 False 425830 False Name: amount, Length: 210, dtype: bool . hs = high_spender[high_spender].index.values mask = data.index.isin(hs, level=0) data[mask].sort_index() . transaction_id transaction_description amount auto_tag merchant_name account_type . user_id transaction_date . 20912 2013-12-18 12152457 | card purchase o2 uk ref xxx xxxxxx0000 bcc | 11.990000 | mobile | o2 | current | . 2014-04-17 17466459 | direct debit o2 gedxxxxx154 ddr | 44.160000 | mobile | o2 | current | . 2015-01-19 54324542 | direct debit o2 gedxxxx9154 ddr | 38.380001 | mobile | o2 | current | . 2015-03-24 80422755 | direct debit o2 gedxxxx9097 ddr | 34.799999 | mobile | o2 | current | . 2015-09-23 96952964 | counter credit &lt;mdbremoved&gt; o2 bgc | 54.500000 | mobile | o2 | current | . 2016-04-25 134730196 | direct debit o2 gedxxxx9097 ddr | 40.990002 | mobile | o2 | current | . 2016-05-17 138708140 | direct debit o2 gedxxxx1717 ddr | 30.000000 | mobile | o2 | current | . 2016-08-17 154735556 | direct debit o2 gedxxxx9154 ddr | 34.270000 | mobile | o2 | current | . 2016-10-07 163699913 | counter credit &lt;mdbremoved&gt; o2/ &lt;mdbremoved&gt; bgc | 68.059998 | mobile | o2 | current | . 160992 2014-09-29 38730708 | o2 uk cd 9531 | 25.000000 | mobile | o2 | current | . 2015-01-19 54406685 | o2 uk cd 9531 | 175.000000 | mobile | o2 | current | . 2015-01-19 54406686 | o2 uk cd 9531 | 202.539993 | mobile | o2 | current | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/12/10/learning.html",
            "relUrl": "/python/pandas/2020/12/10/learning.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Iterators and generators",
            "content": "Writing my own iterator pattern . (From the Python Cookbook recipee 4.3) . def frange(start, stop, increment): x = start while x &lt; stop: yield x x += increment rng = frange(1, 10, 2) next(rng) next(rng) . 3 . list(rng) . [5, 7, 9] . I&#39;m a little confused by this still. . def aritprog(begin, step, end=None): result = type(begin + step)(begin) forever = end is None index = 0 while forever or result &lt; end: yield result index += 1 result = begin + step * index a = aritprog(0, 5, 20) for a in a: print(a) . 0 5 10 15 . a = iter([1, 2, 3]) . import inspect def gen(x): yield x a = gen(5) print(inspect.getgeneratorstate(a)) next(a) print(inspect.getgeneratorstate(a)) try: next(a) except StopIteration: print(inspect.getgeneratorstate(a)) . GEN_CREATED GEN_SUSPENDED GEN_CLOSED . Using generators for line-by-line data processing for large files . Example here, see &quot;Case Study: Generators in a Database Conversion Utility&quot; at end of Chap 14 in Fluent Python for context. . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/12/10/iterators-and-generators.html",
            "relUrl": "/python/2020/12/10/iterators-and-generators.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fast groupby operations",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . path = &#39;/Users/fgu/tmp/data_777.parquet&#39; df = pd.read_parquet(path) print(df.shape) df.head() . (115197, 21) . user_id transaction_date amount transaction_description merchant_name tag transaction_id user_registration_date account_last_refreshed bank latest_balance year_of_birth salary_range gender ym credit_debit account_id postcode account_created account_type merchant_business_line . 6553 14777 | 2012-10-10 | 5.00 | new southern railw cd 7715 | southern rail | public transport | 2994290 | 2013-01-07 | 2015-01-13 05:19:00 | lloyds bank | 729.59 | 1986.0 | NaN | m | 2012-10 | debit | 247061 | sw11 5 | 2013-01-07 | current | southern rail | . 6554 14777 | 2012-10-10 | 11.10 | new southern railw cd 7715 | southern rail | public transport | 2994291 | 2013-01-07 | 2015-01-13 05:19:00 | lloyds bank | 729.59 | 1986.0 | NaN | m | 2012-10 | debit | 247061 | sw11 5 | 2013-01-07 | current | southern rail | . 6555 14777 | 2012-10-10 | 26.91 | ticketscript*14064 cd 7715 | no merchant | public transport | 2994292 | 2013-01-07 | 2015-01-13 05:19:00 | lloyds bank | 729.59 | 1986.0 | NaN | m | 2012-10 | debit | 247061 | sw11 5 | 2013-01-07 | current | unknown merchant | . 6556 14777 | 2012-10-10 | 83.20 | selfserve ticket cd 2412 | south west trains | public transport | 2994293 | 2013-01-07 | 2015-01-13 05:19:00 | lloyds bank | 729.59 | 1986.0 | NaN | m | 2012-10 | debit | 247061 | sw11 5 | 2013-01-07 | current | south west trains | . 6557 14777 | 2012-10-11 | 19.00 | liv*livingsocial cd 7715 | no merchant | dining and drinking | 2994289 | 2013-01-07 | 2015-01-13 05:19:00 | lloyds bank | 729.59 | 1986.0 | NaN | m | 2012-10 | debit | 247061 | sw11 5 | 2013-01-07 | current | unknown merchant | . %%timeit # baseline def income_pmts(df, prop=3/4): &quot;&quot;&quot;Income payments in prop of all observed months.&quot;&quot;&quot; def helper(g): tot_months = g.ym.nunique() inc_months = g[g.tag.str.contains(&#39;_income&#39;)].ym.nunique() return (inc_months / tot_months) &gt; (prop) data = df[[&#39;user_id&#39;, &#39;transaction_date&#39;, &#39;tag&#39;, &#39;ym&#39;]] usrs = data.groupby(&#39;user_id&#39;).filter(helper).user_id.unique() return df[df.user_id.isin(usrs)] income_pmts(df) . 119 ms ± 6.28 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . Ravel . a = np.array([[1, 2, 3], [4, 5, 6]]) np.ravel(a) . array([1, 2, 3, 4, 5, 6]) . np.ravel(a.T) . array([1, 4, 2, 5, 3, 6]) . a.T . array([[1, 4], [2, 5], [3, 6]]) . def new(df): tag = df.tag.to_numpy(&#39;str&#39;) income = np.char.endswith(tag, &#39;_income&#39;) data = df[[&#39;user_id&#39;, &#39;ym&#39;]].values return data.T # return np.ravel_multi_index(data.T, data.max(0)[0] + 1) new(df) . array([[14777, 14777, 14777, ..., 579777, 579777, 579777], [&#39;2012-10&#39;, &#39;2012-10&#39;, &#39;2012-10&#39;, ..., &#39;2020-7&#39;, &#39;2020-7&#39;, &#39;2020-7&#39;]], dtype=object) . np.ravel_multi_index . iris = sns.load_dataset(&#39;iris&#39;) . a = np.array([1, 1, 2, 3, 4, 4, 4, 5, 5, 6]) idx = np.unique(a, return_index=True)[1][1:] np.split(a, idx) . [array([1, 1]), array([2]), array([3]), array([4, 4, 4]), array([5, 5]), array([6])] . np.sort . %%timeit def mygroupby(df, idx, x): data = df[[idx, x]].values data = np.sort(data, axis=0) idx = data[:, 0] x = data[:, 1] group_boundaries = np.unique(idx, return_index=True)[1][1:] grouped = np.split(x, group_boundaries) return [np.sum(g) for g in grouped] mygroupby(iris, &#39;species&#39;, &#39;petal_width&#39;) . 592 µs ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . %timeit iris.groupby(&#39;species&#39;).petal_width.sum() . 490 µs ± 17 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . numpy index package . Use NumPy for groupbys . path = &#39;/Users/fgu/tmp/data_777.parquet&#39; df = pd.read_parquet(path) df.head(2) . user_id transaction_date amount transaction_description merchant_name tag account_created ym user_registration_date year_of_birth account_id salary_range merchant_business_line bank transaction_id latest_balance account_type gender account_last_refreshed postcode credit_debit . 6553 14777 | 2012-10-10 | 5.0 | new southern railw cd 7715 | southern rail | public transport | 2013-01-07 | 2012-10 | 2013-01-07 | 1986.0 | 247061 | NaN | southern rail | lloyds bank | 2994290 | 729.59 | current | m | 2015-01-13 05:19:00 | sw11 5 | debit | . 6554 14777 | 2012-10-10 | 11.1 | new southern railw cd 7715 | southern rail | public transport | 2013-01-07 | 2012-10 | 2013-01-07 | 1986.0 | 247061 | NaN | southern rail | lloyds bank | 2994291 | 729.59 | current | m | 2015-01-13 05:19:00 | sw11 5 | debit | . %timeit df.amount &gt; 0 . 252 µs ± 5.65 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . %timeit df.amount.values &gt; 0 . 20.6 µs ± 553 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each) . (df.amount &gt; 50).values . array([False, False, False, ..., True, False, True]) . %timeit (df.user_id.values == df.user_id.shift(1).values) &amp; (df.amount.values &gt; 50) . 337 µs ± 27.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . k = -1 a = np.roll(df.amount.values, k) # a[:k] = np.nan a . array([ 11.1 , 26.91, 83.2 , ..., -27.98, 60. , 5. ], dtype=float32) . %%timeit k = 1 a = pd.Series(df.amount.values).shift(k) . 152 µs ± 9.11 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) . group means . df = sns.load_dataset(&#39;iris&#39;) . %timeit df.groupby(&#39;species&#39;).sepal_length.mean() . 497 µs ± 19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . %%timeit # from https://cmdlinetips.com/2019/05/how-to-implement-pandas-groupby-operation-with-numpy/ spec = df.species.values sl = df.sepal_length.values g = df.species.unique() [(i, np.mean(sl[spec == i])) for i in g] . 102 µs ± 2.87 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) . Fast groupby operations . %%time data = ( df[[&#39;user_id&#39;, &#39;transaction_date&#39;, &#39;tag&#39;]].copy() .set_index(&#39;user_id&#39;) .assign(ym = lambda df: df.transaction_date.dt.to_period(&#39;M&#39;)) ) tags = [&#39;earnings&#39;, &#39;pensions&#39;, &#39;benefits&#39;, &#39;other income&#39;] income = data[data.tag.str.match(&#39;|&#39;.join(tags))] tot_months = data.groupby(&#39;user_id&#39;).ym.nunique() inc_months = income.groupby(&#39;user_id&#39;).ym.nunique() cond = (inc_months / tot_months) &gt; (2/3) usrs = cond[cond].index result = df[df.user_id.isin(usrs)] . CPU times: user 16.3 s, sys: 1.49 s, total: 17.8 s Wall time: 18.3 s . %%time def helper(df): data = ( df[[&#39;user_id&#39;, &#39;transaction_date&#39;, &#39;tag&#39;]].copy() .set_index(&#39;user_id&#39;) .assign(ym = lambda df: df.transaction_date.dt.to_period(&#39;M&#39;)) ) tags = [&#39;earnings&#39;, &#39;pensions&#39;, &#39;benefits&#39;, &#39;other income&#39;] income = data[data.tag.str.match(&#39;|&#39;.join(tags))] tot_months = data.ym.nunique() inc_months = income.ym.nunique() return (inc_months / tot_months) &gt; (2/3) result2 = df.groupby(&#39;user_id&#39;).filter(helper) . CPU times: user 8.24 s, sys: 594 ms, total: 8.84 s Wall time: 8.89 s . %%time def helper(g): tot_months = g.ym.nunique() inc_months = g[g.inc].ym.nunique() return (inc_months / tot_months) &gt; (2/3) tags = [&#39;earnings&#39;, &#39;pensions&#39;, &#39;benefits&#39;, &#39;other income&#39;] data = ( df[[&#39;user_id&#39;, &#39;transaction_date&#39;, &#39;tag&#39;]].copy() .assign(ym = lambda df: df.transaction_date.dt.to_period(&#39;M&#39;)) .assign(inc = lambda df: df.tag.str.match(&#39;|&#39;.join(tags))) ) usrs = data.groupby(&#39;user_id&#39;).filter(helper).user_id.unique() result3 = df[df.user_id.isin(usrs)] result3 . CPU times: user 4.58 s, sys: 381 ms, total: 4.96 s Wall time: 5.01 s . user_id transaction_date amount transaction_description merchant_name auto_tag tag manual_tag gender latest_balance salary_range credit_debit account_last_refreshed year_of_birth transaction_id user_registration_date bank up_tag account_type account_created account_id postcode merchant_business_line . 4734 6077 | 2012-08-01 | 1.00 | planned o/d fee | no merchant | banking charges | banking charges | no tag | m | 783.65 | 10k to 20k | debit | 2020-05-29 07:39:00 | 1975.0 | 1072703 | 2012-10-21 | halifax personal banking | bank charges | current | 2012-10-22 | 20035 | bb12 7 | account provider | . 4735 6077 | 2012-08-01 | -5.00 | reward (net) | no merchant | rewards/cashback | rewards/cashback | no tag | m | 783.65 | 10k to 20k | credit | 2020-05-29 07:39:00 | 1975.0 | 1072702 | 2012-10-21 | halifax personal banking | rewards/cashback | current | 2012-10-22 | 20035 | bb12 7 | account provider | . 4736 6077 | 2012-08-08 | 200.00 | &lt;mdbremoved&gt; xxxxxxxxxxxxxx4580 08aug12 21:53 | no merchant | transfers | transfers | no tag | m | 783.65 | 10k to 20k | debit | 2020-05-29 07:39:00 | 1975.0 | 1072701 | 2012-10-21 | halifax personal banking | no tag | current | 2012-10-22 | 20035 | bb12 7 | non merchant mbl | . 4737 6077 | 2012-08-15 | 1550.00 | &lt;mdbremoved&gt; xxxxxxxxxxxxxx1075 15aug12 12:53 | no merchant | transfers | transfers | no tag | m | 783.65 | 10k to 20k | debit | 2020-05-29 07:39:00 | 1975.0 | 1072699 | 2012-10-21 | halifax personal banking | no tag | current | 2012-10-22 | 20035 | bb12 7 | non merchant mbl | . 4738 6077 | 2012-08-15 | -1632.37 | &lt;mdbremoved&gt; | no merchant | salary or wages - main | salary or wages - main | no tag | m | 783.65 | 10k to 20k | credit | 2020-05-29 07:39:00 | 1975.0 | 1072700 | 2012-10-21 | halifax personal banking | no tag | current | 2012-10-22 | 20035 | bb12 7 | no merchant business line | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 90316 88277 | 2015-11-23 | -60.00 | call ref.no. 0000 , from a/c xxxxxx41 - otr | no merchant | transfers | transfers | no tag | f | 0.00 | NaN | credit | 2015-11-25 00:00:00 | 1987.0 | 107753283 | 2014-07-06 | natwest bank | current account | current | 2014-07-06 | 8950 | le11 1 | personal | . 90317 88277 | 2015-11-23 | 81.65 | bmach 21nov, charge 1.65 - atm | no merchant | cash | cash | no tag | f | 0.00 | NaN | debit | 2015-11-25 00:00:00 | 1987.0 | 107753281 | 2014-07-06 | natwest bank | cash | current | 2014-07-06 | 8950 | le11 1 | personal | . 90318 88277 | 2015-11-24 | 49.27 | &lt;mdbremoved&gt; balance transfer | no merchant | transfers | transfers | no tag | f | 0.00 | NaN | debit | 2015-11-25 00:00:00 | 1987.0 | 189202573 | 2014-07-06 | natwest bank | transfers | current | 2014-07-06 | 8950 | le11 1 | personal | . 90319 88277 | 2015-11-24 | -61.45 | child tax credit - bac | no merchant | family benefits | benefits | no tag | f | 0.00 | NaN | credit | 2015-11-26 00:00:00 | 1987.0 | 107912211 | 2014-07-06 | natwest bank | family benefits | current | 2014-07-06 | 8947 | le11 1 | public sector | . 90320 88277 | 2015-11-25 | 184.33 | &lt;mdbremoved&gt; balance transfer | no merchant | transfers | transfers | no tag | f | 0.00 | NaN | debit | 2015-11-26 00:00:00 | 1987.0 | 189202572 | 2014-07-06 | natwest bank | transfers | current | 2014-07-06 | 8947 | le11 1 | personal | . 2891833 rows × 23 columns .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/12/10/fast-groupby.html",
            "relUrl": "/python/pandas/2020/12/10/fast-groupby.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://fabiangunzinger.github.io/blog/jupyter/2020/12/10/example-post.html",
            "relUrl": "/jupyter/2020/12/10/example-post.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Error handling",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Only printing message . def divide(a, b): try: return a / b except ZeroDivisionError as e: print(e) divide(4, 0) . division by zero . Raise original error . def divide(a, b): try: return a / b except ZeroDivisionError as e: raise e divide(4, 0) . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-11-4ca87b7cf0dc&gt; in &lt;module&gt; 5 raise e 6 -&gt; 7 divide(4, 0) &lt;ipython-input-11-4ca87b7cf0dc&gt; in divide(a, b) 3 return a / b 4 except ZeroDivisionError as e: -&gt; 5 raise e 6 7 divide(4, 0) &lt;ipython-input-11-4ca87b7cf0dc&gt; in divide(a, b) 1 def divide(a, b): 2 try: -&gt; 3 return a / b 4 except ZeroDivisionError as e: 5 raise e ZeroDivisionError: division by zero . Raise different error type to be clearer that invalid value was supplied . def divide(a, b): try: return a / b except ZeroDivisionError as e: raise ValueError(&#39;Invalid inputs&#39;) divide(4, 0) . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-10-1fa9c1ba7c1d&gt; in divide(a, b) 2 try: -&gt; 3 return a / b 4 except ZeroDivisionError as e: ZeroDivisionError: division by zero During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) &lt;ipython-input-10-1fa9c1ba7c1d&gt; in &lt;module&gt; 5 raise ValueError(&#39;Invalid inputs&#39;) 6 -&gt; 7 divide(4, 0) &lt;ipython-input-10-1fa9c1ba7c1d&gt; in divide(a, b) 3 return a / b 4 except ZeroDivisionError as e: -&gt; 5 raise ValueError(&#39;Invalid inputs&#39;) 6 7 divide(4, 0) ValueError: Invalid inputs . Sources . Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/12/10/error-handling.html",
            "relUrl": "/python/2020/12/10/error-handling.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Data visualisation",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . Matplotlib . Subplots from columns in df . Adapted from this post . with plt.style.context(&#39;seaborn-deep&#39;): fig, axes = plt.subplots(2, 2, figsize=(8, 8)) for ax, col in zip(axes.flatten(), accounts.columns): # zip does the magic! s = accounts[col] ax.hist(s, alpha=0.7) med, p75 = s.quantile([.5, .75]) ax.axvline(med, color=&#39;g&#39;, label=&#39;50th pct: &#39; + format(med, &#39;.0f&#39;)) ax.axvline(p75, color=&#39;orange&#39;, label=&#39;75th pct: &#39; + format(p75, &#39;.0f&#39;)) ax.legend() . NameError Traceback (most recent call last) &lt;ipython-input-1-3a95d9d76c1b&gt; in &lt;module&gt; -&gt; 1 with plt.style.context(&#39;seaborn-deep&#39;): 2 fig, axes = plt.subplots(2, 2, figsize=(8, 8)) 3 4 for ax, col in zip(axes.flatten(), accounts.columns): # zip does the magic! 5 s = accounts[col] NameError: name &#39;plt&#39; is not defined . Effect of holidays on births . From Python Data Science Handbook . file = &#39;https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv&#39; df = pd.read_csv(file).dropna() print(df.shape) df.head() . (15067, 5) . year month day gender births . 0 1969 | 1 | 1.0 | F | 4046 | . 1 1969 | 1 | 1.0 | M | 4440 | . 2 1969 | 1 | 2.0 | F | 4454 | . 3 1969 | 1 | 2.0 | M | 4548 | . 4 1969 | 1 | 3.0 | F | 4548 | . # Eliminate outliers using sigma-clipping pcts = np.percentile(df.births, [25, 50, 75]) mu, sigma = pcts[1], 0.74 * (pcts[2] - pcts[0]) df = df.query(&#39;(births &gt; @mu - 5 * @sigma) &amp; (births &lt; @mu + 5 * @sigma)&#39;) # Create datetime index df.index = pd.to_datetime(df.year * 10_000 + df.month * 100 + df.day, format=&#39;%Y%m%d&#39;) # Make pivot table with month-day index and mean of births column day_means = df.pivot_table(&#39;births&#39;, [df.index.month, df.index.day]) # Turn pivot index into year-month-day index (for leap year) for plotting from datetime import date day_means.index = [date(2020, month, day) for month, day in day_means.index] . fig, ax = plt.subplots(figsize=(12, 4)) style = dict(color=&#39;cornflowerblue&#39;, linewidth=4, style=&#39;-&#39;) day_means.plot(ax=ax, legend=None, **style) # Format ax.set(ylim=(3600, None), ylabel=&#39;Mean number of births&#39;) ax.xaxis.set_major_locator(mpl.dates.MonthLocator()) ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(bymonthday=15)) ax.xaxis.set_major_formatter(plt.NullFormatter()) ax.xaxis.set_minor_formatter(mpl.dates.DateFormatter(&#39;%h&#39;)) # Add text ax.text(&#39;2020-01-01&#39;, 3900, &quot;New year&#39;s day&quot;, ha=&#39;left&#39;) ax.text(&#39;2020-07-04&#39;, 4200, &#39;Independence day&#39;, ha=&#39;center&#39;) ax.text(&#39;2020-12-25&#39;, 3700, &#39;Christmas&#39;, ha=&#39;right&#39;); . Seatle cycling data . From JVDP&#39;s blog . file = &#39;https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD&#39; data = pd.read_csv(file, skiprows=1, names=[&#39;date&#39;, &#39;total&#39;, &#39;east&#39;, &#39;west&#39;], parse_dates=True, index_col=&#39;date&#39;) . data.describe() . total east west . count 67118.000000 | 67118.000000 | 67118.000000 | . mean 112.912527 | 51.559835 | 61.352692 | . std 144.160880 | 66.522811 | 89.768937 | . min 0.000000 | 0.000000 | 0.000000 | . 25% 14.000000 | 6.000000 | 7.000000 | . 50% 60.000000 | 28.000000 | 30.000000 | . 75% 147.000000 | 69.000000 | 74.000000 | . max 1097.000000 | 698.000000 | 850.000000 | . data.plot(); . weekly = data.resample(&#39;W&#39;).sum() weekly.plot(style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . daily = data.resample(&#39;D&#39;).sum() daily.rolling(window=30, center=True).sum().plot(style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . by_time = data.groupby(data.index.time).sum() hourly_ticks = 3 * 60 * 60 * np.arange(8) by_time.plot(xticks=hourly_ticks, style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . by_weekday = data.groupby(data.index.dayofweek).sum() by_weekday.index = [&#39;Mo&#39;, &#39;Tu&#39;, &#39;We&#39;, &#39;Th&#39;, &#39;Fr&#39;, &#39;Sa&#39;, &#39;So&#39;] by_weekday.plot(style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . mpl.get_configdir() . &#39;/Users/fgu/.matplotlib&#39; . wknd = np.where(data.index.dayofweek &gt; 4, &#39;weekend&#39;, &#39;weekday&#39;) hourly = data.groupby([wknd, data.index.time]).sum() fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 5)) hourly.loc[&#39;weekday&#39;].plot(ax=ax1, title=&#39;Weekdays&#39;, xticks=hourly_ticks, style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]) hourly.loc[&#39;weekend&#39;].plot(ax=ax0, title=&#39;Weekends&#39;, xticks=hourly_ticks, style=[&#39;-&#39;, &#39;--&#39;, &#39;:&#39;]); . USA.gov data from Bitly . From Python for Data Analysis . import pandas as pd import numpy as np import seaborn as sns sns.set() %config InlineBackend.figure_format =&#39;retina&#39; . path = &#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/bitly_usagov/example.txt&#39; df = pd.read_json(path, lines=True) df.head() . a c nk tz gr g h l al hh r u t hc cy ll _heartbeat_ kw . 0 Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... | US | 1.0 | America/New_York | MA | A6qOVH | wfLQtf | orofrog | en-US,en;q=0.8 | 1.usa.gov | http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/... | http://www.ncbi.nlm.nih.gov/pubmed/22415991 | 1.331923e+09 | 1.331823e+09 | Danvers | [42.576698, -70.954903] | NaN | NaN | . 1 GoogleMaps/RochesterNY | US | 0.0 | America/Denver | UT | mwszkS | mwszkS | bitly | NaN | j.mp | http://www.AwareMap.com/ | http://www.monroecounty.gov/etc/911/rss.php | 1.331923e+09 | 1.308262e+09 | Provo | [40.218102, -111.613297] | NaN | NaN | . 2 Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ... | US | 1.0 | America/New_York | DC | xxr3Qb | xxr3Qb | bitly | en-US | 1.usa.gov | http://t.co/03elZC4Q | http://boxer.senate.gov/en/press/releases/0316... | 1.331923e+09 | 1.331920e+09 | Washington | [38.9007, -77.043098] | NaN | NaN | . 3 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8)... | BR | 0.0 | America/Sao_Paulo | 27 | zCaLwp | zUtuOu | alelex88 | pt-br | 1.usa.gov | direct | http://apod.nasa.gov/apod/ap120312.html | 1.331923e+09 | 1.331923e+09 | Braz | [-23.549999, -46.616699] | NaN | NaN | . 4 Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... | US | 0.0 | America/New_York | MA | 9b6kNl | 9b6kNl | bitly | en-US,en;q=0.8 | bit.ly | http://www.shrewsbury-ma.gov/selco/ | http://www.shrewsbury-ma.gov/egov/gallery/1341... | 1.331923e+09 | 1.273672e+09 | Shrewsbury | [42.286499, -71.714699] | NaN | NaN | . Let&#39;s plot the most occuring time-zones. . counts = df.tz.str.replace(&#39;^$&#39;, &#39;Unknown&#39;).fillna(&#39;Missing&#39;).value_counts()[:10] sns.barplot(counts.values, counts.index); . Now, let&#39;s split the bars by operating system. . pd.Series.reverse = lambda self: self[::-1] # Cool trick from here: https://stackoverflow.com/a/46624694 df[&#39;os&#39;] = np.where(df.a.str.contains(&#39;Mac&#39;), &#39;Mac&#39;, &#39;Not Mac&#39;) agg_counts = (df.replace(&#39;^$&#39;, &#39;Unknown&#39;, regex=True) .groupby([&#39;tz&#39;, &#39;os&#39;]) .size() .unstack() .fillna(0)) indexer = agg_counts.sum(1).argsort() data = agg_counts.take(indexer[-10:]).reverse().stack() data.name = &#39;totals&#39; data = data.reset_index() sns.barplot(x=&#39;totals&#39;, y=&#39;tz&#39;, hue=&#39;os&#39;, data=data); . MovieLens 1M dataset . From Python for Data Analysis . !ls data/ml-1m . README movies.dat ratings.dat users.dat . import pandas as pd . path = &#39;data/ml-1m/&#39; unames = [&#39;user_id&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;occupation&#39;, &#39;zip&#39;] users = pd.read_table(path + &#39;users.dat&#39;, sep=&#39;::&#39;, engine=&#39;python&#39;, names=unames) rnames = [&#39;user_id&#39;, &#39;movie_id&#39;, &#39;rating&#39;, &#39;timestamp&#39;] ratings = pd.read_table(path + &#39;ratings.dat&#39;, sep=&#39;::&#39;, engine=&#39;python&#39;, names=rnames) mnames = [&#39;movie_id&#39;, &#39;title&#39;, &#39;genres&#39;] movies = pd.read_table(path + &#39;movies.dat&#39;, sep=&#39;::&#39;, engine=&#39;python&#39;, names=mnames) data = pd.merge(pd.merge(users, ratings), movies) data.head() . user_id gender age occupation zip movie_id rating timestamp title genres . 0 1 | F | 1 | 10 | 48067 | 1193 | 5 | 978300760 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 1 2 | M | 56 | 16 | 70072 | 1193 | 5 | 978298413 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 2 12 | M | 25 | 12 | 32793 | 1193 | 4 | 978220179 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 3 15 | M | 25 | 7 | 22903 | 1193 | 4 | 978199279 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 4 17 | M | 50 | 1 | 95350 | 1193 | 5 | 978158471 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . Calculate average ratings by gender . mean_ratings = data.pivot_table(values=&#39;rating&#39;, index=&#39;title&#39;, columns=&#39;gender&#39;, aggfunc=&#39;mean&#39;) mean_ratings.head() . gender F M . title . $1,000,000 Duck (1971) 3.375000 | 2.761905 | . &#39;Night Mother (1986) 3.388889 | 3.352941 | . &#39;Til There Was You (1997) 2.675676 | 2.733333 | . &#39;burbs, The (1989) 2.793478 | 2.962085 | . ...And Justice for All (1979) 3.828571 | 3.689024 | . Keep only movies with at least 200 ratings . ratings_count = data.groupby(&#39;title&#39;).size() active_titles = ratings_count[ratings_count &gt; 200].index mean_ratings = mean_ratings.loc[active_titles] # mean_ratings = mean_ratings.reindex(active_titles) # alternative . Above was mainly to practice, what I actually want is to exclude movies with fewer than 200 ratings from the very start . rating_count = data.groupby(&#39;title&#39;).size() active_movies = rating_count[rating_count &gt; 200].index data = data[data.title.isin(active_movies)] data.head() . user_id gender age occupation zip movie_id rating timestamp title genres . 0 1 | F | 1 | 10 | 48067 | 1193 | 5 | 978300760 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 1 2 | M | 56 | 16 | 70072 | 1193 | 5 | 978298413 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 2 12 | M | 25 | 12 | 32793 | 1193 | 4 | 978220179 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 3 15 | M | 25 | 7 | 22903 | 1193 | 4 | 978199279 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . 4 17 | M | 50 | 1 | 95350 | 1193 | 5 | 978158471 | One Flew Over the Cuckoo&#39;s Nest (1975) | Drama | . Calculate ratings difference by gender (again, just for fun, and to compare to above result) . mean_ratings2 = data.pivot_table(&#39;rating&#39;, &#39;title&#39;, &#39;gender&#39;, &#39;mean&#39;) all(mean_ratings2 == mean_ratings) . True . Look at top movis by gender . mean_ratings.sort_values(&#39;F&#39;, ascending=False).head() . gender F M . title . Close Shave, A (1995) 4.644444 | 4.473795 | . Wrong Trousers, The (1993) 4.588235 | 4.478261 | . General, The (1927) 4.575758 | 4.329480 | . Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) 4.572650 | 4.464589 | . Wallace &amp; Gromit: The Best of Aardman Animation (1996) 4.563107 | 4.385075 | . mean_ratings.sort_values(&#39;M&#39;, ascending=False).head() . gender F M . title . Godfather, The (1972) 4.314700 | 4.583333 | . Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954) 4.481132 | 4.576628 | . Shawshank Redemption, The (1994) 4.539075 | 4.560625 | . Raiders of the Lost Ark (1981) 4.332168 | 4.520597 | . Usual Suspects, The (1995) 4.513317 | 4.518248 | . Calculate rating differences . mean_ratings[&#39;diff&#39;] = np.abs(mean_ratings[&#39;F&#39;] - mean_ratings[&#39;M&#39;]) mean_ratings.sort_values(&#39;diff&#39;, ascending=False).head() . gender F M diff . title . Dirty Dancing (1987) 3.790378 | 2.959596 | 0.830782 | . Good, The Bad and The Ugly, The (1966) 3.494949 | 4.221300 | 0.726351 | . To Wong Foo, Thanks for Everything! Julie Newmar (1995) 3.486842 | 2.795276 | 0.691567 | . Kentucky Fried Movie, The (1977) 2.878788 | 3.555147 | 0.676359 | . Jumpin&#39; Jack Flash (1986) 3.254717 | 2.578358 | 0.676359 | . Find movies with the most rating disagreement among all viwers . data.groupby(&#39;title&#39;).rating.std().sort_values(ascending=False).head() . title Plan 9 from Outer Space (1958) 1.455998 Texas Chainsaw Massacre, The (1974) 1.332448 Dumb &amp; Dumber (1994) 1.321333 Blair Witch Project, The (1999) 1.316368 Natural Born Killers (1994) 1.307198 Name: rating, dtype: float64 . Baby names . From Python for Data Analysis . !head data/names/yob1880.txt . Mary,F,7065 Anna,F,2604 Emma,F,2003 Elizabeth,F,1939 Minnie,F,1746 Margaret,F,1578 Ida,F,1472 Alice,F,1414 Bertha,F,1320 Sarah,F,1288 . import re files = !ls data/names/yob* pieces = [] columns = [&#39;name&#39;, &#39;sex&#39;, &#39;births&#39;] for file in files: frame = pd.read_csv(file, names=columns) year = int(re.findall(&#39; d+&#39;, file)[0]) frame[&#39;year&#39;] = year pieces.append(frame) names = pd.concat(pieces, ignore_index=True) names.head() . name sex births year . 0 Mary | F | 7065 | 1880 | . 1 Anna | F | 2604 | 1880 | . 2 Emma | F | 2003 | 1880 | . 3 Elizabeth | F | 1939 | 1880 | . 4 Minnie | F | 1746 | 1880 | . years = range(1880, 2019) pieces = [] columns = [&#39;name&#39;, &#39;gender&#39;, &#39;births&#39;] for year in years: path = &#39;data/names/yob%d.txt&#39; % year frame = pd.read_csv(path, names=columns) frame[&#39;year&#39;] = year pieces.append(frame) names = pd.concat(pieces, ignore_index=True) names.head() . name gender births year . 0 Mary | F | 7065 | 1880 | . 1 Anna | F | 2604 | 1880 | . 2 Emma | F | 2003 | 1880 | . 3 Elizabeth | F | 1939 | 1880 | . 4 Minnie | F | 1746 | 1880 | . Plot number of girls and boys born over time . names.pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;gender&#39;, &#39;sum&#39;).plot(); . Add a proportion column . def add_prop(group): group[&#39;prop&#39;] = group.births / group.births.sum() return group names = names.groupby([&#39;gender&#39;, &#39;year&#39;]).apply(add_prop) names.head() . name gender births year prop . 0 Mary | F | 7065 | 1880 | 0.077642 | . 1 Anna | F | 2604 | 1880 | 0.028617 | . 2 Emma | F | 2003 | 1880 | 0.022012 | . 3 Elizabeth | F | 1939 | 1880 | 0.021309 | . 4 Minnie | F | 1746 | 1880 | 0.019188 | . Check that prop sums to 1 for each year-gender group . names.groupby([&#39;gender&#39;, &#39;year&#39;]).prop.sum() . gender year F 1880 1.0 1881 1.0 1882 1.0 1883 1.0 1884 1.0 ... M 2014 1.0 2015 1.0 2016 1.0 2017 1.0 2018 1.0 Name: prop, Length: 278, dtype: float64 . Keep only top 1000 names per gender and year . def top1000(group): return group.sort_values(by=&#39;births&#39;, ascending=False)[:1000] top1000 = names.groupby([&#39;gender&#39;, &#39;year&#39;]).apply(top1000).reset_index(drop=True) . Let&#39;s look at the number of births per year for common names . subset = [&#39;John&#39;, &#39;Harry&#39;, &#39;Mary&#39;, &#39;Marilyn&#39;] (names.pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;name&#39;, &#39;sum&#39;)[subset] .plot(subplots=True, figsize=(12,10), title=&#39;Number of births per year&#39;)); . Plot suggest that common names have become less popular. This could be either because people use other names instead, or becasue people just use more names overall. Let&#39;s look into this. First by looking at the proportion of birhts for the top 1000 names. . (top1000.pivot_table(&#39;prop&#39;, &#39;year&#39;, &#39;gender&#39;, &#39;sum&#39;) .plot(title=&#39;Proportion of top 1000 names of all births&#39;, figsize=(6, 5), yticks=np.linspace(0, 1.2, 13))); . It&#39;s clear from the above plot that the top 1000 names are becoming a smaller proportion of all names over time, indicating that naming diversity is increasing. To corroborate this, let&#39;s look at the number of names that account for 50 percent of all births in each year for each sex. . boys2018 = top1000[(top1000.year == 2018) &amp; (top1000.gender == &#39;M&#39;)] boys2018.sort_values(&#39;prop&#39;, ascending=False).prop.cumsum().searchsorted(.5) + 1 . 149 . def get_quantile_count(group, q=0.5): group = group.sort_values(&#39;prop&#39;, ascending=False) return group.prop.cumsum().searchsorted(.5) + 1 diversity = top1000.groupby([&#39;gender&#39;, &#39;year&#39;]).apply(get_quantile_count).unstack(level=0) diversity.plot(figsize=(8, 6)); . Explore the last-letter revolution . import matplotlib.pyplot as plt def get_last_letter(name): return name[-1] names[&#39;last_letter&#39;] = names.name.map(get_last_letter) table = names.pivot_table(&#39;births&#39;, &#39;last_letter&#39;, [&#39;sex&#39;, &#39;year&#39;], &#39;sum&#39;) subtable = table.reindex(columns=[1960, 1990, 2018], level=&#39;year&#39;) subtable = subtable / subtable.sum() subtable fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,10)) subtable[&#39;M&#39;].plot(kind=&#39;bar&#39;, ax=ax1) subtable[&#39;F&#39;].plot(kind=&#39;bar&#39;, ax=ax2); . For boys names, d, n, and y have changed markedly in popularity over the past six decads. Let&#39;s look at this more closely. . table[&#39;M&#39;].reindex([&#39;d&#39;, &#39;n&#39;, &#39;y&#39;]).T.plot(figsize=(8, 6), linewidth=5); . Leslie-like names have evolved from being boy to being girl names . def normalise(df): return df.div(df.sum(1), axis=&#39;rows&#39;) (names[names.name.str.lower().str.contains(&#39;^lesl&#39;)] .pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;sex&#39;, &#39;sum&#39;) .pipe(normalise) .plot(figsize=(8, 6), linewidth=5)); . Evolution of Molly and Fabian . names . name sex births year last_letter . 0 Mary | F | 7065 | 1880 | y | . 1 Anna | F | 2604 | 1880 | a | . 2 Emma | F | 2003 | 1880 | a | . 3 Elizabeth | F | 1939 | 1880 | h | . 4 Minnie | F | 1746 | 1880 | e | . ... ... | ... | ... | ... | ... | . 1957041 Zylas | M | 5 | 2018 | s | . 1957042 Zyran | M | 5 | 2018 | n | . 1957043 Zyrie | M | 5 | 2018 | e | . 1957044 Zyron | M | 5 | 2018 | n | . 1957045 Zzyzx | M | 5 | 2018 | x | . 1957046 rows × 5 columns . (names[names.name.isin([&#39;Molly&#39;, &#39;Fabian&#39;])] .pivot_table(&#39;births&#39;, &#39;year&#39;, &#39;name&#39;, &#39;sum&#39;) .plot(figsize=(10, 8), linewidth=8)); . Baby names in Switzerland . ls data/ch-names/ . f.xlsx m.xlsx . genders = [&#39;f&#39;, &#39;m&#39;] def rename_cols(df, names): df.columns = names return df pieces = [] columns = [&#39;name&#39;, &#39;year&#39;, &#39;births&#39;, &#39;rank&#39;] for gender in genders: path = &#39;data/ch-names/%s.xlsx&#39; % gender data = (pd.read_excel(path, header=[2, 3], index_col=0, skipfooter=5) .stack(level=0) .reset_index() .pipe(rename_cols, columns) .assign(gender=gender)) pieces.append(data) names = pd.concat(pieces, ignore_index=True) names . name year births rank gender . 0 Emma | 1998 | 88 | 78 | f | . 1 Emma | 1999 | 80 | 85 | f | . 2 Emma | 2000 | 127 | 46 | f | . 3 Emma | 2001 | 116 | 46 | f | . 4 Emma | 2002 | 147 | 36 | f | . ... ... | ... | ... | ... | ... | . 41995 Rúben | 2014 | 5 | 1071 | m | . 41996 Rúben | 2015 | 4 | 1263 | m | . 41997 Rúben | 2016 | 6 | 972 | m | . 41998 Rúben | 2017 | 15 | 515 | m | . 41999 Rúben | 2018 | 7 | 924 | m | . 42000 rows × 5 columns . files = !ls data/ch-names/* pieces = [] columns = [&#39;name&#39;, &#39;year&#39;, &#39;births&#39;] for file in files: frame = (pd.read_excel(file, header=[2, 3], index_col=0, skipfooter=5) .stack(level=[0]).reset_index().drop(&#39;Rang&#39;, axis=1)) frame.columns = columns gender = file[-6] frame[&#39;gender&#39;] = gender pieces.append(frame) table = pd.concat(pieces, ignore_index=True) table.head() . name year births gender . 0 Emma | 1998 | 88 | f | . 1 Emma | 1999 | 80 | f | . 2 Emma | 2000 | 127 | f | . 3 Emma | 2001 | 116 | f | . 4 Emma | 2002 | 147 | f | . Add proportions and rank columns . def calc_prop(group): group[&#39;prop&#39;] = group.births / group.births.sum() * 100 return group table = table.groupby([&#39;year&#39;, &#39;gender&#39;]).apply(calc_prop) table[&#39;rank&#39;] = table.groupby([&#39;year&#39;, &#39;gender&#39;]).births.rank(method=&#39;min&#39;) . Most popular names by year . def top_n(group, n=5): return group.sort_values(&#39;births&#39;, ascending=False)[:n] num_names = 10 years = [1998] (table.groupby([&#39;year&#39;, &#39;gender&#39;]) .apply(top_n, num_names) .drop([&#39;year&#39;, &#39;gender&#39;], axis=1) .reset_index(level=2, drop=True) .loc[years]) . name births prop rank . year gender . 1998 m Luca | 648 | 2.214173 | 1000.0 | . m David | 528 | 1.804141 | 999.0 | . m Simon | 511 | 1.746053 | 998.0 | . m Marco | 435 | 1.486366 | 997.0 | . m Joel | 424 | 1.448780 | 996.0 | . m Michael | 407 | 1.390692 | 995.0 | . m Lukas | 392 | 1.339438 | 994.0 | . m Nicolas | 375 | 1.281350 | 993.0 | . m Fabian | 368 | 1.257432 | 992.0 | . m Kevin | 350 | 1.195927 | 991.0 | . w Laura | 607 | 2.405675 | 1000.0 | . w Celine | 414 | 1.640774 | 999.0 | . w Sarah | 407 | 1.613031 | 998.0 | . w Jessica | 391 | 1.549620 | 997.0 | . w Lea | 375 | 1.486208 | 996.0 | . w Michelle | 357 | 1.414870 | 995.0 | . w Sara | 344 | 1.363348 | 994.0 | . w Vanessa | 320 | 1.268231 | 993.0 | . w Lara | 308 | 1.220672 | 992.0 | . w Julia | 299 | 1.185003 | 991.0 | . What&#39;s going on with girl names ending in &#39;a&#39;? . table[table.name.str.endswith(&#39;a&#39;)] . 0 False 1 False 2 False 3 False 4 False ... 41995 False 41996 False 41997 False 41998 False 41999 False Name: name, Length: 42000, dtype: bool . girl_names = [&#39;Emma&#39;, &#39;Ivy&#39;, &#39;Audrey&#39;, &#39;Vivien&#39;] . def get_last_letter(name): return name[-1] table[&#39;last_letter&#39;] = table.name.map(get_last_letter) last_letters = table[table.gender == &#39;w&#39;].pivot_table(&#39;births&#39;, &#39;last_letter&#39;, &#39;year&#39;, &#39;sum&#39;) last_letters = last_letters / last_letters.sum() last_letters.T[&#39;a&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a32573f50&gt; . Popularity of names over time . import matplotlib.pyplot as plt def plot_births(names, axis, values=&#39;prop&#39;): (table[(table.name.isin(names))] .pivot_table(values, &#39;year&#39;, &#39;name&#39;, &#39;sum&#39;) .plot(ax=axis, xticks=range(1998, 2019), rot=45, linewidth=7)) fig, (left, right) = plt.subplots(1, 2, figsize=(20, 8)) girl_names = [&#39;Emma&#39;] boy_names = [&#39;Liam&#39;, &#39;Leo&#39;, &#39;Theo&#39;, &#39;Fabian&#39;] plot_births(girl_names, left, values=&#39;births&#39;) plot_births(boy_names, right, values=&#39;births&#39;) . USDA Food database . From Python for Data Analysis . ls data/usda_foods.json . chap14-examples.ipynb data/ . import json db = json.load(open(&#39;data/usda_foods.json&#39;)) len(db) . db[0].keys() . dict_keys([&#39;id&#39;, &#39;description&#39;, &#39;tags&#39;, &#39;manufacturer&#39;, &#39;group&#39;, &#39;portions&#39;, &#39;nutrients&#39;]) . pd.DataFrame(db[0][&#39;nutrients&#39;]) . value units description group . 0 25.180 | g | Protein | Composition | . 1 29.200 | g | Total lipid (fat) | Composition | . 2 3.060 | g | Carbohydrate, by difference | Composition | . 3 3.280 | g | Ash | Other | . 4 376.000 | kcal | Energy | Energy | . ... ... | ... | ... | ... | . 157 1.472 | g | Serine | Amino Acids | . 158 93.000 | mg | Cholesterol | Other | . 159 18.584 | g | Fatty acids, total saturated | Other | . 160 8.275 | g | Fatty acids, total monounsaturated | Other | . 161 0.830 | g | Fatty acids, total polyunsaturated | Other | . 162 rows × 4 columns . Produce df with info variables . info_keys = [&#39;description&#39;, &#39;id&#39;, &#39;manufacturer&#39;, &#39;group&#39;] new_col_names = {&#39;description&#39;: &#39;food&#39;, &#39;group&#39;: &#39;fgroup&#39;} info = pd.DataFrame(db, columns=info_keys) info = info.rename(columns=new_col_names) info.head() . food id manufacturer fgroup . 0 Cheese, caraway | 1008 | | Dairy and Egg Products | . 1 Cheese, cheddar | 1009 | | Dairy and Egg Products | . 2 Cheese, edam | 1018 | | Dairy and Egg Products | . 3 Cheese, feta | 1019 | | Dairy and Egg Products | . 4 Cheese, mozzarella, part skim milk | 1028 | | Dairy and Egg Products | . Create a df with all the nutrient info for each food . new_col_names = {&#39;description&#39;: &#39;nutrient&#39;, &#39;group&#39;: &#39;ngroup&#39;} pieces = [] for rec in db: nuts = pd.DataFrame(rec[&#39;nutrients&#39;]) nuts[&#39;id&#39;] = rec[&#39;id&#39;] pieces.append(nuts) nutrients = pd.concat(pieces, ignore_index=True) nutrients = nutrients.rename(columns=new_col_names) nutrients.head() . value units nutrient ngroup id . 0 25.18 | g | Protein | Composition | 1008 | . 1 29.20 | g | Total lipid (fat) | Composition | 1008 | . 2 3.06 | g | Carbohydrate, by difference | Composition | 1008 | . 3 3.28 | g | Ash | Other | 1008 | . 4 376.00 | kcal | Energy | Energy | 1008 | . Combine info and nutrient dfs . foods = pd.merge(info, nutrients).drop_duplicates() foods.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 375176 entries, 0 to 389354 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 food 375176 non-null object 1 id 375176 non-null int64 2 manufacturer 293054 non-null object 3 fgroup 375176 non-null object 4 value 375176 non-null float64 5 units 375176 non-null object 6 nutrient 375176 non-null object 7 ngroup 375176 non-null object dtypes: float64(1), int64(1), object(6) memory usage: 25.8+ MB . Plot nutrient content by food group . nutrient = &#39;Carbohydrate, by difference&#39; (foods[foods.nutrient.isin([nutrient])] .groupby(&#39;fgroup&#39;) .value .quantile(.5) .sort_values() .plot(kind=&#39;barh&#39;, figsize=(8, 6)) .set(xlabel=&#39;Median %s content&#39; % nutrient, ylabel=&#39;&#39;)); . Find the food with the maxium nutritional content for each nutrient . get_max = lambda x: x.loc[x.value.idxmax()] foods.groupby(&#39;nutrient&#39;).apply(get_max).head() . food id manufacturer fgroup value units nutrient ngroup . nutrient . Adjusted Protein Baking chocolate, unsweetened, squares | 19078 | | Sweets | 12.900 | g | Adjusted Protein | Composition | . Alanine Gelatins, dry powder, unsweetened | 19177 | | Sweets | 8.009 | g | Alanine | Amino Acids | . Alcohol, ethyl Alcoholic beverage, distilled, all (gin, rum, ... | 14533 | | Beverages | 42.500 | g | Alcohol, ethyl | Other | . Arginine Seeds, sesame flour, low-fat | 12033 | | Nut and Seed Products | 7.436 | g | Arginine | Amino Acids | . Ash Desserts, rennin, tablets, unsweetened | 19225 | | Sweets | 72.500 | g | Ash | Other | . FEC 2012 presidential election campaign contributions . From Python for Data Analysis . columns = {&#39;cand_nm&#39;:&#39;candidate&#39;, &#39;contbr_city&#39;:&#39;city&#39;, &#39;contbr_occupation&#39;:&#39;occupation&#39;, &#39;contb_receipt_amt&#39;:&#39;amount&#39;, &#39;contb_receipt_dt&#39;:&#39;date&#39;} parties = {&#39;Bachmann, Michelle&#39;: &#39;r&#39;, &#39;Romney, Mitt&#39;: &#39;r&#39;, &#39;Obama, Barack&#39;: &#39;d&#39;, &quot;Roemer, Charles E. &#39;Buddy&#39; III&quot;: &#39;r&#39;, &#39;Pawlenty, Timothy&#39;: &#39;r&#39;, &#39;Johnson, Gary Earl&#39;: &#39;r&#39;, &#39;Paul, Ron&#39;: &#39;r&#39;, &#39;Santorum, Rick&#39;: &#39;r&#39;, &#39;Cain, Herman&#39;: &#39;r&#39;, &#39;Gingrich, Newt&#39;: &#39;r&#39;, &#39;McCotter, Thaddeus G&#39;: &#39;r&#39;, &#39;Huntsman, Jon&#39;: &#39;r&#39;, &#39;Perry, Rick&#39;: &#39;r&#39;} path = &#39;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/fec/P00000001-ALL.csv&#39; fec = (pd.read_csv(path) [columns.keys()] .rename(columns=columns) .assign(party = lambda df: df.candidate.map(parties))) . /Users/fgu/miniconda3/envs/basics/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . fec.head() . candidate city occupation amount date party . 0 Bachmann, Michelle | MOBILE | RETIRED | 250.0 | 20-JUN-11 | r | . 1 Bachmann, Michelle | MOBILE | RETIRED | 50.0 | 23-JUN-11 | r | . 2 Bachmann, Michelle | LANETT | INFORMATION REQUESTED | 250.0 | 05-JUL-11 | r | . 3 Bachmann, Michelle | PIGGOTT | RETIRED | 250.0 | 01-AUG-11 | r | . 4 Bachmann, Michelle | HOT SPRINGS NATION | RETIRED | 300.0 | 20-JUN-11 | r | . Compare total donations . fec.groupby(&#39;party&#39;).amount.sum() . party d 1.335026e+08 r 1.652488e+08 Name: amount, dtype: float64 . Compare donations by occupation . occ_mapping = {&#39;INFORMATION REQUESTED&#39;:&#39;Not provided&#39;, &#39;INFORMATION REQUESTED PER BEST EFFORTS&#39;: &#39;Not provided&#39;} f = lambda x: occ_mapping.get(x, x) fec.occupation = fec.occupation.map(f) fec . candidate city occupation amount date party . 0 Bachmann, Michelle | MOBILE | RETIRED | 250.0 | 20-JUN-11 | r | . 1 Bachmann, Michelle | MOBILE | RETIRED | 50.0 | 23-JUN-11 | r | . 2 Bachmann, Michelle | LANETT | Not provided | 250.0 | 05-JUL-11 | r | . 3 Bachmann, Michelle | PIGGOTT | RETIRED | 250.0 | 01-AUG-11 | r | . 4 Bachmann, Michelle | HOT SPRINGS NATION | RETIRED | 300.0 | 20-JUN-11 | r | . ... ... | ... | ... | ... | ... | ... | . 1001726 Perry, Rick | INFO REQUESTED | Not provided | 5000.0 | 29-SEP-11 | r | . 1001727 Perry, Rick | INFO REQUESTED | BUSINESS OWNER | 2500.0 | 30-SEP-11 | r | . 1001728 Perry, Rick | INFO REQUESTED | Not provided | 500.0 | 29-SEP-11 | r | . 1001729 Perry, Rick | INFO REQUESTED | LONGWALL MAINTENANCE FOREMAN | 500.0 | 30-SEP-11 | r | . 1001730 Perry, Rick | INFO REQUESTED | Not provided | 2500.0 | 31-AUG-11 | r | . 1001731 rows × 6 columns . Discretise donations into buckets for contribution size . Seaborn . From Python Data Science Handbook . Create a simple random walk plot using default . rng = np.random.RandomState(2312) x = np.linspace(0, 10, 500) y = np.cumsum(rng.randn(500, 6), axis=0) plt.plot(x, y) plt.legend(&#39;abcdef&#39;, ncol=2, loc=&#39;lower left&#39;); . import seaborn as sns sns.set() plt.plot(x, y); plt.legend(&#39;abcdef&#39;, ncol=2, loc=&#39;lower left&#39;); . Explore marathon data . from datetime import timedelta # Read data def convert_time(s): h, m, s = map(int, s.split(&#39;:&#39;)) return timedelta(hours=h, minutes=m, seconds=s) file = &#39;https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv&#39; df = pd.read_csv(file, converters={&#39;split&#39;: convert_time, &#39;final&#39;: convert_time}) # Add times in seconds df[&#39;split_sec&#39;] = df.split.astype(int) / 1E9 df[&#39;final_sec&#39;] = df.final.astype(int) / 1E9 print(df.shape) df.head() . (37250, 6) . age gender split final split_sec final_sec . 0 33 | M | 01:05:38 | 02:08:51 | 3938.0 | 7731.0 | . 1 32 | M | 01:06:26 | 02:09:28 | 3986.0 | 7768.0 | . 2 31 | M | 01:06:49 | 02:10:42 | 4009.0 | 7842.0 | . 3 38 | M | 01:06:16 | 02:13:45 | 3976.0 | 8025.0 | . 4 31 | M | 01:06:32 | 02:13:59 | 3992.0 | 8039.0 | . with sns.axes_style(&#39;white&#39;): g = sns.jointplot(&#39;split_sec&#39;, &#39;final_sec&#39;, data=df, kind=&#39;hex&#39;) g.ax_joint.plot(np.linspace(4000, 17000), np.linspace(8000, 33000), &#39;:&#39;) . df[&#39;split_frac&#39;] = 1 - 2 * df.split_sec / df.final_sec sns.distplot(df.split_frac, kde=False) plt.axvline(0, linestyle=&#39;--&#39;); . g = sns.PairGrid(df, hue=&#39;gender&#39;, vars=[&#39;age&#39;, &#39;split_frac&#39;, &#39;final_sec&#39;, &#39;split_sec&#39;]) g.map(sns.scatterplot, alpha=0.5) g.add_legend(); . sns.kdeplot(df.split_frac[df.gender==&#39;M&#39;], label=&#39;Men&#39;, shade=True) sns.kdeplot(df.split_frac[df.gender==&#39;W&#39;], label=&#39;Women&#39;, shade=True); . sns.violinplot(&#39;gender&#39;, &#39;split_frac&#39;, data=df); . df[&#39;age_dec&#39;] = df.age.map(lambda age: age // 10 * 10) sns.violinplot(&#39;age_dec&#39;, &#39;split_frac&#39;, hue=&#39;gender&#39;, split=True, data=df); . g = sns.lmplot(&#39;final_sec&#39;, &#39;split_frac&#39;, col=&#39;gender&#39;, data=df, markers=&#39;.&#39;, scatter_kws=dict(color=&#39;cornflowerblue&#39;)) g.map(plt.axhline, y=0.1, color=&#39;k&#39;, ls=&#39;:&#39;); . Sources . Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/dataviz/2020/12/10/dataviz.html",
            "relUrl": "/python/pandas/dataviz/2020/12/10/dataviz.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Concurrency",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Theory . Elements: . RAM | memory controller (interface between cup and ram) | CPU | Kernel . Lowest level of software. It manages cpu resources, file system, memory resources, drivers, networking, etc. | . | Program . Code and data structures that can be executed (like newspaper) | . | . Cores (units of cpu?) . Multi core: instead of dividing up cpu time among different processes, can actually run stuff in parallel, but within each core, the process is exactly the same as with only one core. | . | Process . A logical container that tells kernel what program to run. Process has id, priority, status, memory space, etc.. Kernel uses that info to allocate cpu time to each program. Because happens at milisecond level, creates illusion of concurrency (comic analogy). | A program that competes for CPU resources (e.g. a web browser, or a bit of cleaning code) | The program in execution. Given that program is composed of multiple threads, program is program + state of all threads executing the program | . | Threads . Threads run within a single process, to allow it to do multiple things at once (like a newspaper) | They are lines of control cursing through the code and data structures of the program (called a thread of execution through the program). Akin to one reader scanning through a section of the newspaper. | Multiple threads: different lines of control cursing through the program (multiple readers reading different sections of the newspaper). | Potential conflict: threads operate on (read/update) same data structures. Solution: locks | A ... running within a program (e.g. reading data, processing, writing to disk) | . | . Type of tasks: . CPU-bound tasks: using CPU to crunch numbers . | I/O-bound tasks: waiting for some I/O operations to complete (downloading files from internet, reading and writing to file-system) . | . Theading: . Gives speed ups for I/O-bound tasks | . Multiprocessing: . Run CPU-bound tasks in prallel | . Usage . os.cpu_count() . 8 . Sources . Udacity on process vs threads | Gary explains - processes and threads | Gary explains - what is a kernel | Corey Schafer on threading | Fluent Python | Python Cookbook | Learning Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/12/10/concurrency.html",
            "relUrl": "/python/2020/12/10/concurrency.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Beautiful code",
            "content": "Sources to look at: . Hitchhiker&#39;s guide (link below) | IPython cookbook | . &quot;Terseness and obscurity are the limits where brevity should stop&quot; -- The Hitchhiker&#39;s Guide to Python . Use common design patterns . Use extra variables and functions instead of comments . Naming . Be descriptive. | Use plain and unabbreviated words. | Be concise and omit needless words (e.g. &quot;get&quot;, &quot;calculate&quot;) | . Add a docstring to each function . I roughly follow PEP-257 and Google. | Omit docstring if function is short and obvious (the one below would easily qualify...). | . def sum_numbers(num1=0, num2=1): &quot;&quot;&quot;Sum two numbers. Arguments: num1: the first number (default, 0) num2: the second number (default, 1) Return: Sum of numbers. &quot;&quot;&quot; return num1 + num2 . One function performs one task . import pandas as pd df = pd.DataFrame({&#39;data&#39;: [1, 2, 3, 4]}) # bad def calc_and_print_stats(df): mean = df.data.mean() maximum = df.data.max() print(f&#39;Mean is {mean}&#39;) print(f&#39;Max is {maximum}&#39;) calc_and_print_stats(df) . Mean is 2.5 Max is 4 . def calc_stats(df): return df.data.mean(), df.data.max() def print_stats(stats): print(f&#39;Mean is {stats[0]}&#39;) print(f&#39;Max is {stats[1]}&#39;) stats = calc_stats(df) print_stats(stats) . Mean is 2.5 Max is 4 . Principles . Based on this . 1. . Return a value | Such as True, to show that the function completed when there is nothing obvious to return. . Keep them short | Less than 50 lines as a rule of thumb. . Idempotent and pure | Idempotent functions return the same output for a given input every time, pure functions are idempotent and have no side-effects. . Main sources . The Hitchhiker&#39;s Guide to Python, code style | IPython cookbook, writing high-quality Python code | Google style guide | Jeff Knupp post | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/12/10/beautiful-code.html",
            "relUrl": "/python/2020/12/10/beautiful-code.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "AWS",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Loading data from S3 . import s3fs . List content of bucket . bucket = &#39;fgu-mdb&#39; fs = s3fs.S3FileSystem() fs.ls(bucket) . [&#39;fgu-mdb/data_000.csv&#39;] . Read file from S3 . file = &#39;data_000.csv&#39; df = pd.read_csv(f&#39;s3://{bucket}/{file}&#39;, sep=&#39;|&#39;) df.shape . (1000, 27) . Sources .",
            "url": "https://fabiangunzinger.github.io/blog/python/aws/2020/12/02/aws.html",
            "relUrl": "/python/aws/2020/12/02/aws.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Simulations",
            "content": "import numpy as np . Simulating loan repayments using Monte Carlo simulation . Entirely based on this excellent QuantEcon tutorial. . A small company makes loans of up to 25k to be repaid a year later. 75 percent of loans are repaid in full, 20 percent of loans are repaid in half, the rest are not repaid. The company discounts the future at an annual rate of 5 percent. How much would the company be willing to loan if it wants to break even on average? . We can calculate this by hand. One year from now, the average repayment is 0.75(25000) + 0.2(12500) + 0.05(0) = 21250. Because that repayment occurs a year from now while the loan is made today, the company calculates the repayment&#39;s present value as: (1/1.05)21250 = 20238.1. Assuming that loan repayment is independent of loan size, then this is the loan size for which the company will break even on average (if it makes enough loans). . We can verify this using a simulation: . def repayment_simulator(n, r=0.05, full_repayment=25_000, partial_repayment=12_500): &quot;&quot;&quot;Simulate present value of repayments for N loans.&quot;&quot;&quot; repayments = np.zeros(n) outcome = np.random.rand(n) full_repayments = outcome &lt;= 0.75 repayments[full_repayments] = full_repayment partial_repayments = (outcome &lt;= 0.95) &amp; ~full_repayments repayments[partial_repayments] = partial_repayment return (1 / (1 + r)) * repayments np.mean(repayment_simulator(10_000_000)) . 20237.311904761867 . Instead of breaking even, the company is now looking for the largest loan size that will give it a 95 percent chance of being profitable in a year it makes 250 loans. . def simulate_year(n, k): &quot;&quot;&quot;Simulate a year with N loans k times.&quot;&quot;&quot; year_avgs = np.zeros(k) for year in range(k): year_avgs[year] = np.mean(repayment_simulator(n)) return year_avgs np.percentile(simulate_year(250, 10_000), [5, 50, 95]) . array([19523.80952381, 20238.0952381 , 20904.76190476]) . The result tells us that in 95 percent of simulated years, the average repayment is larger than 19,523. Hence, if the company wants to make a profit with 95 percent probability, it should loan no more than this amount. . Sources . QuantEcon | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/11/03/simulations.html",
            "relUrl": "/python/2020/11/03/simulations.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Cool Python tricks",
            "content": "Coerce input to type of something else . type(&#39;s&#39;)(5) . &#39;5&#39; . Sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/10/18/cool-tricks.html",
            "relUrl": "/python/2020/10/18/cool-tricks.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Built in heroes",
            "content": "Title inspired by this David Beazley talk. My notes on useful built in functions. . import functools import itertools import operator . Iterable unpacking . When looping over a list of records (maybe of unequal length), we can access each records elements directly using star expressions. (From Python Cookbook recipe 1.2.) . records = [ (&#39;foo&#39;, 1, 2), (&#39;bar&#39;, &#39;hello&#39;) ] . for record in records: print(record) . (&#39;foo&#39;, 1, 2) (&#39;bar&#39;, &#39;hello&#39;) . for a, *b in records: print(a) print(b) . foo [1, 2] bar [&#39;hello&#39;] . def do_foo(x, y): print(f&#39;foo: args are {x} and {y}.&#39;) def do_bar(x): print(f&#39;bar: arg is {x}.&#39;) for tag, *args in records: if tag == &#39;foo&#39;: do_foo(*args) elif tag == &#39;bar&#39;: do_bar(*args) . foo: args are 1 and 2. bar: arg is hello. . Creating a callable_iterator . Roll a die until a 6 is rolled . import random def roll(): return random.randint(1, 6) roll_iter = iter(roll, 6) roll_iter . &lt;callable_iterator at 0x112c75850&gt; . for r in roll_iter: print(r) . 1 2 4 2 . list(iter(roll, 4)) . [3, 1, 2, 5, 3, 5, 3] . To read file until an empty line: . with open(&#39;filepath&#39;) as f: for line in iter(f.readline, &#39;&#39;): process_line(line) . itertools.starmap() . Using starmap() to calculate a running average: . numbers = range(1, 11, 4) list(itertools.starmap(lambda a, b: b / a, enumerate(itertools.accumulate(numbers), 1))) . [1.0, 3.0, 5.0] . list(itertools.accumulate(numbers)) . [1, 6, 15] . list(enumerate(itertools.accumulate(numbers), 1)) . [(1, 1), (2, 6), (3, 15)] . Multiplying characters . name = &#39;Emily&#39; list(itertools.starmap(operator.mul, enumerate(name, 1))) . [&#39;E&#39;, &#39;mm&#39;, &#39;iii&#39;, &#39;llll&#39;, &#39;yyyyy&#39;] . functools.reduce() . import pandas as pd df = pd.DataFrame({&#39;AAA&#39;: [4, 5, 6, 7], &#39;BBB&#39;: [10, 20, 30, 40], &#39;CCC&#39;: [100, 50, -30, -50]}) . What I usually do . crit1 = df.AAA &gt; 5 crit2 = df.BBB &gt; 30 crits = crit1 &amp; crit2 df[crits] . AAA BBB CCC . 3 7 | 40 | -50 | . Alternative using functools.reduce() . import functools crit1 = df.AAA &gt; 5 crit2 = df.BBB &gt; 30 crits = [crit1, crit2] mask = functools.reduce(lambda x, y: x &amp; y, crits) df[mask] . AAA BBB CCC . 3 7 | 40 | -50 | . Understanding dropwhile() . From docs . def dropwhile(predicate, iterable): iterable = iter(iterable) for x in iterable: if not predicate(x): yield x break for x in iterable: yield x predicate = lambda x: x &lt; 5 iterable = [1, 2, 3, 6, 7, 3] list(dropwhile(predicate, iterable)) . [6, 7, 3] . What happens here? . iter() is used so that the iterable becomes an iterator (which gets emptied as it&#39;s being iterated over). . | The first for loop moves until the first element fails the condition in predicate, at which point that element is yielded and the program breakes out of that for loop, advancing to the next. . | Because of step 1, iterable now only contains all elements after the element that caused the previous for loop to break, and all of these are yielded. . | def sensemaker(predicate, iterable): iterable = iter(iterable) for x in iterable: if not predicate(x): print(&#39;First loop&#39;) print(x) break print(&#39;Second loop&#39;) for x in iterable: print(x) sensemaker(predicate, iterable) . First loop 6 Second loop 7 3 . def sensemaker(predicate, iterable): # iterable = iter(iterable) for x in iterable: if not predicate(x): print(&#39;First loop&#39;) print(x) break print(&#39;Second loop&#39;) for x in iterable: print(x) sensemaker(predicate, iterable) . First loop 6 Second loop 1 2 3 6 7 3 . If we don&#39;t turn the iterable into an iterator, it doesn&#39;t get exhausted and the second loop simply loops over all its objects. . From more itertools . import more_itertools more_itertools.take(4, more_itertools.pairwise(itertools.count())) . [(0, 1), (1, 2), (2, 3), (3, 4)] . Sources . Fluent Python | Pandas cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/10/18/builtin-heroes.html",
            "relUrl": "/python/pandas/2020/10/18/builtin-heroes.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Pythonic objects",
            "content": "Practicing the use of special methods. . An artificially powerful French card deck . Based on part 4 in Fluent Python. . import collections import functools import numbers import operator Card = collections.namedtuple(&#39;Card&#39;, [&#39;rank&#39;, &#39;suit&#39;]) class FrenchDeck: ranks = [str(n) for n in range(2, 11)] + list(&#39;JQKA&#39;) suits = &#39;spades diamonds clubs hearts&#39;.split() def __init__(self, cards=None): if cards is None: self._cards = [Card(rank, suit) for suit in self.suits for rank in self.ranks] else: self._cards = cards def __len__(self): return len(self._cards) # basic implementation returns a list def __getitem__(self, position): return self._cards[position] # make slicing return a FrenchDeck rather than a list # this is what requires the conditional init statement def __getitem__(self, index): cls = type(self) if isinstance(index, slice): return cls(self._cards[index]) if isinstance(index, numbers.Integral): return self._cards[index] else: msg = &#39;{cls.__name__} indices must be integers&#39; raise TypeError(msg.format(cls=cls)) def __str__(self): cards = [(card.suit, card.rank) for card in self._cards] return str(tuple(cards)) def __repr__(self): cards = [(card.suit, card.rank) for card in self._cards] return &#39;FrenchDeck({})&#39;.format(cards) shortcut_names = &#39;abc&#39; # access first three elements with abc shortcut def __getattr__(self, name): cls = type(self) if len(name) == 1: pos = self.shortcut_names.find(name) if 0 &lt;= pos &lt; len(self._cards): return self._cards[pos] msg = &#39;{.__name__!r} object has no attribute {!r}&#39; raise AttributeError(msg.format(cls, name)) # avoid attribute setting to avoid confusion with abc shortcuts def __setattr__(self, name, value): cls = type(self) if len(name) == 1: if name in self.shortcut_names: error = &#39;readonly attribute {attr_name!r}&#39; elif name.islower(): error = &quot;can&#39;t set attributes &#39;a&#39; to &#39;z&#39; in {cls_name!r}&quot; else: error = &#39;&#39; if error: msg = error.format(attr_name=name, cls_name=cls.__name__) raise AttributeError(msg) super().__setattr__(name, value) # basic comparison def __eq__(self, other): return tuple(self) == tuple(other) # more efficient comparison for large deck def __eq__(self, other): return (len(self) == len(other) and all(a == b for a, b in zip(self, other))) def __hash__(self): hashes = map(hash, self._cards) return functools.reduce(operator.xor, hashes) # totally artificial format attribute def __format__(self, fmt): cards = ((format(card.rank, fmt), card.suit) for card in deck._cards[:3]) return &#39;{}, {}, {}&#39;.format(*cards) deck = FrenchDeck() # print(deck[3:5]) # print(deck.a) # deck[2:3] ddeck = FrenchDeck() format(deck, &#39;-^5&#39;) . &#34;(&#39;--2--&#39;, &#39;spades&#39;), (&#39;--3--&#39;, &#39;spades&#39;), (&#39;--4--&#39;, &#39;spades&#39;)&#34; . FrenchDeck as a subclass of an existing ABC, collections.MutableSequence. . import collections Card = collections.namedtuple(&#39;Card&#39;, [&#39;card&#39;, &#39;suit&#39;]) class FrenchDeck2(collections.MutableSequence): cards = [str(n) for n in range(2, 11)] + list(&#39;JQKA&#39;) suits = &#39;spandes clubs diamonds hearts&#39;.split() def __init__(self): self._cards = [Card(card, suit) for suit in self.suits for card in self.cards] def __len__(self): return len(self._cards) def __getitem__(self, position): return self._cards[position] def __iter__(self): return (c for c in self._cards) def __delitem__(self, position): del self._cards[position] def __setitem__(self, key, value): self._cards[key] = value def insert(self, position, value): self._cards.insert(position, value) a = FrenchDeck2() a.insert(1, &#39;hello&#39;) len(a) a[1] = &#39;ha&#39; len(a) a[:3] list(reversed(a[:5])) . [Card(card=&#39;5&#39;, suit=&#39;spandes&#39;), Card(card=&#39;4&#39;, suit=&#39;spandes&#39;), Card(card=&#39;3&#39;, suit=&#39;spandes&#39;), &#39;ha&#39;, Card(card=&#39;2&#39;, suit=&#39;spandes&#39;)] . A helper class to get data out of a closure . From Effective Python Item 15. We want to order items in a list such that priority items are listed first, and we want a flag to indicate whether the list contained any priority items. . Using a function and nonlocal: . numbers = [4, 6, 5, 2, 7, 8, 9, 0, 1, 3] group = {3, 4, 5} def sort_priority(numbers, group): found = False def helper(x): nonlocal found if x in group: found = True return (0, x) return (1, x) numbers.sort(key=helper) return found sort_priority(numbers, group) . True . numbers . [3, 4, 5, 0, 1, 2, 6, 7, 8, 9] . This works. But Slatkin is not too keen on the nonlocal keyword except for use in very simple functions (the above would most definitely qualify), and defines a helper class to achieve the same result. . numbers = [4, 6, 5, 2, 7, 8, 9, 0, 1, 3] group = {3, 4, 5} class Sorter(object): def __init__(self, group): self.group = group self.found = False def __call__(self, x): if x in self.group: self.found = True return (0, x) return (1, x) sorter = Sorter(group) numbers.sort(key=sorter) assert sorter.found is True numbers . [3, 4, 5, 0, 1, 2, 6, 7, 8, 9] . sorter(5), sorter(7) . ((0, 5), (1, 7)) . Sources . Effective Python | Fluent Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/10/14/pythonic-objects.html",
            "relUrl": "/python/2020/10/14/pythonic-objects.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Mutable function parameters",
            "content": "Notes based on chapter 8 in Fluent Python. . Functions that take mutable objects as arguments require caution, because function arguments are aliases for the passed arguments. This can cause unintended behaviour in two types of situations: . When setting a mutable object as default | When aliasing a mutable object passed to the constructor | . Setting a mutable object as default . class HauntedBus(): def __init__(self, passengers=[]): self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) bus1 = HauntedBus([&#39;pete&#39;, &#39;lara&#39;, &#39;nick&#39;]) bus1.drop(&#39;nick&#39;) print(bus1.passengers) bus2 = HauntedBus() bus2.pick(&#39;heather&#39;) print(bus2.passengers) bus3 = HauntedBus() bus3.pick(&#39;katy&#39;) print(bus3.passengers) . [&#39;pete&#39;, &#39;lara&#39;] [&#39;heather&#39;] [&#39;heather&#39;, &#39;katy&#39;] . Between bus 1 and 2, all works as intended, since we passed our own list when creating bus 1. Then things get a bit weird, though: how did Heather get into bus 3? When we define the HauntedBus class, we create a single empty list that is kept in the background and will be used whenever we instantiate a new bus without a custom passenger list. Importantly, all such buses will operate on the same list. We can see this by checking the object ids of the three buses&#39; passenger lists: . assert bus1.passengers is not bus2.passengers assert bus2.passengers is bus3.passengers . This shows that while the passenger list of bus 1 and 2 are not the same object, the lists of bus 2 and 3 are. Once we know that, the above behaviour makes sense: all passenger list operations on buses without a custom list operate on the same list. Anohter way of seeing this by inspecting the default dict of HauntedBus after our operations abve. . HauntedBus.__init__.__defaults__ . ([&#39;heather&#39;, &#39;katy&#39;],) . The above shows that after the bus3.pick(&#39;katy&#39;) call above, the default list is now changed, and will be inherited by future instances of HauntedBus. . bus4 = HauntedBus() bus4.passengers . [&#39;heather&#39;, &#39;katy&#39;] . This behaviour is an example of why it matters whether we think of variables as boxes or labels. If we think that variables are boxes, then the above bevaviour doesn&#39;t make sense, since each passenger list would be its own box with its own content. But when we think of variables as labels -- the correct way to think about them in Python -- then the behaviour makes complete sense: each time we instantiate a bus without a custom passenger list, we create a new label -- of the form name-of-bus.passengers -- for the empty list we created when we loaded or created HauntedBus. . What to do to avoid the unwanted behaviour? The solution is to create a new empty list each time no list is provided. . class Bus(): def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = list(passengers) def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) bus1 = Bus() bus1.pick(&#39;tim&#39;) bus2 = Bus() bus2.passengers . [] . Aliasing a mutable object argument inside the function . The init method of the above class copies the passed passenger list by calling list(passengers). This is critical. If, instead of copying we alias the passed list, we change lists defined outside the function that are passed as arguments, which is probably not what we want. . class Bus(): def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) team = [&#39;hugh&#39;, &#39;lisa&#39;, &#39;gerd&#39;, &#39;adam&#39;, &#39;emily&#39;] bus = Bus(team) bus.drop(&#39;hugh&#39;) team . [&#39;lisa&#39;, &#39;gerd&#39;, &#39;adam&#39;, &#39;emily&#39;] . Again, the reason for this is that self.passengers is an alias for passengers, which is itself an alias for team, so that all operations we perfom on self.passengers are actually performed on team. The identity check below shows what the passengers attribute of bus is indeed the same object as the team list. . bus.passengers is team . True . To summarise: unless there is a good reason for an exception, for functions that take mutable objects as arguments do the following: . Create a new object each time a class is instantiated by using None as the default parameter, rather than creating the object at the time of the function definition. . | Make a copy of the mutable object for processing inside the function to leave the original object unchanged. . | Main sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/10/05/mutable-function-parameters.html",
            "relUrl": "/python/2020/10/05/mutable-function-parameters.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Documenting sample selection",
            "content": "import numpy as np import pandas as pd . Problem . I have a dataframe on which I perform a series of data selection steps. What I want is to automatically build a table for the appendix of my paper that tells me the number of users left in the data after each selection step. . Here&#39;s a mock dataset: . df = (pd.DataFrame({&#39;user_id&#39;: [1, 2, 3, 4] * 2, &#39;data&#39;: np.random.rand(8)}) .sort_values(&#39;user_id&#39;)) df . user_id data . 0 1 | 0.382558 | . 4 1 | 0.923332 | . 1 2 | 0.606272 | . 5 2 | 0.760290 | . 2 3 | 0.103158 | . 6 3 | 0.044038 | . 3 4 | 0.708062 | . 7 4 | 0.955849 | . here some selection functions: . def first_five(df): return df[:5] def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def select_sample(df): return ( df .pipe(first_five) .pipe(n_largest) ) select_sample(df) . user_id data . 4 1 | 0.923332 | . 5 2 | 0.760290 | . 1 2 | 0.606272 | . Solution . If we have a single dataframe on which to perform selection, as in the setting above, we can use a decorator and a dictionary. . As a first step, let&#39;s build a decorator that prints out the number of users after applying each function: . from functools import wraps def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() print(f&#39;{func.__name__}: {num_users}&#39;) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def select_sample(df): return ( df .pipe(first_five) .pipe(n_largest) ) select_sample(df) . first_five: 3 n_largest: 2 . user_id data . 4 1 | 0.923332 | . 5 2 | 0.760290 | . 1 2 | 0.606272 | . That&#39;s already nice. But I need those counts for the data appendix of my paper, so what I really want is to store the counts in a container that I can turn into a table. To do this, we can store the counts in a dictionary instead of printing them. . counts = dict() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def select_sample(df): return ( df .pipe(first_five) .pipe(n_largest) ) display(select_sample(df)) counts . user_id data . 7 7 | 0.961156 | . 2 2 | 0.934121 | . 0 0 | 0.771461 | . {&#39;first_five&#39;: 5, &#39;n_largest&#39;: 3} . Next, I want to add the number of users at the beginning and the end of the process (the count at the end is identical with the final step, but I think it&#39;s worth adding so readers can easily spot the final numbers). . counts = dict() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def add_user_count(df, step): num_users = df.user_id.nunique() counts.update({step: num_users}) return df def select_sample(df): return ( df .pipe(add_user_count, &#39;start&#39;) .pipe(first_five) .pipe(n_largest) .pipe(add_user_count, &#39;end&#39;) ) display(select_sample(df)) counts . user_id data . 7 7 | 0.961156 | . 2 2 | 0.934121 | . 0 0 | 0.771461 | . {&#39;start&#39;: 6, &#39;first_five&#39;: 5, &#39;n_largest&#39;: 3, &#39;end&#39;: 3} . We&#39;re nearly there. Let&#39;s turn this into a table that we can store to disk (as a Latex table, say) and automatically import in our paper. . table = pd.DataFrame(counts.items(), columns=[&#39;Processing step&#39;, &#39;Number of unique users&#39;]) table . Processing step Number of unique users . 0 start | 4 | . 1 first_five | 4 | . 2 n_largest | 3 | . 3 end | 3 | . Finally, let&#39;s make sure readers of our paper (and we ourselves a few weeks from now) actually understand what&#39;s going on at each step. . description = { &#39;start&#39;: &#39;Raw dataset&#39;, &#39;first_five&#39;: &#39;Keep first five observations&#39;, &#39;n_largest&#39;: &#39;Keep three largest datapoints&#39;, &#39;end&#39;: &#39;Final dataset&#39; } table[&#39;Processing step&#39;] = table[&#39;Processing step&#39;].map(description) table . Processing step Number of unique users . 0 Raw dataset | 4 | . 1 Keep first five observations | 4 | . 2 Keep three largest datapoints | 3 | . 3 Final dataset | 3 | . That&#39;s it. We can can now export this as a Latex table (or some other format) and automatically load it in our paper. . Multiple datasets . Instead of having a single dataframe on which to perform selection, I actually have multiple pieces of a large dataframe (because the full dataframe doesn&#39;t fit into memory). What I want is to perform the data selection on each chunk separately but have the values in the counter object add up so that -- at the end -- the counts represent the counts for the full dataset. The solution here is to use collection.Counter() instead of a dictionary. . So, my setup is akin to the following: . large_df = pd.DataFrame({&#39;user_id&#39;: list(range(12)), &#39;data&#39;: np.random.rand(12)}) large_df . user_id data . 0 0 | 0.771461 | . 1 1 | 0.091924 | . 2 2 | 0.934121 | . 3 3 | 0.170945 | . 4 4 | 0.132203 | . 5 5 | 0.854464 | . 6 6 | 0.469576 | . 7 7 | 0.961156 | . 8 8 | 0.363705 | . 9 9 | 0.560698 | . 10 10 | 0.769054 | . 11 11 | 0.019288 | . buckets = pd.cut(large_df.user_id, bins=2) raw_pieces = [data for key, data in large_df.groupby(buckets)] for piece in raw_pieces: display(piece) . user_id data . 0 0 | 0.771461 | . 1 1 | 0.091924 | . 2 2 | 0.934121 | . 3 3 | 0.170945 | . 4 4 | 0.132203 | . 5 5 | 0.854464 | . user_id data . 6 6 | 0.469576 | . 7 7 | 0.961156 | . 8 8 | 0.363705 | . 9 9 | 0.560698 | . 10 10 | 0.769054 | . 11 11 | 0.019288 | . What happens if we use a dict() as our counts object as we did above. . counts = dict() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def add_user_count(df, step=&#39;start&#39;): num_users = df.user_id.nunique() counts.update({step: num_users}) return df def select_sample(df): return ( df .pipe(add_user_count) .pipe(first_five) .pipe(n_largest) .pipe(add_user_count, &#39;end&#39;) ) selected_pieces = [] for piece in raw_pieces: selected_pieces.append(select_sample(piece)) print(counts) df = pd.concat(selected_pieces) . {&#39;start&#39;: 6, &#39;first_five&#39;: 5, &#39;n_largest&#39;: 3, &#39;end&#39;: 3} {&#39;start&#39;: 6, &#39;first_five&#39;: 5, &#39;n_largest&#39;: 3, &#39;end&#39;: 3} . The counts are replaced rather than added up, which is how updating works for a dictionary: . m = dict(a=1, b=2) n = dict(b=3, c=4) m.update(n) m . {&#39;a&#39;: 1, &#39;b&#39;: 3, &#39;c&#39;: 4} . collections.Counter() (docs) solve this problem. . import collections counts = collections.Counter() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def add_user_count(df, step=&#39;start&#39;): num_users = df.user_id.nunique() counts.update({step: num_users}) return df def select_sample(df): return ( df .pipe(add_user_count) .pipe(first_five) .pipe(n_largest) .pipe(add_user_count, &#39;end&#39;) ) selected_pieces = [] for piece in raw_pieces: selected_pieces.append(select_sample(piece)) print(counts) df = pd.concat(selected_pieces) . Counter({&#39;start&#39;: 6, &#39;first_five&#39;: 5, &#39;n_largest&#39;: 3, &#39;end&#39;: 3}) Counter({&#39;start&#39;: 12, &#39;first_five&#39;: 10, &#39;n_largest&#39;: 6, &#39;end&#39;: 6}) . Now, updating adds up the values for each key, just as we want. We can add the same formatting as we did above and are done with our table. . Background . Other cool stuff Counter() can do . o = Counter(a=1, b=2) p = Counter(b=3, c=-4) o.update(p) o . Counter({&#39;a&#39;: 1, &#39;b&#39;: 5, &#39;c&#39;: -4}) . Counters can also do cool things like this: . list(o.elements()) . [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;] . o.most_common(2) . [(&#39;b&#39;, 5), (&#39;a&#39;, 1)] . o - p . Counter({&#39;a&#39;: 1, &#39;b&#39;: 2}) . Why is counts a global variable? . Because I want all decorated functions to write to the same counter object. . Often, decorators make use of closures instead, which have access to a nonlocal variable defined inside the outermost function. Let&#39;s look at what happens if we do this for our user counter. . def user_counter(func): counts = Counter() @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) print(counts) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def largest(df): return df.loc[df.data.nlargest(3).index] def select(df): return ( df .pipe(first_five) .pipe(largest) ) result = select(df) . Counter({&#39;first_five&#39;: 4}) Counter({&#39;largest&#39;: 3}) . Now, each decorated function gets its own counter object, which is not what we want here. For more on decorator state retention options, see chapter 39 in Learning Python. . What are closures and nonlocal variables? . (Disclaimer: Just about all of the text and code on closures is taken -- sometimes verbatim -- from chapter 7 in Fluent Python. So the point here is not to produce new insight, but to absorb the material and write an easily accessible note to my future self.) . Closures are functions that have access to nonlocal arguments -- variabls that are neither local nor global, but are defined inside an outer function within which the closure was defined, and to which the closure has access. . Let&#39;s look at an example. A simple function that takes one number as an argument and returns the average of all numbers passed to it since it&#39;s definition. For this, we need a way to store all previously passed values. One way to do this is to define a class with a call method. . class Averager(): def __init__(self): self.series = [] def __call__(self, new_value): self.series.append(new_value) total = sum(self.series) return total / len(self.series) avg = Averager() avg(10), avg(20), avg(30) . (10.0, 15.0, 20.0) . Another way is to use a closure function and store the series of previously passed numbers as a free variable. . def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total / len(series) return averager avg = make_averager() avg(10), avg(20), avg(30) . (10.0, 15.0, 20.0) . This gives the same result, but is arguably simpler than defining a class. . We can improve the above function by storing previous results so that we don&#39;t have to calculate the new average from scratch at every function call. . def make_fast_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averager avg = make_fast_averager() avg(10), avg(11), avg(12) . (10.0, 10.5, 11.0) . %%timeit avg = make_averager() [avg(n) for n in range(10_000)] . 210 ms ± 2.72 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . %%timeit avg = make_fast_averager() [avg(n) for n in range(10_000)] . 1.57 ms ± 23.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . This simple change gives us a massive speedup. . Notice the nonlocal statement inside the averager function. Why do we need this? Let&#39;s see what happens if we don&#39;t specify it: . def make_fast_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total / count return averager avg = make_fast_averager() avg(10), avg(11), avg(12) . UnboundLocalError Traceback (most recent call last) &lt;ipython-input-27-9b1a9d59caaa&gt; in &lt;module&gt; 10 avg = make_fast_averager() 11 &gt; 12 avg(10), avg(11), avg(12) &lt;ipython-input-27-9b1a9d59caaa&gt; in averager(new_value) 3 total = 0 4 def averager(new_value): -&gt; 5 count += 1 6 total += new_value 7 return total / count UnboundLocalError: local variable &#39;count&#39; referenced before assignment . How come our fast averager can&#39;t find count and total even though our slow averager could find series just fine? . The answer lies in Python&#39;s variable scope rules and the difference between assigning to unmutable objects and updating mutable ones. . Whenever we assign to a variable inside a function, it is treated as a local variable. . | count += 1 is the same as count = count + 1, so we are assigning to count, which makes it a local variable (the same goes for total). We are assigning new values to count rather than updaing it because integers are immutable, so we can&#39;t update it. . | Lists are mutable, so series.append() doesn&#39;t create a new list, but merely appends to it, which doesn&#39;t count as an assignment, so that series is not treated as a local variable. . | Hence, we need to explicitly tell Python that count and total are nonlocal variables. . Main sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/30/documenting-sample-selection.html",
            "relUrl": "/python/2020/09/30/documenting-sample-selection.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Decorators",
            "content": "Decorators with arguments . I have a simple logger decorator. . def logger(func): calls = 0 def wrapper(*args, **kwargs): nonlocal calls calls +=1 print(f&#39;Call #{calls} of {func.__name__}&#39;) return func(*args, **kwargs) return wrapper @logger def greeter(): print(&#39;Hello&#39;) @logger def singer(): print(&#39;lalala&#39;) @logger def congratulator(): print(&#39;Congratulations!&#39;) greeter() greeter() singer() congratulator() . Call #1 of greeter Hello Call #2 of greeter Hello Call #1 of singer lalala Call #1 of congratulator Congratulations! . Now I want the ability to deactivate the logger for certain functions. So I wrap the decorator in a decorator factory, like so: . def param_logger(active=True): def decorator(func): calls = 0 def wrapper(*args, **kwargs): nonlocal calls if active: calls +=1 print(f&#39;Call #{calls} of {func.__name__}&#39;) return func(*args, **kwargs) return wrapper return decorator @param_logger() def greeter(): print(&#39;Hello&#39;) @param_logger(active=True) def singer(): print(&#39;lalala&#39;) @param_logger(active=False) def congratulator(): print(&#39;Congratulations!&#39;) greeter() greeter() singer() congratulator() . Call #1 of greeter Hello Call #2 of greeter Hello Call #1 of singer lalala Congratulations! . How does this work? I&#39;m not completely confident, actually, but this is how I explain it to myself. . In our initial logger function above, both the argument to the outer function (func) and the variable defined inside the outer function (calls) are free variables of the closure function wrapper, meaning that wrapper has access to them even though they are not bound inside wrapper. . If we remember that . @logger def singer(): print(&#39;lalala&#39;) . is equivalent to . singer = logger(singer) . then it&#39;s clear that we can get a view of the free variables of the decorated greeter variable like so: . logger(greeter).__code__.co_freevars . (&#39;calls&#39;, &#39;func&#39;) . Now, what are the free variables of param_logger? . param_logger().__code__.co_freevars . (&#39;active&#39;,) . This makes sense: active is the function argument and we do not define any additional variables inside the scope of param_logger, so given our result above, this is what we would expect. . But param_logger is a decorator factory and not a decorator, which means it produces a decorator at the time of decoration. So, what are the free variables of the decorator is produces? . Similar to above, remembering that . @param_logger() def singer(): print(&#39;lalala&#39;) . is equivalent to . singer = param_logger()(singer) . we can inspect the decorated singer function&#39;s free variables like so: . param_logger()(singer).__code__.co_freevars . (&#39;active&#39;, &#39;calls&#39;, &#39;func&#39;) . We can see that active is now an additional free variable that our wrapper function has access to, which provides us with the answer to our question: decorator factories work by producing decorators at decoration time and passing on the specified keyword to the decorated function. . A final point for those into aesthetics or coding consistency: we can tweak our decorator factory so that we can ommit the () if we pass no keyword arguments. . def logger(func=None, active=True): def decorator(func): calls = 0 def wrapper(*args, **kwargs): nonlocal calls if active: calls +=1 print(f&#39;Call #{calls} of {func.__name__}&#39;) return func(*args, **kwargs) return wrapper if func: return decorator(func) else: return decorator @logger def greeter(): print(&#39;Hello&#39;) @logger() def babler(): print(&#39;bablebalbe&#39;) @logger(active=True) def singer(): print(&#39;lalala&#39;) @logger(active=False) def congratulator(): print(&#39;Congratulations!&#39;) greeter() greeter() babler() singer() congratulator() . Call #1 of greeter Hello Call #2 of greeter Hello Call #1 of babler bablebalbe Call #1 of singer lalala Congratulations! . To understand what happens here, remember that decorating func with a decorator is equivalent to . func = decorator(func) . While decorating it with a decorator factory is equivalent to . func = decorator()(func) . The control flow inside the above decorator factory simply switches between these two cases: if logger gets a function argument, then that&#39;s akin to the first scenario, where the func argument is passed into decorator directly, and so the decorator factory returns decorator(func) to mimik this behaviour. If func is not passed, then we&#39;re in the standard decorator factory scenario above, and we simply return the decorator uncalled, just as any plain decorator factory would. . Recipe 9.6 in the Python Cookbook discusses a neat solution to the above for a registration decorator using functools.partial(), which I haven&#39;t managed to a scenario with a decorator factory. Might give it another go later. . Mistakes I often make . I often do the below: . from functools import wraps def decorator(func): @wraps def wrapper(*args, **kwargs): print(&#39;Func is called:&#39;, func.__name__) return func(*args, **kwargs) return wrapper @decorator def greeter(name): return f&#39;Hello {name}&#39; greeter(&#39;World&#39;) . AttributeError Traceback (most recent call last) &lt;ipython-input-8-d69c0930e7ac&gt; in &lt;module&gt; 12 return f&#39;Hello {name}&#39; 13 &gt; 14 greeter(&#39;World&#39;) ~/miniconda3/envs/habits/lib/python3.7/functools.py in update_wrapper(wrapper, wrapped, assigned, updated) 56 pass 57 else: &gt; 58 setattr(wrapper, attr, value) 59 for attr in updated: 60 getattr(wrapper, attr).update(getattr(wrapped, attr, {})) AttributeError: &#39;str&#39; object has no attribute &#39;__module__&#39; . What&#39;s wrong, there? @wraps should be @wraps(func). . from functools import wraps def decorator(func): @wraps(func) def wrapper(*args, **kwargs): print(&#39;Func is called:&#39;, func.__name__) return func(*args, **kwargs) return wrapper @decorator def greeter(name): return f&#39;Hello {name}&#39; greeter(&#39;World&#39;) . Func is called: greeter . &#39;Hello World&#39; . Main sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/30/decorators.html",
            "relUrl": "/python/2020/09/30/decorators.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "String formatting",
            "content": "Basics . import string print(string.digits) print(string.ascii_lowercase) print(string.punctuation) . 0123456789 abcdefghijklmnopqrstuvwxyz !&#34;#$%&amp;&#39;()*+,-./:;&lt;=&gt;?@[ ]^_`{|}~ . Cool example from the docs: . for align, text in zip(&#39;&lt;^&gt;&#39;, [&#39;left&#39;, &#39;center&#39;, &#39;right&#39;]): print(&#39;{0:{filler}{align}30}&#39;.format(text, filler=align, align=align)) . left&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; ^^^^^^^^^^^^center^^^^^^^^^^^^ &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;right . Unpack and nicely format nested city information . Example from page 29 in Fluent Python. Code is available here. . metro_areas = [ (&#39;Tokyo&#39;, &#39;JP&#39;, 36.933, (35.689722, 139.691667)), (&#39;Delhi NCR&#39;, &#39;IN&#39;, 21.935, (28.613889, 77.208889)), (&#39;Mexico City&#39;, &#39;MX&#39;, 20.142, (19.433333, -99.133333)), (&#39;New York-Newark&#39;, &#39;US&#39;, 20.104, (40.808611, -74.020386)), (&#39;Sao Paulo&#39;, &#39;BR&#39;, 19.649, (-23.547778, -46.635833)), ] print(&#39;{:15} | {:^9} | {:^9}&#39;.format(&#39;&#39;, &#39;lat.&#39;, &#39;long.&#39;)) fmt = &#39;{:15} | {:&gt;9.4f} | {:&gt;9.4f}&#39; for name, cc, pop, (lat, long) in metro_areas: if long &lt;= 0: print(fmt.format(name, lat, long)) . | lat. | long. Mexico City | 19.4333 | -99.1333 New York-Newark | 40.8086 | -74.0204 Sao Paulo | -23.5478 | -46.6358 . Playing around with the format specification . print(&#39;=&#39;*39) print(&#39;{:15} | {:^9} | {:^9}&#39;.format(&#39; &#39;, &#39;lat.&#39;, &#39;long.&#39;)) print(&#39;-&#39;*39) fmt = &#39;{:15} | {:&gt;9.4f} | {:&gt;9.4f}&#39; for name, cc, pop, (lat, long) in metro_areas: if long &lt;= 0: print(fmt.format(name, lat, long)) print(&#39;=&#39;*39) . ======================================= | lat. | long. Mexico City | 19.4333 | -99.1333 New York-Newark | 40.8086 | -74.0204 Sao Paulo | -23.5478 | -46.6358 ======================================= . fmt = &#39;{:15} | {:0=+20.5f} | {:20.5e}&#39; for name, cc, pop, (lat, long) in metro_areas: if long &lt;= 0: print(fmt.format(name, lat, long)) . Mexico City | +0000000000019.43333 | -9.91333e+01 New York-Newark | +0000000000040.80861 | -7.40204e+01 Sao Paulo | -0000000000023.54778 | -4.66358e+01 . Main sources . String docs | Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/29/string-formatting.html",
            "relUrl": "/python/2020/09/29/string-formatting.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "SQL",
            "content": "SQLite . Based on docs. . import os import sqlite3 PATH = &#39;/Users/fgu/tmp&#39; sqlite3.version . &#39;2.6.0&#39; . Establish a connection to database (or create and connect if it doesn&#39;t exist yet) . db_path = os.path.join(PATH, &#39;some.db&#39;) conn = sqlite3.connect(db_path) . To manupulate the database, create a cursor) object (not strictly needed for sqlite3), but I still use it for practice and to make db code portable to other databases. . Adding a table with a row of data . c = conn.cursor() # Create table c.execute(&quot;&quot;&quot;CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)&quot;&quot;&quot;) # Insert a row of data c.execute(&quot;INSERT INTO stocks VALUES (&#39;2006-01-05&#39;,&#39;BUY&#39;,&#39;RHAT&#39;,100,35.14)&quot;) # Save (commit) changes conn.commit() # Close connection conn.close() . To check that the database now contains our stocks table, list all its tables. . conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(&quot;select name from sqlite_master where type = &#39;table&#39;&quot;).fetchall() . [(&#39;stocks&#39;,)] . Retrieving data . conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(&quot;SELECT * FROM stocks&quot;).fetchall() . [(&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14)] . When adding Python variables to the query, never use string substitution directly like so: . symbol = &#39;RHAT&#39; c.execute(f&quot;select * from stocks where symbol = &#39;{symbol}&#39;&quot;).fetchall() . [(&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14)] . While this works, it&#39;s vulnerable to injection attacks. Use parameter substition instead. Either using question marks like so . symbol = (&#39;RHAT&#39;, ) c.execute(&quot;select * from stocks where symbol = ?&quot;, symbol).fetchall() . [(&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14)] . or using named placedholders like so . c.execute(&quot;select * from stocks where symbol = :symbol&quot;, {&#39;symbol&#39;: &#39;RHAT&#39;}).fetchall() . [(&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14)] . Why do I need fetchall() after cursor.execute()? . Because the curse.execute() returns an iterater object containing all query results. . Using namedtuples . EmployeeRecord = namedtuple(&#39;EmployeeRecord&#39;, &#39;name, age, title, department, paygrade&#39;) import csv for emp in map(EmployeeRecord._make, csv.reader(open(&quot;employees.csv&quot;, &quot;rb&quot;))): print emp.name, emp.title import sqlite3 conn = sqlite3.connect(&#39;/companydata&#39;) cursor = conn.cursor() cursor.execute(&#39;SELECT name, age, title, department, paygrade FROM employees&#39;) for emp in map(EmployeeRecord._make, cursor.fetchall()): print emp.name, emp.title . Using Pandas . Pandas is a very handy way to interact with databased in Python, as it makes dumping and retrieving dataframes very easy. . import pandas as pd pd.read_sql_query(&#39;SELECT * FROM stocks&#39;, conn) . date trans symbol qty price newcol . 0 2006-01-05 | BUY | RHAT | 100.0 | 35.14 | None | . SQLAlchemy . Summary of this video. .",
            "url": "https://fabiangunzinger.github.io/blog/python/sql/2020/09/23/sql.html",
            "relUrl": "/python/sql/2020/09/23/sql.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Looping like a native",
            "content": "Based on this talk. . my_list = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] for i in range(len(my_list)): v = my_list[i] print(v) . a b c . This works, but we unnecessarily use integers. Just directly print what you want to print instead. . for v in my_list: print(v) . a b c .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/22/looping.html",
            "relUrl": "/python/2020/09/22/looping.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Data cleaning",
            "content": "Based on excellent materials materials from Daniel Chen here and talk here . from imports import * %load_ext autoreload %autoreload 2 . Creating tidy data . Definition by Wikham here: . Each variable is a column | Each observation is a row | Each type of observational unit is a table. | Columns are values . pew = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/pew.csv&#39;) pew.head() . religion &lt;$10k $10-20k $20-30k $30-40k $40-50k $50-75k $75-100k $100-150k &gt;150k Don&#39;t know/refused . 0 Agnostic | 27 | 34 | 60 | 81 | 76 | 137 | 122 | 109 | 84 | 96 | . 1 Atheist | 12 | 27 | 37 | 52 | 35 | 70 | 73 | 59 | 74 | 76 | . 2 Buddhist | 27 | 21 | 30 | 34 | 33 | 58 | 62 | 39 | 53 | 54 | . 3 Catholic | 418 | 617 | 732 | 670 | 638 | 1116 | 949 | 792 | 633 | 1489 | . 4 Don’t know/refused | 15 | 14 | 15 | 11 | 10 | 35 | 21 | 17 | 18 | 116 | . pew.melt( id_vars=&#39;religion&#39;, var_name=&#39;income&#39;, value_name=&#39;count&#39; ).head() . religion income count . 0 Agnostic | &lt;$10k | 27 | . 1 Atheist | &lt;$10k | 12 | . 2 Buddhist | &lt;$10k | 27 | . 3 Catholic | &lt;$10k | 418 | . 4 Don’t know/refused | &lt;$10k | 15 | . billboard = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/billboard.csv&#39;) billboard.columns . Index([&#39;year&#39;, &#39;artist&#39;, &#39;track&#39;, &#39;time&#39;, &#39;date.entered&#39;, &#39;wk1&#39;, &#39;wk2&#39;, &#39;wk3&#39;, &#39;wk4&#39;, &#39;wk5&#39;, &#39;wk6&#39;, &#39;wk7&#39;, &#39;wk8&#39;, &#39;wk9&#39;, &#39;wk10&#39;, &#39;wk11&#39;, &#39;wk12&#39;, &#39;wk13&#39;, &#39;wk14&#39;, &#39;wk15&#39;, &#39;wk16&#39;, &#39;wk17&#39;, &#39;wk18&#39;, &#39;wk19&#39;, &#39;wk20&#39;, &#39;wk21&#39;, &#39;wk22&#39;, &#39;wk23&#39;, &#39;wk24&#39;, &#39;wk25&#39;, &#39;wk26&#39;, &#39;wk27&#39;, &#39;wk28&#39;, &#39;wk29&#39;, &#39;wk30&#39;, &#39;wk31&#39;, &#39;wk32&#39;, &#39;wk33&#39;, &#39;wk34&#39;, &#39;wk35&#39;, &#39;wk36&#39;, &#39;wk37&#39;, &#39;wk38&#39;, &#39;wk39&#39;, &#39;wk40&#39;, &#39;wk41&#39;, &#39;wk42&#39;, &#39;wk43&#39;, &#39;wk44&#39;, &#39;wk45&#39;, &#39;wk46&#39;, &#39;wk47&#39;, &#39;wk48&#39;, &#39;wk49&#39;, &#39;wk50&#39;, &#39;wk51&#39;, &#39;wk52&#39;, &#39;wk53&#39;, &#39;wk54&#39;, &#39;wk55&#39;, &#39;wk56&#39;, &#39;wk57&#39;, &#39;wk58&#39;, &#39;wk59&#39;, &#39;wk60&#39;, &#39;wk61&#39;, &#39;wk62&#39;, &#39;wk63&#39;, &#39;wk64&#39;, &#39;wk65&#39;, &#39;wk66&#39;, &#39;wk67&#39;, &#39;wk68&#39;, &#39;wk69&#39;, &#39;wk70&#39;, &#39;wk71&#39;, &#39;wk72&#39;, &#39;wk73&#39;, &#39;wk74&#39;, &#39;wk75&#39;, &#39;wk76&#39;], dtype=&#39;object&#39;) . idvars = billboard.columns[~bb.columns.str.startswith(&#39;wk&#39;)] tidy_billboard = billboard.melt( id_vars=idvars, var_name=&#39;week&#39;, value_name=&#39;rating&#39; ) tidy_billboard.head() . year artist track time date.entered week rating . 0 2000 | 2 Pac | Baby Don&#39;t Cry (Keep... | 4:22 | 2000-02-26 | wk1 | 87.0 | . 1 2000 | 2Ge+her | The Hardest Part Of ... | 3:15 | 2000-09-02 | wk1 | 91.0 | . 2 2000 | 3 Doors Down | Kryptonite | 3:53 | 2000-04-08 | wk1 | 81.0 | . 3 2000 | 3 Doors Down | Loser | 4:24 | 2000-10-21 | wk1 | 76.0 | . 4 2000 | 504 Boyz | Wobble Wobble | 3:35 | 2000-04-15 | wk1 | 57.0 | . Multiple variables stored in one column . ebola = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/country_timeseries.csv&#39;) ebola.head() . Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone Cases_Nigeria Cases_Senegal Cases_UnitedStates Cases_Spain Cases_Mali Deaths_Guinea Deaths_Liberia Deaths_SierraLeone Deaths_Nigeria Deaths_Senegal Deaths_UnitedStates Deaths_Spain Deaths_Mali . 0 1/5/2015 | 289 | 2776.0 | NaN | 10030.0 | NaN | NaN | NaN | NaN | NaN | 1786.0 | NaN | 2977.0 | NaN | NaN | NaN | NaN | NaN | . 1 1/4/2015 | 288 | 2775.0 | NaN | 9780.0 | NaN | NaN | NaN | NaN | NaN | 1781.0 | NaN | 2943.0 | NaN | NaN | NaN | NaN | NaN | . 2 1/3/2015 | 287 | 2769.0 | 8166.0 | 9722.0 | NaN | NaN | NaN | NaN | NaN | 1767.0 | 3496.0 | 2915.0 | NaN | NaN | NaN | NaN | NaN | . 3 1/2/2015 | 286 | NaN | 8157.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 3496.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 12/31/2014 | 284 | 2730.0 | 8115.0 | 9633.0 | NaN | NaN | NaN | NaN | NaN | 1739.0 | 3471.0 | 2827.0 | NaN | NaN | NaN | NaN | NaN | . tidy_ebola = ebola.melt(id_vars=[&#39;Date&#39;, &#39;Day&#39;], value_name=&#39;Cases&#39;) tidy_ebola[[&#39;Statistic&#39;, &#39;Country&#39;]] = (tidy_ebola.variable .str.split(&#39;_&#39;, expand=True)) tidy_ebola.drop(&#39;variable&#39;, axis=1, inplace=True) tidy_ebola.head() . Date Day Cases Statistic Country . 0 1/5/2015 | 289 | 2776.0 | Cases | Guinea | . 1 1/4/2015 | 288 | 2775.0 | Cases | Guinea | . 2 1/3/2015 | 287 | 2769.0 | Cases | Guinea | . 3 1/2/2015 | 286 | NaN | Cases | Guinea | . 4 12/31/2014 | 284 | 2730.0 | Cases | Guinea | . (ebola .melt(id_vars=[&#39;Date&#39;, &#39;Day&#39;], value_name=&#39;Cases&#39;) .assign(Statistic = lambda df: df.variable.str.split(&#39;_&#39;, expand=True)[0]) .assign(Country = lambda df: df.variable.str.split(&#39;_&#39;, expand=True)[1]) .drop(&#39;variable&#39;, axis=1) ).head() . Date Day Cases Statistic Country . 0 1/5/2015 | 289 | 2776.0 | Cases | Guinea | . 1 1/4/2015 | 288 | 2775.0 | Cases | Guinea | . 2 1/3/2015 | 287 | 2769.0 | Cases | Guinea | . 3 1/2/2015 | 286 | NaN | Cases | Guinea | . 4 12/31/2014 | 284 | 2730.0 | Cases | Guinea | . Variables are stored in both rows and columns . weather = pd.read_csv(&#39;https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/weather.csv&#39;) weather.head() . id year month element d1 d2 d3 d4 d5 d6 d7 d8 d9 d10 d11 d12 d13 d14 d15 d16 d17 d18 d19 d20 d21 d22 d23 d24 d25 d26 d27 d28 d29 d30 d31 . 0 MX17004 | 2010 | 1 | tmax | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 27.8 | NaN | . 1 MX17004 | 2010 | 1 | tmin | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 14.5 | NaN | . 2 MX17004 | 2010 | 2 | tmax | NaN | 27.3 | 24.1 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 29.7 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 29.9 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 MX17004 | 2010 | 2 | tmin | NaN | 14.4 | 14.4 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 13.4 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 10.7 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 MX17004 | 2010 | 3 | tmax | NaN | NaN | NaN | NaN | 32.1 | NaN | NaN | NaN | NaN | 34.5 | NaN | NaN | NaN | NaN | NaN | 31.1 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . (weather .melt( id_vars=[&#39;id&#39;, &#39;year&#39;, &#39;month&#39;, &#39;element&#39;], var_name=&#39;day&#39;, value_name=&#39;temp&#39; ) .pivot_table( index=[&#39;id&#39;, &#39;year&#39;, &#39;month&#39;, &#39;day&#39;], columns=&#39;element&#39;, values=&#39;temp&#39; ) .reset_index() .assign(day = lambda df: df.day.str.extract(&#39;( d+)&#39;)) ).head() . element id year month day tmax tmin . 0 MX17004 | 2010 | 1 | 30 | 27.8 | 14.5 | . 1 MX17004 | 2010 | 2 | 11 | 29.7 | 13.4 | . 2 MX17004 | 2010 | 2 | 2 | 27.3 | 14.4 | . 3 MX17004 | 2010 | 2 | 23 | 29.9 | 10.7 | . 4 MX17004 | 2010 | 2 | 3 | 24.1 | 14.4 | . Multiple types of observational units are stored in a single table . tidy_billboard.head() tidy_bb = tidy_billboard tidy_bb.head() . year artist track time date.entered week rating . 0 2000 | 2 Pac | Baby Don&#39;t Cry (Keep... | 4:22 | 2000-02-26 | wk1 | 87.0 | . 1 2000 | 2Ge+her | The Hardest Part Of ... | 3:15 | 2000-09-02 | wk1 | 91.0 | . 2 2000 | 3 Doors Down | Kryptonite | 3:53 | 2000-04-08 | wk1 | 81.0 | . 3 2000 | 3 Doors Down | Loser | 4:24 | 2000-10-21 | wk1 | 76.0 | . 4 2000 | 504 Boyz | Wobble Wobble | 3:35 | 2000-04-15 | wk1 | 57.0 | . bb_songs = ( tidy_bb[[&#39;year&#39;, &#39;artist&#39;, &#39;track&#39;, &#39;time&#39;, &#39;date.entered&#39;]] .drop_duplicates() .assign(id = lambda df: range(len(df))) ) bb_songs.head() . year artist track time date.entered id . 0 2000 | 2 Pac | Baby Don&#39;t Cry (Keep... | 4:22 | 2000-02-26 | 0 | . 1 2000 | 2Ge+her | The Hardest Part Of ... | 3:15 | 2000-09-02 | 1 | . 2 2000 | 3 Doors Down | Kryptonite | 3:53 | 2000-04-08 | 2 | . 3 2000 | 3 Doors Down | Loser | 4:24 | 2000-10-21 | 3 | . 4 2000 | 504 Boyz | Wobble Wobble | 3:35 | 2000-04-15 | 4 | . bb_ratings = ( tidy_bb .merge(bb_songs) .loc[:, [&#39;week&#39;, &#39;rating&#39;, &#39;id&#39;]] ) bb_ratings.head() . week rating id . 0 wk1 | 87.0 | 0 | . 1 wk2 | 82.0 | 0 | . 2 wk3 | 72.0 | 0 | . 3 wk4 | 77.0 | 0 | . 4 wk5 | 87.0 | 0 | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/22/data-cleaning.html",
            "relUrl": "/python/pandas/2020/09/22/data-cleaning.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "os",
            "content": "Full documentation is here. . from imports import * import os %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . os.walk(os.) . TypeError Traceback (most recent call last) &lt;ipython-input-16-7f906606989b&gt; in &lt;module&gt; -&gt; 1 os.walk() TypeError: walk() missing 1 required positional argument: &#39;top&#39; . Listing files in directory . List filenames: . path = &#39;/Users/fgu/tmp/testdir&#39; os.listdir(path) . [&#39;a.py&#39;, &#39;b.py&#39;] . List filenames with full path: . There are many ways to do this. I find using scandir() most useful. . paths = [f.path for f in os.scandir(path)] paths . [&#39;/Users/fgu/tmp/testdir/a.py&#39;, &#39;/Users/fgu/tmp/testdir/b.py&#39;] . This only works if path is an absolute path, else scandir() will return relative paths just like listdir(). . The os.DirEntry object returned by scandir() is quite useful for other things, too: . for f in os.scandir(path): print(f) print(f.name) print(f.path) print(f.is_file()) print(f.is_dir()) print() . &lt;DirEntry &#39;a.py&#39;&gt; a.py /Users/fgu/tmp/testdir/a.py True False &lt;DirEntry &#39;b.py&#39;&gt; b.py /Users/fgu/tmp/testdir/b.py True False .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/20/os.html",
            "relUrl": "/python/2020/09/20/os.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Functools",
            "content": "partial . from operator import mul from functools import partial print(mul(2, 3)) tripple = partial(mul, 3) tripple(2) . 6 . 6 . Sources . Fluent Python | Python Cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/17/functools.html",
            "relUrl": "/python/2020/09/17/functools.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Time series in Pandas",
            "content": "import pandas as pd from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . Number of periods observed . Say I want to know the number of months for which each user in my dataset is observed (allowing for possible gaps in their data). . df = pd.read_parquet(SAMPLEDATA, columns=[&#39;user_id&#39;, &#39;transaction_date&#39;]) print(df.shape) df.head() . (438853, 2) . user_id transaction_date . 0 60777 | 2014-11-27 | . 1 60777 | 2014-11-27 | . 2 60777 | 2014-11-28 | . 3 60777 | 2014-12-08 | . 4 60777 | 2014-12-12 | . def num_periods(g, freq=&#39;M&#39;): &quot;&quot;&quot;Return number of periods observed.&quot;&quot;&quot; return (g.transaction_date.max().to_period(freq) - g.transaction_date.min().to_period(freq)).n + 1 df.groupby(&#39;user_id&#39;).apply(num_periods, freq=&#39;M&#39;).head(3) . user_id 777 103 1777 28 7777 90 dtype: int64 . df.groupby(&#39;user_id&#39;).apply(num_periods, freq=&#39;W&#39;).head(3) . user_id 777 447 1777 117 7777 387 dtype: int64 . Groupby vs resample . df = pd.DataFrame({&#39;data&#39;: [1, 2, 3, 4, 5]}, pd.date_range(&#39;2020-01-01&#39;, &#39;2020-01-10&#39;, freq=&#39;2d&#39;)) df . data . 2020-01-01 1 | . 2020-01-03 2 | . 2020-01-05 3 | . 2020-01-07 4 | . 2020-01-09 5 | . df.resample(&#39;d&#39;).sum() . data . 2020-01-01 1 | . 2020-01-02 0 | . 2020-01-03 2 | . 2020-01-04 0 | . 2020-01-05 3 | . 2020-01-06 0 | . 2020-01-07 4 | . 2020-01-08 0 | . 2020-01-09 5 | . dd.groupby(level=0).sum() . data . 2020-01-01 1 | . 2020-01-03 2 | . 2020-01-05 3 | . 2020-01-07 4 | . 2020-01-09 5 | . Main sources . Python for Data Analysis | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/15/time-series.html",
            "relUrl": "/python/pandas/2020/09/15/time-series.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Imports",
            "content": "Import basics . From reading Chapter 8 in Python Essential Reference: . Calling import module for the first time does three things: . Create a new namespace that acts as the global namespace for all objects defined in module. . | Execute the entire module. . | Create a name -- identical to the module name -- within the caller namespace that references to the module. This can be used to access module objects in the caller namespace as module.object. . | | Calling from module import symbol imports symbol into the current namespace. However, the global namespace for symbol (if it&#39;s a function) always remains the namespace in which it was defined, not the caller&#39;s namespace. . | One reason from module import * is generally discouraged is that it directly imports all the module&#39;s objects into the caller&#39;s namespace, which is often said to cluter it up. Especially when importing large modules this makes sense, as it&#39;s much cleaner to keep objects defined in imported modules in eponymous namespaces and accessing them via module.object, which immediately makes clear where object comes from and can help greatly with debugging. . | . Running files as scripts or modules . For relative imports to work as described, for instance, here and in Chapter 8 in Python Essential References and in recipees 10.1 and 10.3 in the Python Cookbook, the file into which you import has itself to be a module rather than a top-level script. If it&#39;s the latter, it&#39;s name will be main and it won&#39;t be considered part of a package, regardless of where on the file system it is saved. Generally, for a file to be considered part of a package, it needs to nave a dot (.) in its name, as in package.submodule.modulename. From this brilliant SO answer. | . Main sources . The Hitchhiker&#39;s Guide to Python | Fluent Python | Python Cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/15/import.html",
            "relUrl": "/python/2020/09/15/import.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Web frameworks",
            "content": "Every time I visit a website, an application called a web server who usually resides on a machine also called a web server sends HTML to my browser. HTML (Hypertext markup language) is used by browsers to describe the content and structure of a website. . | The essence of every web application is to send HTML to a browser. . | How does a web server know what data it needs to send? It sends what I request using the HTTP protocol. (HTTP means hypertext transfer protocol; a protocol is a universally agreed data format and sequence of steps to enable communication between two parties.) . | So, a web application receives HTTP requests (e.g. get or post) and responds with an HTTP request, usually in the form of the HTML for the requested page. . | . Main sources . Jeff Knupp post | Robert Chang post | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/06/web-frameworks.html",
            "relUrl": "/python/2020/09/06/web-frameworks.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Dictionaries",
            "content": "Constructing a dictionary . a = dict(x=1, y=2) b = {&#39;x&#39;: 1, &#39;y&#39;: 2} c = dict(zip([&#39;x&#39;, &#39;y&#39;], [1, 2])) d = dict([(&#39;x&#39;, 1), (&#39;y&#39;, 2)]) e = dict({&#39;y&#39;: 2, &#39;x&#39;: 1}) mylist = [(1, &#39;x&#39;), (2, &#39;y&#39;)] f = {key: value for value, key in mylist} a == b == c == d == e == f . True . a = {1, 2, 3} b = {2, 3, 4} a &amp;= b a . {2, 3} . Main sources . Fluent Python | Python Cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/06/dictionaries.html",
            "relUrl": "/python/2020/09/06/dictionaries.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Splitting dataframes based on column values",
            "content": "import pandas as pd import numpy as np . The problem . df = pd.DataFrame(data={&#39;case&#39;: [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;,&#39;A&#39;], &#39;data&#39;: np.random.randn(9)}) df . case data . 0 A | 0.684978 | . 1 A | 0.000269 | . 2 A | -1.040497 | . 3 B | 0.451358 | . 4 A | 0.448596 | . 5 A | 0.222168 | . 6 B | 1.031011 | . 7 A | -2.208787 | . 8 A | -0.440758 | . You want to split the dataframe every time case equals B and store the resulting dataframes in a list. . Understanding the cookbook solution . From the cookbook: . dfs = list(zip(*df.groupby((1 * (df[&#39;case&#39;] == &#39;B&#39;)).cumsum().rolling(window=3, min_periods=1).median())))[-1] dfs . ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 3 B 0.451358, case data 4 A 0.448596 5 A 0.222168 6 B 1.031011, case data 7 A -2.208787 8 A -0.440758) . This works. But because it&#39;s so heavily nested and uses methods like rolling() and median() not really designed for that purpose, the code is impossible to interpret at a glance. . Let&#39;s break this down into separate pieces. . First, the code creates a grouping variable that changes its value each time case equaled B on the previous row. The code below shows how it does this. . a = (df.case == &#39;B&#39;) b = 1 * (df.case == &#39;B&#39;) c = 1 * (df.case == &#39;B&#39;).cumsum() d = 1 * (df.case == &#39;B&#39;).cumsum().rolling(window=3, min_periods=1).median() a, b, c, d . (0 False 1 False 2 False 3 True 4 False 5 False 6 True 7 False 8 False Name: case, dtype: bool, 0 0 1 0 2 0 3 1 4 0 5 0 6 1 7 0 8 0 Name: case, dtype: int64, 0 0 1 0 2 0 3 1 4 1 5 1 6 2 7 2 8 2 Name: case, dtype: int64, 0 0.0 1 0.0 2 0.0 3 0.0 4 1.0 5 1.0 6 1.0 7 2.0 8 2.0 Name: case, dtype: float64) . Series d above is the argument passed to groupby() in the solution. This works, but is a very roundabout way to create such a series. I&#39;ll use a different approach below. . Next, the code uses list(), zip(), and argument expansion to pack the data for each group into a single list of dataframes. Let&#39;s look at these one by one. . First, a quick review of how argument expansion works: . def printer(*args, **kwargs): print(&#39;Printing args:&#39;) for arg in args: print(arg) print(&#39;Printing kwargs:&#39;) for kwarg in kwargs.items(): print(kwarg) mylist = [&#39;a&#39;, 2, &#39;k&#39;, 3] mydict = {&#39;first&#39;: 1, &#39;second&#39;: 2} printer(*mylist, **mydict) . Printing args: a 2 k 3 Printing kwargs: (&#39;first&#39;, 1) (&#39;second&#39;, 2) . Now, groupby() stores the grouped data as (label, dataframe) tuples, like so: . groups = df.groupby(&#39;case&#39;) for g in groups: print(g) print(type(g)) . (&#39;A&#39;, case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 4 A 0.448596 5 A 0.222168 7 A -2.208787 8 A -0.440758) &lt;class &#39;tuple&#39;&gt; (&#39;B&#39;, case data 3 B 0.451358 6 B 1.031011) &lt;class &#39;tuple&#39;&gt; . So zip() is used to separate the group label from the data, and list() consumes the iterator created by zip and displays its content. . list(zip(*groups)) . [(&#39;A&#39;, &#39;B&#39;), ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 4 A 0.448596 5 A 0.222168 7 A -2.208787 8 A -0.440758, case data 3 B 0.451358 6 B 1.031011)] . Because we only want the data, we select the last element from the list: . list(zip(*groups))[-1] . ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 4 A 0.448596 5 A 0.222168 7 A -2.208787 8 A -0.440758, case data 3 B 0.451358 6 B 1.031011) . Now we&#39;re basically done. What remains is to use the list(zip(*groups)) procedure on the more complicated grouping variable, to obtain the original result. . d = 1 * (df.case == &#39;B&#39;).cumsum().rolling(window=3, min_periods=1).median() groups = df.groupby(d) list(zip(*groups))[-1] . ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 3 B 0.451358, case data 4 A 0.448596 5 A 0.222168 6 B 1.031011, case data 7 A -2.208787 8 A -0.440758) . Simplifying the code . I think this can be made much more readable like so: . df . case data . 0 A | 0.684978 | . 1 A | 0.000269 | . 2 A | -1.040497 | . 3 B | 0.451358 | . 4 A | 0.448596 | . 5 A | 0.222168 | . 6 B | 1.031011 | . 7 A | -2.208787 | . 8 A | -0.440758 | . grouper = df.case.eq(&#39;B&#39;).cumsum().shift().fillna(0) grouper . 0 0.0 1 0.0 2 0.0 3 0.0 4 1.0 5 1.0 6 1.0 7 2.0 8 2.0 Name: case, dtype: float64 . dfs = [df for (g, df) in df.groupby(grouper)] dfs . [ case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 3 B 0.451358, case data 4 A 0.448596 5 A 0.222168 6 B 1.031011, case data 7 A -2.208787 8 A -0.440758] . In case the logic of this isn&#39;t immediately obvious, the below makes clear what&#39;s going on. . dd = df.set_index(&#39;case&#39;, drop=False) # Use case as index for clarity a = dd.case.eq(&#39;B&#39;) # Boolean logic b = a.cumsum() # Create groups c = b.shift() # Shift so B included in previous group d = c.fillna(0) # Replace 0th element emptied by shift a, b, c, d . (case A False A False A False B True A False A False B True A False A False Name: case, dtype: bool, case A 0 A 0 A 0 B 1 A 1 A 1 B 2 A 2 A 2 Name: case, dtype: int64, case A NaN A 0.0 A 0.0 B 0.0 A 1.0 A 1.0 B 1.0 A 2.0 A 2.0 Name: case, dtype: float64, case A 0.0 A 0.0 A 0.0 B 0.0 A 1.0 A 1.0 B 1.0 A 2.0 A 2.0 Name: case, dtype: float64) .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/04/splitting-dataframes.html",
            "relUrl": "/python/pandas/2020/09/04/splitting-dataframes.html",
            "date": " • Sep 4, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Multiindexing in Pandas",
            "content": "Working with indices, expecially column indices, and especially with hierarchical ones, is an area of Pandas I keep finding perplexing. The point of this notebook is to help my future self. . from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . iris = sns.load_dataset(&#39;iris&#39;) iris.head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . Stacking and unstacking column labels . df = df.set_index(&#39;species&#39;) df.head() . sepal_length sepal_width petal_length petal_width . species . setosa 5.1 | 3.5 | 1.4 | 0.2 | . setosa 4.9 | 3.0 | 1.4 | 0.2 | . setosa 4.7 | 3.2 | 1.3 | 0.2 | . setosa 4.6 | 3.1 | 1.5 | 0.2 | . setosa 5.0 | 3.6 | 1.4 | 0.2 | . tups = [tuple(name.split(&#39;_&#39;)) for name in df.columns] colnames = pd.MultiIndex.from_tuples(tups) df.columns = colnames df.head() . sepal petal . length width length width . species . setosa 5.1 | 3.5 | 1.4 | 0.2 | . setosa 4.9 | 3.0 | 1.4 | 0.2 | . setosa 4.7 | 3.2 | 1.3 | 0.2 | . setosa 4.6 | 3.1 | 1.5 | 0.2 | . setosa 5.0 | 3.6 | 1.4 | 0.2 | . df.columns = [&#39;_&#39;.join(name) for name in df.columns] df = df.reset_index() df.head() . species sepal_length sepal_width petal_length petal_width . 0 setosa | 5.1 | 3.5 | 1.4 | 0.2 | . 1 setosa | 4.9 | 3.0 | 1.4 | 0.2 | . 2 setosa | 4.7 | 3.2 | 1.3 | 0.2 | . 3 setosa | 4.6 | 3.1 | 1.5 | 0.2 | . 4 setosa | 5.0 | 3.6 | 1.4 | 0.2 | . Use column values as column label . ... . Sources . Python for Data Analysis | Pandas cookbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/02/multiindexing-in-pandas.html",
            "relUrl": "/python/pandas/2020/09/02/multiindexing-in-pandas.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Grouping in Pandas",
            "content": "import pandas as pd import numpy as np . Using apply with groupby . From the cookbook: . df = pd.DataFrame({&#39;animal&#39;: &#39;cat dog cat fish dog cat cat&#39;.split(), &#39;size&#39;: list(&#39;SSMMMLL&#39;), &#39;weight&#39;: [8, 10, 11, 1, 20, 12, 12], &#39;adult&#39;: [False] * 5 + [True] * 2}) df . animal size weight adult . 0 cat | S | 8 | False | . 1 dog | S | 10 | False | . 2 cat | M | 11 | False | . 3 fish | M | 1 | False | . 4 dog | M | 20 | False | . 5 cat | L | 12 | True | . 6 cat | L | 12 | True | . df.groupby(&#39;animal&#39;).apply(lambda g: g.loc[g.weight.idxmax(), &#39;size&#39;]) . animal cat L dog M fish M dtype: object . Expanding apply . Assume you want to calculate the cumulative return from a series of one-period returns in an expanding fashion -- in each period, you want the cumulative return up to that period. . s = pd.Series([i / 100.0 for i in range(1, 4)]) s . 0 0.01 1 0.02 2 0.03 dtype: float64 . The solution is given here. . import functools def cum_return(x, y): return x * (1 + y) def red(x): res = functools.reduce(cum_return, x, 1) return res s.expanding().apply(red, raw=True) . 0 1.010000 1 1.030200 2 1.061106 dtype: float64 . I found that somewhere between bewildering and magical. To see what&#39;s going on, it helps to add a few print statements: . import functools def cum_return(x, y): print(&#39;x:&#39;, x) print(&#39;y:&#39;, y) return x * (1 + y) def red(x): print(&#39;Series:&#39;, x) res = functools.reduce(cum_return, x, 1) print(&#39;Result:&#39;, res) print() return res s.expanding().apply(red, raw=True) . Series: [0.01] x: 1 y: 0.01 Result: 1.01 Series: [0.01 0.02] x: 1 y: 0.01 x: 1.01 y: 0.02 Result: 1.0302 Series: [0.01 0.02 0.03] x: 1 y: 0.01 x: 1.01 y: 0.02 x: 1.0302 y: 0.03 Result: 1.061106 . 0 1.010000 1 1.030200 2 1.061106 dtype: float64 . This makes transparent how reduce works: it takes the starting value (1 here) as the initial x value and the first value of the series as y value, and then returns the result of cum_returns. Next, it uses that result as x, and the second element in the series as y, and calculates the new result of cum_returns. This is then repeated until it has run through the entire series. . What surprised me is to see that reduce always starts the calculation from the beginning, rather than re-using the last calculated result. This seems inefficient, but is probably necessary for some reason. . Sort by sum of group values . df = pd.DataFrame({&#39;code&#39;: [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;] * 2, &#39;data&#39;: [0.16, -0.21, 0.33, 0.45, -0.59, 0.62], &#39;flag&#39;: [False, True] * 3}) df . code data flag . 0 foo | 0.16 | False | . 1 bar | -0.21 | True | . 2 baz | 0.33 | False | . 3 foo | 0.45 | True | . 4 bar | -0.59 | False | . 5 baz | 0.62 | True | . g = df.groupby(&#39;code&#39;) sort_order = g[&#39;data&#39;].transform(sum).sort_values().index df.loc[sort_order] . code data flag . 1 bar | -0.21 | True | . 4 bar | -0.59 | False | . 0 foo | 0.16 | False | . 3 foo | 0.45 | True | . 2 baz | 0.33 | False | . 5 baz | 0.62 | True | . g = df.groupby(&#39;code&#39;) g.apply(lambda g: g.loc[g.data.idxmax()]) . code data flag . code . bar bar | -0.21 | True | . baz baz | 0.62 | True | . foo foo | 0.45 | True | . Expanding group operations . Based on this answer. . df = pd.DataFrame({&#39;code&#39;: [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;] * 4, &#39;data&#39;: [0.16, -0.21, 0.33, 0.45, -0.59, 0.62] * 2, &#39;flag&#39;: [False, True] * 6}) df . code data flag . 0 foo | 0.16 | False | . 1 bar | -0.21 | True | . 2 baz | 0.33 | False | . 3 foo | 0.45 | True | . 4 bar | -0.59 | False | . 5 baz | 0.62 | True | . 6 foo | 0.16 | False | . 7 bar | -0.21 | True | . 8 baz | 0.33 | False | . 9 foo | 0.45 | True | . 10 bar | -0.59 | False | . 11 baz | 0.62 | True | . g = df.groupby(&#39;code&#39;) def helper(g): s = g.data.expanding() g[&#39;exp_mean&#39;] = s.mean() g[&#39;exp_sum&#39;] = s.sum() g[&#39;exp_count&#39;] = s.count() return g g.apply(helper).sort_values(&#39;code&#39;) . code data flag exp_mean exp_sum exp_count . 1 bar | -0.21 | True | -0.210000 | -0.21 | 1.0 | . 4 bar | -0.59 | False | -0.400000 | -0.80 | 2.0 | . 7 bar | -0.21 | True | -0.336667 | -1.01 | 3.0 | . 10 bar | -0.59 | False | -0.400000 | -1.60 | 4.0 | . 2 baz | 0.33 | False | 0.330000 | 0.33 | 1.0 | . 5 baz | 0.62 | True | 0.475000 | 0.95 | 2.0 | . 8 baz | 0.33 | False | 0.426667 | 1.28 | 3.0 | . 11 baz | 0.62 | True | 0.475000 | 1.90 | 4.0 | . 0 foo | 0.16 | False | 0.160000 | 0.16 | 1.0 | . 3 foo | 0.45 | True | 0.305000 | 0.61 | 2.0 | . 6 foo | 0.16 | False | 0.256667 | 0.77 | 3.0 | . 9 foo | 0.45 | True | 0.305000 | 1.22 | 4.0 | . Pivoting . From here . df = pd.DataFrame(data={&#39;province&#39;: [&#39;ON&#39;, &#39;QC&#39;, &#39;BC&#39;, &#39;AL&#39;, &#39;AL&#39;, &#39;MN&#39;, &#39;ON&#39;], &#39;city&#39;: [&#39;Toronto&#39;, &#39;Montreal&#39;, &#39;Vancouver&#39;, &#39;Calgary&#39;, &#39;Edmonton&#39;, &#39;Winnipeg&#39;, &#39;Windsor&#39;], &#39;sales&#39;: [13, 6, 16, 8, 4, 3, 1]}) df . province city sales . 0 ON | Toronto | 13 | . 1 QC | Montreal | 6 | . 2 BC | Vancouver | 16 | . 3 AL | Calgary | 8 | . 4 AL | Edmonton | 4 | . 5 MN | Winnipeg | 3 | . 6 ON | Windsor | 1 | . You want to group sales by province and get subtotal for total state. . table = ( df .pivot_table(values=&#39;sales&#39;, index=&#39;province&#39;, columns=&#39;city&#39;, aggfunc=&#39;sum&#39;, margins=True) .stack() .drop(&#39;All&#39;) ) table . province city AL Calgary 8.0 Edmonton 4.0 All 12.0 BC Vancouver 16.0 All 16.0 MN Winnipeg 3.0 All 3.0 ON Toronto 13.0 Windsor 1.0 All 14.0 QC Montreal 6.0 All 6.0 dtype: float64 . Aggregating . From here . df = pd.DataFrame( {&#39;StudentID&#39;: [&quot;x1&quot;, &quot;x10&quot;, &quot;x2&quot;,&quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;], &#39;StudentGender&#39; : [&#39;F&#39;, &#39;M&#39;, &#39;F&#39;, &#39;M&#39;, &#39;F&#39;, &#39;M&#39;, &#39;F&#39;, &#39;M&#39;, &#39;M&#39;, &#39;M&#39;], &#39;ExamYear&#39;: [&#39;2007&#39;,&#39;2007&#39;,&#39;2007&#39;,&#39;2008&#39;,&#39;2008&#39;,&#39;2008&#39;,&#39;2008&#39;,&#39;2009&#39;,&#39;2009&#39;,&#39;2009&#39;], &#39;Exam&#39;: [&#39;algebra&#39;, &#39;stats&#39;, &#39;bio&#39;, &#39;algebra&#39;, &#39;algebra&#39;, &#39;stats&#39;, &#39;stats&#39;, &#39;algebra&#39;, &#39;bio&#39;, &#39;bio&#39;], &#39;Participated&#39;: [&#39;no&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;no&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;], &#39;Passed&#39;: [&#39;no&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;no&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;no&#39;,&#39;yes&#39;]}, columns = [&#39;StudentID&#39;, &#39;StudentGender&#39;, &#39;ExamYear&#39;, &#39;Exam&#39;, &#39;Participated&#39;, &#39;Passed&#39;]) df.columns = [str.lower(c) for c in df.columns] df . studentid studentgender examyear exam participated passed . 0 x1 | F | 2007 | algebra | no | no | . 1 x10 | M | 2007 | stats | yes | yes | . 2 x2 | F | 2007 | bio | yes | yes | . 3 x3 | M | 2008 | algebra | yes | yes | . 4 x4 | F | 2008 | algebra | no | no | . 5 x5 | M | 2008 | stats | yes | yes | . 6 x6 | F | 2008 | stats | yes | yes | . 7 x7 | M | 2009 | algebra | yes | yes | . 8 x8 | M | 2009 | bio | yes | no | . 9 x9 | M | 2009 | bio | yes | yes | . numyes = lambda x: sum(x == &#39;yes&#39;) df.groupby(&#39;examyear&#39;).agg({&#39;participated&#39;: numyes, &#39;passed&#39;: numyes}) . participated passed . examyear . 2007 2 | 2 | . 2008 3 | 3 | . 2009 3 | 2 | . Sources . Python for Data Analysis | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/02/grouping-in-pandas.html",
            "relUrl": "/python/pandas/2020/09/02/grouping-in-pandas.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Sequences",
            "content": "Disclaimer: most of the code here is directly taken from one of the sources mentioned at the bottom. The point of this post is not to produce new knowledge, but to understand and practice what I read, and to leave a reference for my future self. . Splitting and slicing . Slice from (and/or) to particular characters . a = &#39;abc[def]&#39; a[a.find(&#39;[&#39;):] . &#39;[def]&#39; . this works because . a.find(&#39;[&#39;) . 3 . Lists . Elegantly creating a list . Instead of this: . a = [&#39;apple&#39;, &#39;banana&#39;, &#39;cherry&#39;] . I can do this: . b = &#39;apple banana cherry&#39;.split() . a == b . True . List comprehensions . colors = [&#39;blue&#39;, &#39;yellow&#39;] sizes = &#39;SML&#39; for color in colors: for size in sizes: print(color, size) . blue S blue M blue L yellow S yellow M yellow L . shirts = [(color, size) for color in colors for size in sizes] shirts . [(&#39;blue&#39;, &#39;S&#39;), (&#39;blue&#39;, &#39;M&#39;), (&#39;blue&#39;, &#39;L&#39;), (&#39;yellow&#39;, &#39;S&#39;), (&#39;yellow&#39;, &#39;M&#39;), (&#39;yellow&#39;, &#39;L&#39;)] . shirts = [(color, size) for size in sizes for color in colors] shirts . [(&#39;blue&#39;, &#39;S&#39;), (&#39;yellow&#39;, &#39;S&#39;), (&#39;blue&#39;, &#39;M&#39;), (&#39;yellow&#39;, &#39;M&#39;), (&#39;blue&#39;, &#39;L&#39;), (&#39;yellow&#39;, &#39;L&#39;)] . Note that the ordering depends on the order of the for statements, just as it would in a nested for loop. . Tuples . Named tuples . Main sources . Fluent Python | Python Cookbook | Learning Python | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2020/09/01/sequences.html",
            "relUrl": "/python/2020/09/01/sequences.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Pandas cookbook notes",
            "content": "The cookbook is here . import pandas as pd import numpy as np . Sort rows based on closeness to certain value . df = pd.DataFrame({&#39;AAA&#39;: [4, 5, 6, 7], &#39;BBB&#39;: [10, 20, 30, 40], &#39;CCC&#39;: [100, 50, -30, -50]}) df . AAA BBB CCC . 0 4 | 10 | 100 | . 1 5 | 20 | 50 | . 2 6 | 30 | -30 | . 3 7 | 40 | -50 | . To sort the rows in order of closeness to myval, do this: . myval = 35 df.loc[(df.CCC - myval).abs().argsort()] . AAA BBB CCC . 1 5 | 20 | 50 | . 0 4 | 10 | 100 | . 2 6 | 30 | -30 | . 3 7 | 40 | -50 | . A note of caution on argsort: . myval = 35 a = (df.CCC - myval).abs() b = a.argsort() a, b . (0 65 1 15 2 65 3 85 Name: CCC, dtype: int64, 0 1 1 0 2 2 3 3 Name: CCC, dtype: int64) . myval = 34 a = (df.CCC - myval).abs() b = a.argsort() a, b . (0 66 1 16 2 64 3 84 Name: CCC, dtype: int64, 0 1 1 2 2 0 3 3 Name: CCC, dtype: int64) . I tripped up expecting the result of argsort to look like the first case and got really confused by the result of the second case because I expected argsort to return a series containing the rank of each value in the original series. This post provided the solution: argsort doesn&#39;t return a series of ranks but a series such that a[b] returns a sorted version a. Hence, the first value in b tells us that in the sorted series a[b], the 0th element will be element 1 in the original series a. . Compound boolean selection . Careful when using compound boolean conditions; it took me a moment to figure out why the result below (from the cookbook) is correct. . df . AAA BBB CCC . 0 4 | 10 | 100 | . 1 5 | 20 | 50 | . 2 6 | 30 | -30 | . 3 7 | 40 | -50 | . df[~((df.AAA &lt;= 6) &amp; (df.index.isin([0, 2, 4])))] . AAA BBB CCC . 1 5 | 20 | 50 | . 3 7 | 40 | -50 | . What confused me was that 5 is smaller than 6. The key thing to remember is that not (a &amp; b) equals (not a) | (not b). . Creating new columns based on existing ones using mappings . The below is a straightforward adaptation from the cookbook: . df = pd.DataFrame({&#39;AAA&#39;: [1, 2, 1, 3], &#39;BBB&#39;: [1, 1, 4, 2], &#39;CCC&#39;: [2, 1, 3, 1]}) source_cols = [&#39;AAA&#39;, &#39;BBB&#39;] new_cols = [str(c) + &#39;_cat&#39; for c in source_cols] cats = {1: &#39;One&#39;, 2: &#39;Two&#39;, 3: &#39;Three&#39;} dd = df.copy() dd[new_cols] = df[source_cols].applymap(cats.get) dd . AAA BBB CCC AAA_cat BBB_cat . 0 1 | 1 | 2 | One | One | . 1 2 | 1 | 1 | Two | One | . 2 1 | 4 | 3 | One | None | . 3 3 | 2 | 1 | Three | Two | . But it made me wonder why applymap required the use of the get method while we can map values of a series like so: . s = pd.Series([1, 2, 3, 1]) s.map(cats) . 0 One 1 Two 2 Three 3 One dtype: object . or so . s.map(cats.get) . 0 One 1 Two 2 Three 3 One dtype: object . The answer is simple: applymap requires a function as argument, while map takes functions or mappings. . One limitation of the cookbook solution above is that is doesn&#39;t seem to allow for default values (notice that 4 gets substituted with &quot;None&quot;). . One way around this is the following: . df[new_cols] = df[source_cols].applymap(lambda x: cats.get(x, &#39;Hello&#39;)) df . AAA BBB CCC AAA_cat BBB_cat . 0 1 | 1 | 2 | One | One | . 1 2 | 1 | 1 | Two | One | . 2 1 | 4 | 3 | One | Hello | . 3 3 | 2 | 1 | Three | Two | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/01/pandas-cookbook-notes.html",
            "relUrl": "/python/pandas/2020/09/01/pandas-cookbook-notes.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Categorical variables in Pandas",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-4c1e2ea1ae5b&gt; in &lt;module&gt; -&gt; 1 from imports import * 2 3 get_ipython().run_line_magic(&#39;config&#39;, &#34;InlineBackend.figure_format = &#39;retina&#39;&#34;) 4 get_ipython().run_line_magic(&#39;load_ext&#39;, &#39;autoreload&#39;) 5 get_ipython().run_line_magic(&#39;autoreload&#39;, &#39;2&#39;) ~/Library/Mobile Documents/com~apple~CloudDocs/fab/projects/blog/_notebooks/imports.py in &lt;module&gt; 5 import sqlite3 6 -&gt; 7 import linearmodels 8 import numpy as np 9 import pandas as pd ModuleNotFoundError: No module named &#39;linearmodels&#39; . drinks = pd.read_csv(&#39;http://bit.ly/drinksbycountry&#39;) drinks.head() . country beer_servings spirit_servings wine_servings total_litres_of_pure_alcohol continent . 0 Afghanistan | 0 | 0 | 0 | 0.0 | Asia | . 1 Albania | 89 | 132 | 54 | 4.9 | Europe | . 2 Algeria | 25 | 0 | 14 | 0.7 | Africa | . 3 Andorra | 245 | 138 | 312 | 12.4 | Europe | . 4 Angola | 217 | 57 | 45 | 5.9 | Africa | . Inspect memory usage . drinks.info(memory_usage=&#39;deep&#39;) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 193 entries, 0 to 192 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 country 193 non-null object 1 beer_servings 193 non-null int64 2 spirit_servings 193 non-null int64 3 wine_servings 193 non-null int64 4 total_litres_of_pure_alcohol 193 non-null float64 5 continent 193 non-null object dtypes: float64(1), int64(3), object(2) memory usage: 30.5 KB . Inspect memory usage by column . drinks.memory_usage(deep=&#39;true&#39;) . Index 128 country 12588 beer_servings 1544 spirit_servings 1544 wine_servings 1544 total_litres_of_pure_alcohol 1544 continent 12332 dtype: int64 . Convert country to category type . drinks.continent = drinks.continent.astype(&#39;category&#39;) . drinks.memory_usage(deep=&#39;true&#39;) . Index 128 country 18094 beer_servings 1544 spirit_servings 1544 wine_servings 1544 total_litres_of_pure_alcohol 1544 continent 744 dtype: int64 . drinks.continent.cat.categories . Index([&#39;Africa&#39;, &#39;Asia&#39;, &#39;Europe&#39;, &#39;North America&#39;, &#39;Oceania&#39;, &#39;South America&#39;], dtype=&#39;object&#39;) . df = pd.DataFrame({&#39;id&#39;:[1, 2, 3, 4, 5], &#39;quality&#39;:[&#39;good&#39;, &#39;excellent&#39;, &#39;very good&#39;, &#39;excellent&#39;, &#39;good&#39;]}) df . id quality . 0 1 | good | . 1 2 | excellent | . 2 3 | very good | . 3 4 | excellent | . 4 5 | good | . df.sort_values(&#39;quality&#39;) . id quality . 1 2 | excellent | . 3 4 | excellent | . 0 1 | good | . 4 5 | good | . 2 3 | very good | . from pandas.api.types import CategoricalDtype quality_cat = CategoricalDtype([&#39;good&#39;, &#39;very good&#39;, &#39;excellent&#39;], ordered=True) df.quality = df.quality.astype(quality_cat) df . id quality . 0 1 | good | . 1 2 | excellent | . 2 3 | very good | . 3 4 | excellent | . 4 5 | good | . df.quality . 0 good 1 excellent 2 very good 3 excellent 4 good Name: quality, dtype: category Categories (3, object): [good &lt; very good &lt; excellent] . dummies = pd.get_dummies(df.quality) df = pd.concat([df, dummies], axis=1) df . id quality good very good excellent . 0 1 | good | 1 | 0 | 0 | . 1 2 | excellent | 0 | 0 | 1 | . 2 3 | very good | 0 | 1 | 0 | . 3 4 | excellent | 0 | 0 | 1 | . 4 5 | good | 1 | 0 | 0 | . Sources . Python for Data Analysis | .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/09/01/categorical-variables-in-pandas.html",
            "relUrl": "/python/pandas/2020/09/01/categorical-variables-in-pandas.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "Reset cumsum counter at missing values",
            "content": "Problem . I want to reset cumcount() at each missing value in an array or series. . Solution . From the Pandas cookbook, the below two approaches solve the problem using Numpy and Pandas. . Numpy solution, from here . import numpy as np v = np.array([1., 1., 1., np.nan, 1., 1., 1., 1., np.nan, 1.]) isna = np.isnan(v) notna = ~isna # replace missing values with cumsum up to this point cumsum = np.cumsum(notna) diffs = np.diff(np.concatenate(([0], cumsum[isna]))) v[isna] = -diffs np.cumsum(v) . array([1., 2., 3., 0., 1., 2., 3., 4., 0., 1.]) . Using Pandas . From here, has the additional benefit of also working with values other than 1. . import pandas as pd s = pd.Series([1., 3., 1., np.nan, 1., 1., 1., 1., np.nan, 1.]) cumsum = s.cumsum().ffill() reset = -cumsum[s.isna()].diff().fillna(cumsum) result = s.where(s.notna(), reset).cumsum() result . 0 1.0 1 4.0 2 5.0 3 0.0 4 1.0 5 2.0 6 3.0 7 4.0 8 0.0 9 1.0 dtype: float64 .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/08/09/reset-cumsum-at-missing.html",
            "relUrl": "/python/pandas/2020/08/09/reset-cumsum-at-missing.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Counting number of equal adjacent values",
            "content": "Problem . Count the number of adjacent equal values in a series. . import pandas as pd df = pd.DataFrame([0, 1, 5, 7, 7, 1, 1, 1, 0, 1, 1], columns=[&#39;a&#39;]) df . a . 0 0 | . 1 1 | . 2 5 | . 3 7 | . 4 7 | . 5 1 | . 6 1 | . 7 1 | . 8 0 | . 9 1 | . 10 1 | . Answer . Only slightly adapted from this Stack Overflow answer. . df[&#39;count&#39;] = df.groupby((df.a != df.a.shift()).cumsum()).cumcount().add(1) df . a count . 0 0 | 1 | . 1 1 | 1 | . 2 5 | 1 | . 3 7 | 1 | . 4 7 | 2 | . 5 1 | 1 | . 6 1 | 2 | . 7 1 | 3 | . 8 0 | 1 | . 9 1 | 1 | . 10 1 | 2 | . How it works . Demark group boundaries by checking for different preceeding value . df.a != df.a.shift() . 0 True 1 True 2 True 3 True 4 False 5 True 6 False 7 False 8 True 9 True 10 False Name: a, dtype: bool . Label groups using cumulative counting . (df.a != df.a.shift()).cumsum() . 0 1 1 2 2 3 3 4 4 4 5 5 6 5 7 5 8 6 9 7 10 7 Name: a, dtype: int64 . Group dataset by labelled groups . for idx, data in df.groupby((df.a != df.a.shift()).cumsum()): display(data) print() . a . 0 0 | . . a . 1 1 | . . a . 2 5 | . . a . 3 7 | . 4 7 | . . a . 5 1 | . 6 1 | . 7 1 | . . a . 8 0 | . . a . 9 1 | . 10 1 | . . Generate a cumulative count for each group . df.a.groupby((df.a != df.a.shift()).cumsum()).cumcount() . 0 0 1 0 2 0 3 0 4 1 5 0 6 1 7 2 8 0 9 0 10 1 dtype: int64 .",
            "url": "https://fabiangunzinger.github.io/blog/pandas/2020/08/09/count-equal-adjacent-values.html",
            "relUrl": "/pandas/2020/08/09/count-equal-adjacent-values.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Tidy Tuesdays with Python",
            "content": "Tidy Tuesday site | David Robinson&#39;s approach | . The project was to do what David Robinson does -- to give myself an hour to analyse a dataset -- and then to compare my results to his. Only did this once, and incompletely. But hope to get back to it at some point. It&#39;s a great and fun way to practice fluency in data munging and visualisation. . My code (after 1 hour) . import pandas as pd import numpy as np import matplotlib.pyplot as plt plt.style.use(&quot;seaborn&quot;) # %load_ext lab_black . cran = pd.read_csv( &quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-12/loc_cran_packages.csv&quot; ) . cran.describe() . file blank comment code . count 34477.000000 | 34477.000000 | 34477.000000 | 3.447700e+04 | . mean 11.165821 | 257.097775 | 432.708820 | 1.506400e+03 | . std 66.075754 | 2011.333988 | 2814.100058 | 1.255484e+04 | . min 1.000000 | 0.000000 | 0.000000 | 0.000000e+00 | . 25% 1.000000 | 17.000000 | 1.000000 | 8.300000e+01 | . 50% 3.000000 | 53.000000 | 33.000000 | 3.360000e+02 | . 75% 10.000000 | 174.000000 | 284.000000 | 1.043000e+03 | . max 10737.000000 | 310945.000000 | 304465.000000 | 1.580460e+06 | . fig, [ax0, ax1, ax2] = plt.subplots(1, 3, figsize=(15, 5), sharey=True) cran[&quot;code&quot;].hist(bins=np.arange(0, 5000, 50), ax=ax0) ax0.set_xlabel(&quot;Lines of code&quot;) ax0.set_ylabel(&quot;Count&quot;) cran[&quot;comment&quot;].hist(bins=np.arange(0, 2000, 20), ax=ax1) ax1.set_xlabel(&quot;Lines of comment&quot;) cran[&quot;blank&quot;].hist(bins=np.arange(0, 2000, 20), ax=ax2) ax2.set_xlabel(&quot;Number of blank lines&quot;) . Text(0.5, 0, &#39;Number of blank lines&#39;) . counts = cran[&quot;language&quot;].value_counts().head(20) counts.sort_values(ascending=True).plot(kind=&quot;barh&quot;) plt.xlabel(&quot;Number of packages&quot;) . Text(0.5, 0, &#39;Number of packages&#39;) . top = counts.head(4) colors = [&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;] for idx, lang in enumerate(top.index): data = df[df[&quot;language&quot;] == lang] data[&quot;code&quot;].hist( bins=np.arange(0, 2000, 20), alpha=0.3, label=lang, color=colors[idx] ) plt.legend() . &lt;matplotlib.legend.Legend at 0x120c3dc10&gt; . Stuff based on David Robinson&#39;s approach . print(cran.shape) cran.head() . (34477, 7) . file language blank comment code pkg_name version . 0 2 | R | 96 | 353 | 365 | A3 | 1.0.0 | . 1 1 | HTML | 347 | 5 | 2661 | aaSEA | 1.0.0 | . 2 23 | R | 63 | 325 | 676 | aaSEA | 1.0.0 | . 3 3 | HTML | 307 | 9 | 1275 | abbyyR | 0.5.5 | . 4 30 | R | 224 | 636 | 587 | abbyyR | 0.5.5 | . by_language = ( cran.groupby(&quot;language&quot;) .agg( packages=(&quot;code&quot;, &quot;count&quot;), files=(&quot;file&quot;, &quot;sum&quot;), code=(&quot;code&quot;, &quot;sum&quot;), comment=(&quot;comment&quot;, &quot;sum&quot;), ) .assign( lines_per_package=lambda df: df.code / df.packages, files_per_package=lambda df: df.files / df.packages, comment_code_ratio=lambda df: df.comment / df.code, ) .sort_values(&quot;packages&quot;, ascending=False) ) by_language . packages files code comment lines_per_package files_per_package comment_code_ratio . language . R 14689 | 267967 | 22822548 | 9414210 | 1553.716931 | 18.242699 | 0.412496 | . Markdown 5710 | 9036 | 636948 | 1 | 111.549562 | 1.582487 | 0.000002 | . HTML 3680 | 7893 | 4293856 | 32783 | 1166.808696 | 2.144837 | 0.007635 | . C 2162 | 13540 | 4764598 | 1171456 | 2203.791859 | 6.262720 | 0.245867 | . C++ 2041 | 16442 | 3957771 | 817848 | 1939.133268 | 8.055855 | 0.206644 | . ... ... | ... | ... | ... | ... | ... | ... | . SWIG 1 | 5 | 2666 | 507 | 2666.000000 | 5.000000 | 0.190173 | . ActionScript 1 | 2 | 3110 | 0 | 3110.000000 | 2.000000 | 0.000000 | . PowerShell 1 | 1 | 3 | 0 | 3.000000 | 1.000000 | 0.000000 | . Prolog 1 | 1 | 6 | 0 | 6.000000 | 1.000000 | 0.000000 | . Velocity Template Language 1 | 32 | 851 | 194 | 851.000000 | 32.000000 | 0.227967 | . 108 rows × 7 columns . by_language.head(20)[&quot;packages&quot;].sort_values(ascending=True).plot(kind=&quot;barh&quot;) plt.xlabel(&quot;Number of packages written in langauge&quot;) . Text(0.5, 0, &#39;Number of packages written in langauge&#39;) . data = by_language.sort_values(&quot;packages&quot;, ascending=False).query(&quot;packages &gt; 20&quot;) x = data[&quot;packages&quot;].transform(&quot;log10&quot;) y = data[&quot;comment_code_ratio&quot;] label = data.index plt.figure(figsize=(10, 7)) plt.scatter(x, y) for x, y, label in zip(x, y, label): plt.text(x + 0.02, y + 0.01, label) plt.xlabel(&quot;Number of packages (in log10)&quot;) plt.ylabel(&quot;Comment to code ratio&quot;) . Text(0, 0.5, &#39;Comment to code ratio&#39;) . data = by_language.sort_values(&quot;packages&quot;, ascending=False).query(&quot;packages &gt; 20&quot;) x = data[&quot;packages&quot;].transform(&quot;log10&quot;) y = data[&quot;lines_per_package&quot;] label = data.index plt.figure(figsize=(10, 7)) plt.scatter(x, y) for x, y, label in zip(x, y, label): plt.text(x + 0.02, y + 0.01, label) plt.xlabel(&quot;Number of packages (in log10)&quot;) plt.ylabel(&quot;Lines per package&quot;) . Text(0, 0.5, &#39;Lines per package&#39;) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) data = cran.query(&quot;language == &#39;R&#39; &amp; code &gt; 0&quot;)[&quot;code&quot;] data.transform(&quot;log10&quot;).hist(bins=40, ax=ax1) ax1.axvline( np.log10(data.median()), color=&quot;green&quot;, label=&quot;Median = {:2.0f}&quot;.format(data.median()), ) ax1.axvline( np.log10(data.mean()), color=&quot;orange&quot;, label=&quot;Mean = {:2.0f}&quot;.format(data.mean()) ) data.hist(bins=40, ax=ax2) ax2.axvline(data.median(), color=&quot;green&quot;) ax2.axvline(data.mean(), color=&quot;orange&quot;) fig.legend() . &lt;matplotlib.legend.Legend at 0x12835ac10&gt; . cran.sort_values(&quot;code&quot;, ascending=False).head(10) # cran.sort_values(&quot;code&quot;, ascending=False).tail(5) . file language blank comment code pkg_name version . 2096 10737 | C/C++ Header | 310945 | 304465 | 1580460 | BH | 1.69.0-1 | . 5647 27 | SVG | 0 | 0 | 1188625 | dabestr | 0.2.2 | . 4753 39 | JSON | 0 | 0 | 481022 | congressbr | 0.2.1 | . 30620 804 | C++ | 59385 | 102574 | 385839 | stringi | 1.4.3 | . 7649 20 | HTML | 17075 | 123 | 342681 | edgarWebR | 1.0.0 | . 23897 294 | JSON | 1019 | 0 | 295553 | rcorpora | 2.0.0 | . 5428 17 | C/C++ Header | 469 | 317 | 198162 | cubature | 2.0.3 | . 28181 48 | C | 23053 | 79977 | 184799 | seqminer | 7.1 | . 6836 913 | C/C++ Header | 46085 | 78546 | 183763 | dlib | 1.0.3 | . 1750 159 | C++ | 53470 | 26909 | 180621 | BayesXsrc | 3.0-1 | . plt.style.use(&#39;ggplot&#39;) (cran[cran.pkg_name.str.contains(&quot;^tidy&quot;)] .groupby([&quot;pkg_name&quot;, &quot;language&quot;]).code.sum() .unstack() .assign(tot=lambda x: x.sum(axis=1)) .sort_values(&#39;tot&#39;).drop(&#39;tot&#39;, axis=1) ).plot.barh(stacked=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x126467390&gt; . Replicate above again from memory and as fast as I can while takling out lound tomorrow . (cran.groupby(&#39;pkg_name&#39;).sum() .query(&#39;code &gt; 100 &amp; comment &gt; 0&#39;) .assign(code_comment_ratio = lambda df: (df.code / df.comment).transform(&#39;log10&#39;), tidy = lambda df: df.index.str.startswith(&#39;tidy&#39;) ) )[[&#39;tidy&#39;, &#39;code_comment_ratio&#39;]].groupby(&#39;tidy&#39;).hist(bins=10) . tidy False [[AxesSubplot(0.125,0.125;0.775x0.755)]] True [[AxesSubplot(0.125,0.125;0.775x0.755)]] dtype: object . Correlation between size and downloads . downloads = pd.read_csv(&#39;http://cran-logs.rstudio.com/2020/2020-03-02.csv.gz&#39;) . print(downloads.shape) downloads.head() . (5015440, 10) . date time size r_version r_arch r_os package version country ip_id . 0 2020-03-02 | 16:12:36 | 817352 | NaN | NaN | NaN | rlang | 0.4.5 | US | 1 | . 1 2020-03-02 | 16:15:07 | 65936 | NaN | NaN | NaN | generics | 0.0.2 | GB | 2 | . 2 2020-03-02 | 16:15:00 | 461452 | NaN | NaN | NaN | scatterplot3d | 0.3-41 | US | 3 | . 3 2020-03-02 | 16:13:46 | 202327 | 3.6.2 | x86_64 | mingw32 | modelr | 0.1.6 | US | 4 | . 4 2020-03-02 | 16:10:55 | 20721 | NaN | NaN | NaN | ColorPalette | 1.0-1 | US | 5 | . downloads_per_package = downloads.rename({&#39;package&#39;: &#39;pkg_name&#39;, &#39;data&#39;:&#39;downloads&#39;}, axis=1).groupby(&#39;pkg_name&#39;).downloads.count() . AttributeError Traceback (most recent call last) &lt;ipython-input-177-fae9a88496e2&gt; in &lt;module&gt; -&gt; 1 downloads_per_package = downloads.rename({&#39;package&#39;: &#39;pkg_name&#39;, &#39;data&#39;:&#39;downloads&#39;}, axis=1).groupby(&#39;pkg_name&#39;).downloads.count() ~/miniconda3/envs/basics/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in __getattr__(self, attr) 578 579 raise AttributeError( --&gt; 580 f&#34;&#39;{type(self).__name__}&#39; object has no attribute &#39;{attr}&#39;&#34; 581 ) 582 AttributeError: &#39;DataFrameGroupBy&#39; object has no attribute &#39;downloads&#39; . pd.merge(cran, downloads_per_package, on=&#39;pkg_name&#39;) . file language blank comment code pkg_name version date . 0 2 | R | 96 | 353 | 365 | A3 | 1.0.0 | 40 | . 1 1 | HTML | 347 | 5 | 2661 | aaSEA | 1.0.0 | 17 | . 2 23 | R | 63 | 325 | 676 | aaSEA | 1.0.0 | 17 | . 3 3 | HTML | 307 | 9 | 1275 | abbyyR | 0.5.5 | 26 | . 4 30 | R | 224 | 636 | 587 | abbyyR | 0.5.5 | 26 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 34146 1 | Markdown | 13 | 0 | 15 | ztype | 0.1.0 | 18 | . 34147 10 | R | 121 | 221 | 447 | ZVCV | 1.0.0 | 8 | . 34148 2 | C++ | 20 | 7 | 70 | ZVCV | 1.0.0 | 8 | . 34149 1 | Markdown | 7 | 0 | 11 | ZVCV | 1.0.0 | 8 | . 34150 1 | R | 51 | 17 | 169 | zyp | 0.10-1.1 | 27 | . 34151 rows × 8 columns .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/05/03/tidy-tuesday.html",
            "relUrl": "/python/pandas/2020/05/03/tidy-tuesday.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "What were the "best" events in TED history to attend",
            "content": "Kevin Markham best practices, following this talk. . import pandas as pd path = &#39;https://raw.githubusercontent.com/justmarkham/pycon-2019-tutorial/master/ted.csv&#39; ted = pd.read_csv(path) . ted.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2550 entries, 0 to 2549 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 comments 2550 non-null int64 1 description 2550 non-null object 2 duration 2550 non-null int64 3 event 2550 non-null object 4 film_date 2550 non-null int64 5 languages 2550 non-null int64 6 main_speaker 2550 non-null object 7 name 2550 non-null object 8 num_speaker 2550 non-null int64 9 published_date 2550 non-null int64 10 ratings 2550 non-null object 11 related_talks 2550 non-null object 12 speaker_occupation 2544 non-null object 13 tags 2550 non-null object 14 title 2550 non-null object 15 url 2550 non-null object 16 views 2550 non-null int64 17 film_year 2550 non-null int64 dtypes: int64(8), object(10) memory usage: 358.7+ KB . ted.isna().sum() . comments 0 description 0 duration 0 event 0 film_date 0 languages 0 main_speaker 0 name 0 num_speaker 0 published_date 0 ratings 0 related_talks 0 speaker_occupation 6 tags 0 title 0 url 0 views 0 film_year 0 dtype: int64 . Which talks provoke the most online discussion? . We can rank the talks by number of comments, normalised by the number of views. . ted[&#39;views_per_comment&#39;] = ted.views / ted.comments ted.sort_values(&#39;views_per_comment&#39;)[[&#39;name&#39;, &#39;views_per_comment&#39;]].head() . name views_per_comment . 744 Diane J. Savino: The case for same-sex marriage | 450.531587 | . 803 David Bismark: E-voting without fraud | 651.739808 | . 96 Richard Dawkins: Militant atheism | 683.134291 | . 694 Sharmeen Obaid-Chinoy: Inside a school for sui... | 703.886818 | . 954 Janet Echelman: Taking imagination seriously | 735.525682 | . Visualise the distribution of comments . ted.comments.hist(); . ted[ted.comments &gt;= 1500].shape . (17, 19) . Most talks have fewer than 1500 comments (only 17 have more), so I drop the ones that have more. I initially tried to avoid this, but as Keving points out in the video: plotting entails decision making, as a plot is a summary of your data and not a representation of all your data. This is a good lesson to take away from this. . Use query to filter data . ted.query(&#39;comments &lt; 1000&#39;).comments.hist(); . Use indexing to filter . ted[ted.comments &lt; 1000].comments.hist(); . Use loc to filter, which is considered best practice . ted.loc[ted.comments &lt; 1000, &#39;comments&#39;].hist(bins=20, grid=False); . Plot the number of talks that took place each year . film_year = pd.to_datetime(ted.film_date, unit=&#39;s&#39;).dt.year ted.groupby(film_year).size().plot(marker=&#39;o&#39;); . Alternative approach (no need to group data first, here) . film_year[film_year.between(1997, 2016)].value_counts().sort_index().plot(marker=&#39;o&#39;); . Let&#39;s define &quot;best&quot; by event that had the most discussed talks, defined as talks with low view-to-comments ratio (i.e. talks for which people were likely to leave comments), as used above. . df = (ted.groupby(&#39;event&#39;) .agg({&#39;views_per_comment&#39;:[(&#39;total&#39;, &#39;sum&#39;), (&#39;talks&#39;, &#39;count&#39;)]}) .droplevel(level=0, axis=1) .reset_index()) df = df[df.talks &gt;= 10] df[&#39;weighted_sum&#39;] = df.total / df.talks df.sort_values(&#39;weighted_sum&#39;)[:10] . event total talks weighted_sum . 97 TEDGlobal 2010 | 286404.503200 | 55 | 5207.354604 | . 135 TEDWomen 2010 | 190844.942208 | 34 | 5613.086536 | . 106 TEDIndia 2009 | 212406.277338 | 35 | 6068.750781 | . 24 Mission Blue Voyage | 109437.301134 | 18 | 6079.850063 | . 60 TED2010 | 416144.893485 | 68 | 6119.777845 | . 93 TEDCity2.0 | 81633.440301 | 11 | 7421.221846 | . 9 EG 2007 | 97900.518061 | 13 | 7530.809082 | . 98 TEDGlobal 2011 | 518812.975335 | 68 | 7629.602578 | . 95 TEDGlobal 2007 | 214516.379586 | 27 | 7945.051096 | . 61 TED2011 | 574975.371702 | 70 | 8213.933881 | . Above is somewhat embarassing, as I&#39;m manually, and unnecessarily, calculate the mean. Simpler approach, focusing on total views, rather than normalised comments. . (ted.groupby(&#39;event&#39;).views .agg([&#39;mean&#39;, &#39;count&#39;, &#39;sum&#39;]) .sort_values(&#39;sum&#39;, ascending=False)[:5]) . mean count sum . event . TED2013 2.302700e+06 | 77 | 177307937 | . TED2014 2.072874e+06 | 84 | 174121423 | . TEDGlobal 2013 2.584163e+06 | 66 | 170554736 | . TED2015 2.011017e+06 | 75 | 150826305 | . TED2006 3.274345e+06 | 45 | 147345533 | . Unpack the ratings data . import ast # abstract syntax tree . ast.literal_eval(&#39;[1, 2, 3]&#39;) . [1, 2, 3] . pieces = [] for talk in range(len(ted.index)): ratings = ted.loc[talk, &#39;ratings&#39;] talk_name = ted.loc[talk, &#39;name&#39;] frame = pd.DataFrame(ast.literal_eval(ratings)) frame[&#39;talk_name&#39;] = talk_name pieces.append(frame) ratings = pd.concat(pieces) ratings.head() . id name count talk_name . 0 7 | Funny | 19645 | Ken Robinson: Do schools kill creativity? | . 1 1 | Beautiful | 4573 | Ken Robinson: Do schools kill creativity? | . 2 9 | Ingenious | 6073 | Ken Robinson: Do schools kill creativity? | . 3 3 | Courageous | 3253 | Ken Robinson: Do schools kill creativity? | . 4 11 | Longwinded | 387 | Ken Robinson: Do schools kill creativity? | . Alternative . ted.ratings = ted.ratings.apply(ast.literal_eval) pieces = [] for talk in range(len(ted.index)): ratings = ted.loc[talk, &#39;ratings&#39;] talk_name = ted.loc[talk, &#39;name&#39;] frame = pd.DataFrame(ratings) frame[&#39;talk_name&#39;] = talk_name pieces.append(frame) ratings = pd.concat(pieces) ratings.head() . id name count talk_name . 0 7 | Funny | 19645 | Ken Robinson: Do schools kill creativity? | . 1 1 | Beautiful | 4573 | Ken Robinson: Do schools kill creativity? | . 2 9 | Ingenious | 6073 | Ken Robinson: Do schools kill creativity? | . 3 3 | Courageous | 3253 | Ken Robinson: Do schools kill creativity? | . 4 11 | Longwinded | 387 | Ken Robinson: Do schools kill creativity? | . ratings.head() . id name count talk_name . 0 7 | Funny | 19645 | Ken Robinson: Do schools kill creativity? | . 1 1 | Beautiful | 4573 | Ken Robinson: Do schools kill creativity? | . 2 9 | Ingenious | 6073 | Ken Robinson: Do schools kill creativity? | . 3 3 | Courageous | 3253 | Ken Robinson: Do schools kill creativity? | . 4 11 | Longwinded | 387 | Ken Robinson: Do schools kill creativity? | . Count total number of ratings received by each talk . %%timeit def count_ratings(ratings): return pd.DataFrame(ratings)[&#39;count&#39;].sum() ted[&#39;count_ratings&#39;] = ted.ratings.apply(count_ratings) ted.sort_values(&#39;count_ratings&#39;, ascending=False).name[:5] . 1.53 s ± 11.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . Alternative that&#39;s much faster! . %%timeit def get_num_ratings(list_of_dicts): count = 0 for d in list_of_dicts: count += d[&#39;count&#39;] return count ted[&#39;count_ratings&#39;] = ted.ratings.apply(get_num_ratings) ted.sort_values(&#39;count_ratings&#39;, ascending=False).name[:5] . 6.33 ms ± 173 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) .",
            "url": "https://fabiangunzinger.github.io/blog/2020/04/17/markham-best-practices.html",
            "relUrl": "/2020/04/17/markham-best-practices.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "Python data science handbook tricks",
            "content": "Based on content from the Python Data Science Handbook. . Planets dataset from data science handbook . import pandas as pd import seaborn as sns df = sns.load_dataset(&#39;planets&#39;) print(df.shape) df.head() . (1035, 6) . method number orbital_period mass distance year . 0 Radial Velocity | 1 | 269.300 | 7.10 | 77.40 | 2006 | . 1 Radial Velocity | 1 | 874.774 | 2.21 | 56.95 | 2008 | . 2 Radial Velocity | 1 | 763.000 | 2.60 | 19.84 | 2011 | . 3 Radial Velocity | 1 | 326.030 | 19.40 | 110.62 | 2007 | . 4 Radial Velocity | 1 | 516.220 | 10.50 | 119.47 | 2009 | . Show number of planets discovered by each methods in each decade . decade = (df.year // 10) * 10 decade = decade.astype(&#39;str&#39;) + &#39;s&#39; df.groupby([&#39;method&#39;, decade]).number.sum().unstack().fillna(0) . year 1980s 1990s 2000s 2010s . method . Astrometry 0.0 | 0.0 | 0.0 | 2.0 | . Eclipse Timing Variations 0.0 | 0.0 | 5.0 | 10.0 | . Imaging 0.0 | 0.0 | 29.0 | 21.0 | . Microlensing 0.0 | 0.0 | 12.0 | 15.0 | . Orbital Brightness Modulation 0.0 | 0.0 | 0.0 | 5.0 | . Pulsar Timing 0.0 | 9.0 | 1.0 | 1.0 | . Pulsation Timing Variations 0.0 | 0.0 | 1.0 | 0.0 | . Radial Velocity 1.0 | 52.0 | 475.0 | 424.0 | . Transit 0.0 | 0.0 | 64.0 | 712.0 | . Transit Timing Variations 0.0 | 0.0 | 0.0 | 9.0 | . Loop over groupbs and use nice formatting . for (method, data) in df.groupby(&#39;method&#39;): print(f&#39;{method:35}{data.shape}&#39;) . Astrometry (2, 6) Eclipse Timing Variations (9, 6) Imaging (38, 6) Microlensing (23, 6) Orbital Brightness Modulation (3, 6) Pulsar Timing (5, 6) Pulsation Timing Variations (1, 6) Radial Velocity (553, 6) Transit (397, 6) Transit Timing Variations (4, 6) . Titanic survivors . tit = sns.load_dataset(&#39;titanic&#39;) print(tit.shape) tit.head() . (891, 15) . survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone . 0 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | Third | man | True | NaN | Southampton | no | False | . 1 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | First | woman | False | C | Cherbourg | yes | False | . 2 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | Third | woman | False | NaN | Southampton | yes | True | . 3 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | First | woman | False | C | Southampton | yes | False | . 4 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | Third | man | True | NaN | Southampton | no | True | . tit.pivot_table(&#39;survived&#39;, index=&#39;sex&#39;, columns=&#39;class&#39;) . class First Second Third . sex . female 0.968085 | 0.921053 | 0.500000 | . male 0.368852 | 0.157407 | 0.135447 | . age = pd.cut(tit.age, [0, 18, 100]) fare = pd.qcut(tit.fare, 2) tit.pivot_table(&#39;survived&#39;, index=[&#39;sex&#39;, age], columns=[&#39;class&#39;, fare]) . class First Second Third . fare (-0.001, 14.454] (14.454, 512.329] (-0.001, 14.454] (14.454, 512.329] (-0.001, 14.454] (14.454, 512.329] . sex age . female (0, 18] NaN | 0.909091 | 1.000000 | 1.000000 | 0.714286 | 0.318182 | . (18, 100] NaN | 0.972973 | 0.880000 | 0.914286 | 0.444444 | 0.391304 | . male (0, 18] NaN | 0.800000 | 0.000000 | 0.818182 | 0.260870 | 0.178571 | . (18, 100] 0.0 | 0.391304 | 0.098039 | 0.030303 | 0.125000 | 0.192308 | . US birth rate data . import seaborn as sns sns.set() sns.set_style(&#39;white&#39;) df = pd.read_csv(&#39;./data/births.csv&#39;) print(df.shape) df.head() . (15547, 5) . year month day gender births . 0 1969 | 1 | 1.0 | F | 4046 | . 1 1969 | 1 | 1.0 | M | 4440 | . 2 1969 | 1 | 2.0 | F | 4454 | . 3 1969 | 1 | 2.0 | M | 4548 | . 4 1969 | 1 | 3.0 | F | 4548 | . Number of birhts per year by gender . df.pivot_table(&#39;births&#39;, index=&#39;year&#39;, columns=&#39;gender&#39;, aggfunc=&#39;sum&#39;).plot(); .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/04/05/python-datascience-handbook-tricks.html",
            "relUrl": "/python/pandas/2020/04/05/python-datascience-handbook-tricks.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "Creating dummy variables in Pandas",
            "content": "A quick post to remind my future self of how to create dummy variables. All based on this video from Data School. . import pandas as pd . df = pd.DataFrame({&#39;id&#39;:[1, 2, 3, 4, 5], &#39;quality&#39;:[&#39;good&#39;, &#39;excellent&#39;, &#39;very good&#39;, &#39;excellent&#39;, &#39;good&#39;]}) df.head() . id quality . 0 1 | good | . 1 2 | excellent | . 2 3 | very good | . 3 4 | excellent | . 4 5 | good | . Pandas makes creating dummies easy: . pd.get_dummies(df.quality) . excellent good very good . 0 0 | 1 | 0 | . 1 1 | 0 | 0 | . 2 0 | 0 | 1 | . 3 1 | 0 | 0 | . 4 0 | 1 | 0 | . If you want to label the source of the data, you can use the prefix argument: . pd.get_dummies(df.quality, prefix=&#39;quality&#39;) . quality_excellent quality_good quality_very good . 0 0 | 1 | 0 | . 1 1 | 0 | 0 | . 2 0 | 0 | 1 | . 3 1 | 0 | 0 | . 4 0 | 1 | 0 | . Often when we work with dummies from a variable with $n$ distinct values, we create $n-1$ dummies and treat the remaining group as the reference group. Pandas provides a convenient way to do this: . pd.get_dummies(df.quality, prefix=&#39;quality&#39;, drop_first=True) . quality_good quality_very good . 0 1 | 0 | . 1 0 | 0 | . 2 0 | 1 | . 3 0 | 0 | . 4 1 | 0 | . Usually, we&#39;ll want to use the dummies with the rest of the data, so it&#39;s conveninet to have them in the original dataframe. One way to do this is to use concat like so: . dummies = pd.get_dummies(df.quality, prefix=&#39;quality&#39;, drop_first=True) df_with_dummies = pd.concat([df, dummies], axis=1) df_with_dummies.head() . id quality quality_good quality_very good . 0 1 | good | 1 | 0 | . 1 2 | excellent | 0 | 0 | . 2 3 | very good | 0 | 1 | . 3 4 | excellent | 0 | 0 | . 4 5 | good | 1 | 0 | . This works. But Pandas provides a much easier way: . df_with_dummies1 = pd.get_dummies(df, columns=[&#39;quality&#39;], drop_first=True) df_with_dummies1 . id quality_good quality_very good . 0 1 | 1 | 0 | . 1 2 | 0 | 0 | . 2 3 | 0 | 1 | . 3 4 | 0 | 0 | . 4 5 | 1 | 0 | . That&#39;s it. In one line we get a new dataframe that includes the dummies and excludes the original quality column. . Sources .",
            "url": "https://fabiangunzinger.github.io/blog/python/pandas/2020/04/01/creating-dummies.html",
            "relUrl": "/python/pandas/2020/04/01/creating-dummies.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://fabiangunzinger.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "New post",
            "content": "from imports import * %config InlineBackend.figure_format = &#39;retina&#39; %load_ext autoreload %autoreload 2 . plt.plot([0, 1]) . [&lt;matplotlib.lines.Line2D at 0x1a196b0a10&gt;] . Sources . Fluent Python | Python Cookbook | Learning Python | The Hitchhiker&#39;s Guide to Python | Effective Python | Python for Data Analysis | Python Data Science Handbook | .",
            "url": "https://fabiangunzinger.github.io/blog/python/2000/01/01/new.html",
            "relUrl": "/python/2000/01/01/new.html",
            "date": " • Jan 1, 2000"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi. I’m a PhD student at Warwick Business School, where I use insights from behavioural economics and tools from econometrics and machine learning to understand consumer financial behaviour from mass transaction data. .",
          "url": "https://fabiangunzinger.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fabiangunzinger.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}